[
["data-cleaning-dorian-cassie.html", " Data cleaning (Dorian &amp; Cassie) 1.1 Remove blank, NULL, and missing values 1.2 Remove Qualified data 1.3 Duplicate observations", " Data cleaning (Dorian &amp; Cassie) Before jumping into your analysis you’ll want to clean it up with some helpful quality checks and formatting procedures. These procedures include steps to: 1.1 Remove blank, invalid, NULL, and missing values 1.2 Evaluate qualified data 1.3 Remove duplicate observations 1.1 Remove blank, NULL, and missing values Description Large monitoring data sets often contain observations with missing concentrations, detection limits or other labeling errors that can lead to incorrect summary statistics. Recommended steps Identify any blank, NULL, -999, and missing values. If available, review Null Data Codes. These codes will tell you why the data is missing. Most data sets will include null data codes. For AQS null code descriptions, see https://aqs.epa.gov/aqsweb/documents/codetables/qualifiers.html. Determine if your analysis needs to account for missing data before deleting observations. You may want to perform a count on all sample dates to quantify the expected number of observations. If you no longer need these records, remove them from the dataset. Document your process. Why not keep them? In most cases, our analyses do not require or allow filling in missing values. For this reason, it makes sense to remove them. However, for some analyses you may want to fill the missing values. The method for filling values will be project specific and decisions should be documented. Data sets should identify any records that include a replaced missing value. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load example monitoring data library(tidyverse) data &lt;- read_csv(&#39;aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,-999.99,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,NA,NA,0.08,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,&quot; &quot;,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,NULL,a,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot;&#39;) Table 1.1: Sample monitoring data. aqs_id poc param_code date conc null_code md_limit pollutant cas 271231003 1 12101 2004-01-04 -999.99 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-10 0.23 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-16 0.35 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-22 0.22 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-28 NA NA 0.08 Aluminum 7429-90-5 271231003 1 12101 2004-02-03 0.07 NA 0.06 Aluminum 7429-90-5 Create a function to test for missing concentration values. # Test for missing concentrations, non-numeric values, and -999 missing_conc &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } Use the function to add a column to your data conc_missing that tests for missing concentration values. # Create a new TRUE/FALSE column labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc)) # Select all missing observations missing_values &lt;- filter(data, conc_missing == TRUE) Table: Missing values Filter the data to only non-missing observations. data &lt;- filter(data, conc_missing == FALSE) Table: The new and improved cleaner data You can create similar functions to test for missing dates, site IDs, detection limits, and parameter codes. # Test for missing dates missing_dates &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &gt; 11 || nchar(as.character(x)) &lt; 6 } # Test for missing site IDs missing_sites &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &lt; 5 } # Test for missing detection limits missing_dls &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } # Test for missing parameter codes missing_param &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 || nchar(as.character(x)) &lt; 5 || nchar(as.character(x)) &gt; 9 } To apply these functions all at once use dplyr’s great function called mutate(). # Create new TRUE/FALSE columns labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc), date_missing = missing_dates(date), site_missing = missing_sites(aqs_id), dl_missing = missing_dls(md_limit), param_missing = missing_param(param_code)) # Filter to remove any rows with a missing parameter. # We use sum() to count the number of missing parameters. # In this case we will drop any row with at least one missing parameter. data &lt;- data %&gt;% filter(sum(c(conc_missing, date_missing, site_missing, dl_missing, param_missing), na.rm = T) &lt; 1) Table: The super cleaner data Contributors Dorian Kvale References 1.2 Remove Qualified data Description Valid sample results may have data qualifiers. These qualifiers provide contextual information about factors that may have influenced the result. In the AQS data format, qualifiers are listed in 10 qualifier fields. A sample may have more than one qualifier. Descriptions of the AQS qualifier codes are available from the EPA at https://aqs.epa.gov/aqsweb/documents/codetables/qualifiers.html. Recommended steps Filter data to view samples with Qualifier codes Evaluate whether any of the qualified data will unduly influence your analysis. For example, you may want to remove samples influenced by unique events. When removing qualified data, maintain a copy of the original data set and document what values have been removed. In some cases, you may want to run your analysis with and without the qualified values to characterize the influence of the qualified data on the results. Sample R script Contributors Cassie McMahon References 1.3 Duplicate observations Description Large monitoring data sets often contain multiple observations from the same monitor for the same time period. When multiple observations do occur they tend to be identified by or by a qualifier describing why the second observation was recorded (e.g. a duplicate for quality control). The treatment of these duplicate values will depend on the analysis. Recommended steps Identify duplicate observations. A duplicate in the data is likely the result of an error. Consult the lab about duplicate values. Treatment of a duplicate observation will depend on the project. For criteria pollutant design values, treatment of duplicate samples is defined for each pollutant in the Appendices of 40 CFR 50. For air toxics, treatment of duplicate values will depend on the analysis. In some cases you may want to average the results. In others, you may want to take the maximum value. If you identify records that are completely duplicated (same date, same site, same POC, same result, same MDL), delete these records prior to completing analyses. For automated annual summaries, use the following hierarchy to remove duplicates: Calculate the mean concentration of all non-censored observations. If all observations are censored, select the observation with the lowest detection limit. If all observations are censored and the detection limits are equal, select a single observation. Why not keep all the data? Leaving multiple observations for some dates in the data set is likely to bias calculated summary statistics. For example, duplicate ozone observations from January would be likely to skew the annual average lower. By limiting the number of observations to 1 per day, we can remove the bias due to duplicate observations. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load the sample monitoring data. library(dplyr) data &lt;- read.csv(text = &#39; aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,0.05,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,0.01,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.02,NA,0.05,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.04,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.18,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; &#39;, stringsAsFactor = F) Table: Sample monitoring data Delete rows of data that are exact duplicates. # Check for exact duplicate rows dup_rows &lt;- data[duplicated(data), ] # Drop the exact duplicates data &lt;- data[!duplicated(data), ] Add a unique column ID to each site/poc/param-code/date combination. # Add unique key to each row data$key &lt;- 1:nrow(data) # Create a unique ID for each site/poc/param-code/date combination data$unique_sample_id &lt;- paste(data$aqs_id, data$poc, data$param_code, data$date, sep = &quot;_&quot;) Figure 1.1: Unique ID column added. Test for duplicate observations. # Label duplicate samples data &lt;- group_by(data, unique_sample_id) %&gt;% mutate(duplicate = n() &gt; 1) # Create duplicate table dupes &lt;- filter(data, duplicate == T) Table: Duplicate observations If duplicates are found, use the following hierarchy to remove duplicates: Calculate the mean concentration of all non-censored observations. If all observations are censored, select the observation with the lowest detection limit. If all observations are censored and the detection limits are equal, select a single observation. # Replace censored observations with NA dupes &lt;- dupes %&gt;% mutate(conc = ifelse(conc &gt;= md_limit, conc, NA)) # Calculate the mean of non-censored observations dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% mutate(conc = mean(conc, na.rm = T)) # Arrange by concentration and detection limit, then select the first observation dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% arrange(desc(conc), md_limit) %&gt;% filter(key == key[1]) # Remove Unique IDs with duplicates from data data &lt;- filter(data, !duplicate) # Attach the selected duplicates with highest result and lowest detection limit data &lt;- rbind(data, dupes) Table: The new cleaner data Figure 1.2: Final data set with 1 duplicate removed. Contributors Dorian Kvale, Cassie McMahon Back to top "]
]
