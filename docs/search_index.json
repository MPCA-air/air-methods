[
["index.html", "Introduction", " Introduction Updated Aug 30, 2017 This guide describes the methods used at the MPCA to analyze air monitoring and modeling data. The charts and code found in this guide were produced using the freely available R statistical software. To follow along with the examples download a copy of R to your computer from the r-project or use R-Fiddle in your browser. "],
["data-cleaning-dorian-cassie.html", " Data cleaning (Dorian &amp; Cassie) 1.1 Remove blank, NULL, and missing values 1.2 Remove Qualified data (Cassie) 1.3 Duplicate observations", " Data cleaning (Dorian &amp; Cassie) Before jumping into your analysis you’ll want to clean it up with some helpful quality checks and formatting procedures. These procedures include steps to: 1.1 Remove blank, invalid, NULL, and missing values 1.2 Evaluate qualified data 1.3 Remove duplicate observations Text summary 1.1 Remove blank, NULL, and missing values Description Large monitoring data sets often contain observations with missing concentrations, detection limits or other labeling errors that can lead to incorrect summary statistics. Recommended steps Identify any blank, NULL, -999, and missing values. Remove those observations. Document your process. Why not keep them? Unless the lab can confirm the meaning of the missing values for you, there is not a reliable means to interpret the values. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load example monitoring data library(tidyverse) data &lt;- read_csv(&#39;aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,-999.99,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,NA,NA,0.08,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,&quot; &quot;,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,NULL,a,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot;&#39;) Table 1.1: Sample monitoring data. aqs_id poc param_code date conc null_code md_limit pollutant cas 271231003 1 12101 2004-01-04 -999.99 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-10 0.23 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-16 0.35 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-22 0.22 NA 0.06 Aluminum 7429-90-5 271231003 1 12101 2004-01-28 NA NA 0.08 Aluminum 7429-90-5 271231003 1 12101 2004-02-03 0.07 NA 0.06 Aluminum 7429-90-5 Create a function to test for missing concentration values. # Test for missing concentrations, non-numeric values, and -999 missing_conc &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } Use the function to add a column to your data conc_missing that tests for missing concentration values. # Create a new TRUE/FALSE column labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc)) # Select all missing observations missing_values &lt;- filter(data, conc_missing == TRUE) Table: Missing values Filter the data to only non-missing observations. data &lt;- filter(data, conc_missing == FALSE) Table: The new and improved cleaner data You can create similar functions to test for missing dates, site IDs, detection limits, and parameter codes. # Test for missing dates missing_dates &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &gt; 11 || nchar(as.character(x)) &lt; 6 } # Test for missing site IDs missing_sites &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &lt; 5 } # Test for missing detection limits missing_dls &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } # Test for missing parameter codes missing_param &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 || nchar(as.character(x)) &lt; 5 || nchar(as.character(x)) &gt; 9 } You can apply these functions all at once by using dplyr’s great function called mutate(). # Create new TRUE/FALSE columns labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc), date_missing = missing_dates(date), site_missing = missing_sites(aqs_id), dl_missing = missing_dls(md_limit), param_missing = missing_param(param_code)) # Filter to remove any rows with a missing parameter. # We use sum() to count the number of missing parameters. # In this case we will drop any row with at least one missing parameter. data &lt;- data %&gt;% filter(sum(c(conc_missing, date_missing, site_missing, dl_missing, param_missing), na.rm = T) &lt; 1) Table: The super cleaner data Contributors Dorian Kvale; Derek Nagel, Kristie Ellickson References 1.2 Remove Qualified data (Cassie) Description Recommended steps Sample R script Contributors Not me References 1.3 Duplicate observations Description Large monitoring data sets often contain a few instances when multiple observations occur over the same time period at the same site. When multiples observations do occur they tend to be identified by a unique monitor POC (parameter occurrence code) or by a qualifier describing why the second observation was recorded (e.g. a duplicate for quality control). Recommended steps Identify duplicate observations. If both values are detected, calculate the mean of the duplicates. If values are identical, use the observation with a lower detection limit. Why not keep them all? If more data is more better, why not keep both values? Leaving both observations in the data set is likely to bias calculated summary statistics. For example, consider a scenario where two monitors were operating on a site during the winter months, but only one monitor was operating the rest of the year. If we were to calculate the annual average for this site using all the observations, the mean would be biased by having twice as many observations from the winter months. By limiting the number of observations to 1 per day, the weighting of the seasons will be balanced. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load the sample monitoring data. library(dplyr) data &lt;- read.csv(text = &#39; aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,0.05,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,0.01,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; &#39;, stringsAsFactor = F) Table: Sample monitoring data Add a unique column identifier to each observation. # Add unique key to each row data$key &lt;- 1:nrow(data) # Create s unique ID for each site/poc/param code/day combination data$unique_sample_id &lt;- paste(data$aqs_id, data$poc, data$param_code, data$date, sep = &quot;_&quot;) Figure 1.1: Unique ID column added. Test for duplicate observations. # Label duplicate samples data &lt;- group_by(data, unique_sample_id) %&gt;% mutate(duplicate = n() &gt; 1) # Create duplicate table dupes &lt;- filter(data, duplicate == T) Table: Duplicate observations If duplicates are found, use the following hierarchy to remove duplicates: Calculate the mean concentration of all detected observations. If no observations are detected, select the observation with the lower detection limit. If no obsercations are detected and the detection limits are all equal, select a single observation. dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% arrange(desc(conc), md_limit) %&gt;% filter(key == key[1]) # Remove Unique IDs with duplicates from data data &lt;- filter(data, !duplicate) # Attach the selected duplicates with highest result and lowest detection limit data &lt;- rbind(data, dupes) Table: The new cleaner data Figure 1.2: Final data set with 1 duplicate removed. Contributors Dorian, Mr. Kitty [^ Back to top](#missing) "],
["data-validation-kristie.html", " Data validation (Kristie) 2.1 Identifying Instrument drift or leaks in a system. 2.2 Identify exceptional and extreme values aka outliers. 2.2 Identify sequential repeats aka “sticky” numbers", " Data validation (Kristie) Environmental monitoring data can be subject to error. Three important errors to look for are: 2.1 Leaks and sample contamination ?? Instrument errors such as drift or repeated results (sticking values) 2.2 Outliers or extreme values Validation steps Identify exceptional values. Identify sequential identical values (sticking). Identify potential instrument drift. Report findings to laboratory for further investigation. Why not average the exceptional data over the year and assume the problem will go away? Further investigation by laboratory staff is required if the laboratory equipment or system is in error. If the laboratory is not in error, these concentrations require additional investigation by data analysts and potentially by air pollution source specialists. For these reasons, these data validation steps are critical. A decreasing signal or repeated sequential measurement means that the data are not interpretable. An exceptionally high value could mean laboratory contamination or potentially could contribute to human health impacts if the high value is accurate. Notes Possible Contamination Issues-by site and pollutant 3*IQR: I used as a simple outlier identifier in EU guidelines. This includes a lot of data for UFPs. Cassie has tried: 3*75th percentile(UFP) - quantiles from previous year of data 2.5 *75th percentile of the data (put in Derek’s explanation) this is for integrated air toxics data 3*SD (Poor performance) Site mean more than twice the second highest site mean(NO) Sites with maxima over 2X the next highest value(NO) Only one detection at a site (special case) And Detection was greater than 3X the MDL And Detection was within 10% of state maximum that year Possible Sample loss issues (leaks) by site and pollutant Check for several (2,3) below MDL values when values have been above MDLs previously (need discussion). MDL would be from the previous year. I tried taking the mean of 5 sequential values, and then the next 5 sequential values. If the second set of means was 1.5 times less than the first mean I flagged the values. Recommendation: Use the [75th percentile * 3] for all pollutants. Sample R script Click the button below to view a step by step example of the methods above. Show R code Goals: Find extreme values that may be in error or from contamination. Find 3 repeating values that are exactly the same. Find low values where this is not common. Do this in a routine way (identify frequency of validation checks). The following sections on data validation will use the example monitoring data loaded below. library(tidyverse) library(stringr) library(RcppRoll) library(lubridate) library(car) ##need to confirm that this data set has sequential repeats, some high values and at least 10 dates with numbers data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) Table: Sample monitoring data 2.1 Identifying Instrument drift or leaks in a system. The test looks for a difference in variance of carbon tetrachloride between a calendar quarter for two consecutive years. Carbon tetrachloride was chosen because it is a banned substance and no longer in use, it has very few below detection limit values, and no direct sources. It has seasonally variable concentrations, so calendar quarters were compared to eliminate the detection of seasonal differences. The statistical test used was a Levenes Test for homogeneity of variance. Sample R script Click the button below to view a step by step example of the method above. Show R code data$conc &lt;- as.numeric(data$conc) data$aqs_id &lt;- as.character(data$aqs_id) sites &lt;- unique(data$aqs_id) years &lt;- unique(data$year) data$quarter = quarter(data$date) quarters &lt;- unique(data$quarter) pocs &lt;- unique(data$poc) clean_values = function(data) { data$conc = as.numeric(as.character(data$conc)) data$conc[abs(data$conc) &gt;= 999] = NA return(data) } data = clean_values(data) levene_function = function(conc, quarter_year, row, col) { if(length(unique(quarter_year))&lt;2){ return(NA)} data = data.frame(conc = conc, quarter_year = quarter_year) return(leveneTest(conc~as.factor(quarter_year), data = data)[row,col]) } leak_table_unfiltered &lt;- data.frame() for(i in max(years):max(years)-1:length(years)){ for(j in i+1){ data_carbontet=data.frame() data_carbontet &lt;- filter(data, pollutant==&quot;Carbon Tetrachloride&quot;, !is.na(conc), year %in% c(i, j)) data_carbontet$quarter_year = paste(data_carbontet$quarter, &quot;_&quot;, data_carbontet$year) data_carbontet &lt;- data_carbontet %&gt;% group_by(aqs_id, poc, quarter) %&gt;% summarise(fvalue_levene = levene_function(conc, quarter_year, 1,2), pval_levene = levene_function(conc, quarter_year, 1,3), deg_free_levene = levene_function(conc, quarter_year, 2,1), Year_1 = min(year), Year_2 = max(year), Mean_Year_1 = mean(conc[year==min(year)], na.rm=T), Mean_Year_2 = mean(conc[year==max(year)], na.rm=T)) %&gt;% ungroup() leak_table_unfiltered &lt;- rbind(leak_table_unfiltered,data_carbontet) } } leak_table &lt;- filter(leak_table_unfiltered, pval_levene&lt;0.01, abs(Year_1-Year_2)==1) leak_table$Warning_Type &lt;- &quot;Decrease_in_Measurements&quot; datatable(head(leak_table, 10), options = dt_options) %&gt;% formatSignif(c(&quot;fvalue_levene&quot;, &quot;pval_levene&quot;, &quot;Mean_Year_1&quot;, &quot;Mean_Year_2&quot;), digits = 2) 2.2 Identify exceptional and extreme values aka outliers. These are detected by comparing each measured concentration to 3 X the 75th percentile of the data set by year, site, and pollutant. For now, pocs collocated measurements are not tested separately. Sample R script Click the button below to view a step by step example of the method above. Show R code # Test for exceptionally high values [above 75th percentile X 3] high_data &lt;- group_by(data, year, aqs_id, pollutant) %&gt;% mutate(AR_Mean = mean(conc, na.rm=T), Percentile_75 = quantile(conc, 0.75, na.rm=T), Percentile_75_X3 = Percentile_75*3) %&gt;% ungroup() high_data &lt;- filter(high_data, conc&gt;Percentile_75_X3, !is.na(conc), AR_Mean&gt;md_limit, Percentile_75&gt;0) high_data$Warning_Type &lt;- &quot;Exceptionally_High_Value_Test&quot; high_data &lt;- unique(high_data) datatable(head(high_data, 10), options = dt_options) high_data &lt;- high_data[, c(&quot;date&quot;, &quot;pollutant&quot;, &quot;aqs_id&quot;, &quot;Warning_Type&quot;)] ##Toxics - Multiplying by 3 returns ~4000/1176818 values as extreme, multiplying by 1.5 returns &gt;20,000. Multiplying by 3 returns between 2 and 45% of analyte/site/year groups. ##UFP - Multiplying by 3 returns 28894 of 1335083 total values. Multiplying by 3 returns between 2 and 15% of analyte/site/year groups. ##PAHs - Multiplying by 3 returns 168 of 39298 total values. Multiplying by 3 returns between 3 and 10% of analyte/site/year groups. 2.2 Identify sequential repeats aka “sticky” numbers Three sequential replicate values may be a result of a machine error. # Test for exceptionally high values [above 75th percentile X 3] repeat_data &lt;- group_by(data, poc, year, aqs_id, pollutant) %&gt;% arrange(year, aqs_id, poc, pollutant, date) %&gt;% mutate(Previous_Day = lag(conc, 1), Two_Days_Prior = lag(conc, 2)) %&gt;% ungroup() repeat_data &lt;- mutate(repeat_data, Repeat_Test = ifelse(round(conc, digits=4)==round(Previous_Day, digits=4) &amp; round(Previous_Day, digits=4)==round(Two_Days_Prior, digits=4), &quot;TRUE&quot;, &quot;FALSE&quot;)) repeat_data$Warning_Type &lt;- &quot;Repeat_Test&quot; repeat_data &lt;- filter(repeat_data, conc&gt;0 &amp; Repeat_Test==TRUE) datatable(head(repeat_data, 10), options = dt_options) repeat_data &lt;- repeat_data[, c(&quot;date&quot;, &quot;pollutant&quot;, &quot;aqs_id&quot;, &quot;Warning_Type&quot;)] ###There are many repeating values in the Toxics data. None in PAHs. Approximately 3200 for UFPs, but this is 15 minute data. We may need a more stringent test for UFPs. Table: Dates and pollutants to check including the potential issue Contributors Kristie Ellickson, Cassie McMahon, Dorian Kvale, Derek Nagel References USEPA Technical Support Document for the Nation Air Toxics Trends Sites European Union Guide toe Data Validation [US EPA Guidance on Data Verification and Data Validation] (https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf) [US EPA Data Validation Workbook Presentation and Training Materials] (https://www3.epa.gov/ttnamti1/files/ambient/airtox/workbook/T-Workbook_Secs1-8.pdf) [US EPA Data Validation Workbook] (https://nepis.epa.gov/Exe/ZyNET.exe/P1006PAB.TXT?ZyActionD=ZyDocument&amp;Client=EPA&amp;Index=2006+Thru+2010&amp;Docs=&amp;Query=&amp;Time=&amp;EndTime=&amp;SearchMethod=1&amp;TocRestrict=n&amp;Toc=&amp;TocEntry=&amp;QField=&amp;QFieldYear=&amp;QFieldMonth=&amp;QFieldDay=&amp;IntQFieldOp=0&amp;ExtQFieldOp=0&amp;XmlQuery=&amp;File=D%3A%5Czyfiles%5CIndex%20Data%5C06thru10%5CTxt%5C00000016%5CP1006PAB.txt&amp;User=ANONYMOUS&amp;Password=anonymous&amp;SortMethod=h%7C-&amp;MaximumDocuments=1&amp;FuzzyDegree=0&amp;ImageQuality=r75g8/r75g8/x150y150g16/i425&amp;Display=hpfr&amp;DefSeekPage=x&amp;SearchBack=ZyActionL&amp;Back=ZyActionS&amp;BackDesc=Results%20page&amp;MaximumPages=1&amp;ZyEntry=1&amp;SeekPage=x&amp;ZyPURL) [Review article by Helsel, D about non-detects and substitution methods.] (https://academic.oup.com/annweh/article/54/3/257/223531/Much-Ado-About-Next-to-Nothing-Incorporating) [A Chemosphere review article describing why BDL substitution should not be done.] (https://pdfs.semanticscholar.org/182e/9278fc36dd73d48a414b8fbc30a44ea314d3.pdf) [Temporal variability of selected air toxics in the United States] (http://www.sciencedirect.com/science/article/pii/S1352231007004840) [Spatial and temporal analysis of national air toxics data] (http://www.tandfonline.com/doi/pdf/10.1080/10473289.2006.10464576) "],
["collocated-monitors-derek.html", " Collocated monitors (Derek) 3.1 Evaluate collocated air monitors (POCs) 3.2 Prioritize multiple air monitors (POCs)", " Collocated monitors (Derek) Analysis steps for collocated monitors include: 3.1 Evaluate sites with collocated monitors (POCs) 3.2 Prioritize observations from collocated monitors (POCs) 3.1 Evaluate collocated air monitors (POCs) Description Collocated samples are multiple samples taken for a pollutant from the same monitoring site at the same time. Collocated samples provide an extra layer of quality assurance by taking multiple measurements under very similar conditions which helps detect machine, lab, and human error. Depending on the results from collocated monitors, only one of the monitors, or potentially neither may be considered as valid measurements. Recommended steps Viewing a scatterplot of collocated measurements is suggested as it helps to view both obvious and subtle irregularities between collocated measurements. If there is evidence of bias in one of the monitors (values from one collocated monitor are usually higher than the other), or there is large variance between the collocated monitors (values differ from each other by significant margins), then both collocated monitors need to be investigated further. EPA recommends that collocated monitors have no higher than a 15% coefficient of variation between their measurements. Calculating the CV for every measurement pair does not make sense as small differences (i.e. 0.001 and 0.002) have the same CV as large differences (i.e. 10 and 20). Therefore, we chose to look at the median CV for each site, pollutant, and year then flag collocated monitors if the median CV is greater than 15 which suggests differences between the collocated monitors are systematic. We only calculate the CV for collocated monitors on days which both have valid measurements greater than or equal to the MDL since there is lower confidence in values below the MDL. We only compare the median CV to the threshold of 15 if there are at least 10 calculated CV values since it does not make sense to flag monitors where only a few values are above detection. If collocated monitors do not agree, then consult with the lab or quality assurance team to determine if there is a problem with one of the POCs. If they determine that one of POCs is reliable and the other is not, then only use data from the reliable POC. If they cannot determine which POC is the problem, then confidence in the measurements from both collocated monitors is reduced and no measurements taken from either collocated monitor should be used. Notes Identify bias Pairwise t-test for the POC means? Look at quantiles of differences? Who should we notify if one POC is higher than the other? Develop a process for this potentially in Air Vision with email notifications to field and lab. Default option if one is biased higher than the other: Go with higher POC? Go with lower POC? Use mean? Exclude both POCs? This might be pollutant dependent. Carbon tetrachloride, for example, we can determine which monitor is correct. Formaldehyde is stable enough to somewhat know what is correct. Maybe this is true for 1,3 butadiene too. Otherwise, take the mean. Exclude extreme values for the POC or all values for the POC? Need input from laboratory or see the decision criteria above. Variance: Minimum correlation for both POCs to be valid? Possibly use a R2 of 0.99 (2 significant digits). May require unique COV for each pollutant or pollutant group. One value above detection, one value below? Derek suggests going with the value above detection. We don’t want to do substitutions unless we have to, so if there are not any problems with the POC above detection, then use the actual measured value. (EPA Data Analysis Workbook?) One value above acute IHB and one value below? First check validity of monitors to see if there are problems with one of the monitors. If both POCs seem valid, use the mean? (I suggest this because I think it’s the most defensible thing to do. The true concentration is likely between the POCs and the mean is the most unbiased estimate given the 2 POC measurements. If one POC is well above the IHB and one is just barely below, then the true concentration is likely above the IHB. If one POC is just barely above the IHB and the other is well below, then the true concentration is likely below the IHB. We could go with the higher POC if we want to be conservative.) Pull the highest POC for the daily maximum and the annual second high comparison to acute IHB. Take the mean of the POCS for the UCL-95, assuming both are valid values. Sample R script POC_check generates scatterplots comparing all POCs in the data. Data must have columns with “AQS_ID”, “POC”, “Param_Code”, “Date”,“Concentration”, “Null_Data_Code”, “MDL”, “Pollutant”, “Year”, “CAS” in order from left to right. POC_check = function(data) { library(tidyverse) library(lubridate) POC_ratio = function(Concentration.x, Concentration.y, MDL) { if(!is.na(Concentration.x) &amp; Concentration.x &gt; MDL &amp; !is.na(Concentration.y) &amp; Concentration.y &gt; MDL) { return (Concentration.y / Concentration.x ) } else { return (NA) } } POC_diff = function(Concentration.x, Concentration.y, MDL) { return( ifelse(!is.na(Concentration.x) &amp; Concentration.x &gt; MDL &amp; !is.na(Concentration.y) &amp; Concentration.y &gt; MDL, Concentration.y - Concentration.x, NA) ) } POC_avg = function(Concentration.x, Concentration.y, MDL) { return( ifelse(!is.na(Concentration.x) &amp; Concentration.x &gt; MDL &amp; !is.na(Concentration.y) &amp; Concentration.y &gt; MDL, (Concentration.y + Concentration.x) / 2 , NA) ) } POC_sd = function(Concentration.x, Concentration.y, MDL) { return( ifelse(!is.na(Concentration.x) &amp; Concentration.x &gt; MDL &amp; !is.na(Concentration.y) &amp; Concentration.y &gt; MDL, apply(rbind(Concentration.x, Concentration.y), 2, function(x) sd(x, na.rm = T) ), NA) ) } names(data)[1:10] &lt;- c(&quot;AQS_ID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Concentration&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data = select(data, AQS_ID, POC, Date, Concentration, MDL, Pollutant, Year) data = distinct(data) POCs = filter(data, POC &gt; 1) data = filter(data, POC == 1) POCs = full_join(data, POCs, by = c(&quot;AQS_ID&quot;, &quot;Pollutant&quot;, &quot;Date&quot;, &quot;MDL&quot;, &quot;Year&quot;)) POC_differences = POCs %&gt;% mutate(difference = POC_diff(Concentration.x, Concentration.y, MDL), average = POC_avg(Concentration.x, Concentration.y, MDL), sd = POC_sd(Concentration.x, Concentration.y, MDL) ) POC_summary = POC_differences %&gt;% mutate(CV = sd/average * 100) POC_flag = POC_summary %&gt;% group_by(AQS_ID, Pollutant, Year) %&gt;% summarise(median_CV = median(CV, na.rm = T), n = sum(!is.na(CV) ) ) %&gt;% mutate(flag = median_CV &gt; 15 &amp; n &gt;= 10) POCs = POCs %&gt;% mutate(Conc = Concentration.y / Concentration.x * (Concentration.x &gt; MDL) * (Concentration.y &gt; MDL)) POCs = filter(POCs, !is.na(Conc) &amp; Conc != 0) POCs$Unique &lt;- paste(POCs$AQS_ID, POCs$Pollutant) POCs$Date = as.Date(POCs$Date) POC_plot = ggplot(POCs, aes(Date, Conc)) + geom_point() + scale_x_date() + scale_y_log10() + facet_wrap ( ~ Unique) + ylab(&quot;POC 2 / POC 1 (log scale&quot;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) return(list(POC_flag, POC_plot) ) } Contributors Derek References 3.2 Prioritize multiple air monitors (POCs) Description If collocated monitors at a site are both considered valid, then a single value must be chosen from multiple collocated values to avoid double-counting values at a single site. Recommended steps For air toxics, measurements from collocated monitors are considered equally valid unless there is a specific reason to treat them differently. Therefore, these methods are used for dealing with collocated samples. If one monitor has a valid measurement, and the other does not, then the valid measurement is used. If one monitor has a valid measurement greater than or equal to the MDL and the other has a valid measurement less than the MDL, then the measurement above the MDL is used as a conservative approach as there is greater confidence in the value above the MDL. If both monitors have valid measurements above the MDL, then those values are averaged since both values are equally valid and the “true” value is assumed to likely be between the two values. Sample R script Averages POCs. All POCs with valid measurements &gt;= MDL are averaged. If no POC has a valid measurement &gt;= MDL, then all valid measurements are averaged (result will be &lt; MDL and estimated using MLE approximation). If there are no valid measurements, then result will be NA and filtered out later. Data must have columns with “AQS_ID”, “POC”, “Param_Code”, “Date”,“Concentration”, “Null_Data_Code”, “MDL”, “Pollutant”, “Year”, “CAS” in order from left to right. POC_average = function(data) { library(dplyr) library(tidyr) POC_averaging = function(Concentration, MDL) { if(all(Concentration &lt; MDL)) { return (mean(Concentration, na.rm = T)) } else { return (mean(Concentration[Concentration &gt; MDL], na.rm = T ) ) } } names(data)[1:10] &lt;- c(&quot;AQS_ID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Concentration&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data = data %&gt;% group_by(AQS_ID, Param_Code, Date, MDL, Pollutant, Year, CAS) %&gt;% summarise(Concentration = POC_averaging(Concentration, MDL) ) return (select(data, AQS_ID, Param_Code, Date, Concentration, MDL, Pollutant, Year, CAS) ) return(data) } Contributors Derek References "],
["detection-limits-kristie-cassie.html", " Detection limits (Kristie &amp; Cassie) 4.1 Method Detection Limit - Definition 4.2 Procedure for calculating method detection limits 4.3 Finding detection limits (Cassie) 4.4 Qualifier Codes for Detection Limits 4.5 Below detection values (Kristie) 4.6 Multiple detection limits", " Detection limits (Kristie &amp; Cassie) Detection limit methods. Notes Types of detection limits (what detection limit the lab calculates, where it is, where it is not and when not to use what lims has) [Cassie] Does a value equal to the detection limit count as detected? “YES!” Things to consider when there are more than one detection limit per year. [Cassie] Discuss averaging two detection limit years and using that instead of the previous detection limit. [Dorian] Estimating below detection values (what not to do) [Kristie]: Don’t delete Deleting data that are below a detection limit biases central tendency estimates and whole data summaries (quantiles of the data set) high. Don’t fill with zeros Substituting below detection limit data with 0’s biases central tendency estimates and whole data summaries (quantiles of the data set) low, and can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. Don’t fill with DLs or ½ DLs Substituting below detection limit data with 1/2 MDL creates the least bias in central tendency estimates and whole data summaries (quantiles of the data set), however, it can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. Substituting below detection limit data with MDLs biases central tendency estimates and whole data summaries (quantiles of the data set) high, and can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. 4.1 Method Detection Limit - Definition The method detection limit (MDL) is the minimum concentration of a substance that can be measured and reported with 99% confidence that the analyte concentration is greater than zero and is determined from analysis of a sample in a given matrix containing the analyte. 4.2 Procedure for calculating method detection limits Prior to 2017, the lab calculated detection limits according to the procedures defined in 40 CFR Appendix B to Part 136 - Definition and Procedure for the determination of Method Detection Limit, https://www.gpo.gov/fdsys/granule/CFR-2011-title40-vol23/CFR-2011-title40-vol23-part136-appB/content-detail.html Begining in 2017, the lab adopted a modified method for calcuating the method detection limit. Known as the “Method Update Rule(MUR)”, this method accounts for media background contamination when establishing the MDL. This method is defined in the NATTS TAD (October 2016),https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjJ3Y6AyfzVAhVL2IMKHXe_DI0QFggoMAA&amp;url=https%3A%2F%2Fwww3.epa.gov%2Fttnamti1%2Ffiles%2Fambient%2Fairtox%2FNATTS%2520TAD%2520Revision%25203_FINAL%2520October%25202016.pdf&amp;usg=AFQjCNGVu-9E0zS1-Gjkt_5sULzOPNeymg. EPA has proposed to revise 40 CFR Appendix B to Part 136 to reflect this methodology. Both methods require preparing and analyzing 7 matrix spikes and 7 method blanks. Samples should be prepped and analyzed over the course of at least 3 seperate batches/days. -Calculate the MDL of spiked samples (MDL_sp) Calculate standard deviation of the calcuated concentrations for the spiked samples. Calculate the MDL for the spiked samples by multiplying the standard deviation of spiked sample by the one-sided student’s T value at 99% confidence corresponding to the number of spikes analyzed (3.143 if 7 samples). -New method requires calculation of the MDL of the method blanks(MDL_b) - If no method blank provides a numerical result, no further action needed. - If method blank pool includes a combination of ND and numeric values, set the method blank MDL to equal the highest of the method blank results. - IF all concentration values for the method blank pool are numeric values, calculate the MDL_b as follows: - Calculate the average concentration of method blanks - Calculate the standard deviation of the method blank concentrations - Multiply the standard deviation by the one-sided student’s T value at 99% confidence corresponding to the number of blanks analyzed -Calculate the MDL_b as the sum of the average concentration method blanks, and the product of the standard deviation of blanks and the associated student’s T Value - MDL_b= mean_blanks + stdv_blanks * Student T - Compare MDL_sp and MDL_b. The higher of the two values is reported as the laboratory MDL. 4.3 Finding detection limits (Cassie) Annual method detection limits used for data analysis are stored on the x:drive, “X:\\Databases\\AQ\\AQ Lab\\Data Analysis\\Air Toxics\\Detection Limits\\Annual Detection Limits (micrograms).xlsx” Use ParamYear field to join data Detection limits are updated when received from the lab. Current year samples may not have a detection limit in the file if the lab has not performed the test. When Promium is adopted, method detection limits will be applied on a sample basis and will account for the varying air volumes of samples. As a result, there will be multiple detection limits for an analyte. 4.4 Qualifier Codes for Detection Limits Begining in 2017, the MPCA will be linking laboratory detection limits to sample data and applying appropriate data qualifiers depending on detection status. These qualifiers include: ND = HAP not qualitatively identified (reported as zero) MD = &lt; MDL SQ = &gt;= MDL but &lt; SQL No qualifier = measured concentration &gt;= to SQL The SQL is 3.18x the MDL 4.5 Below detection values (Kristie) Description Recommended steps Sample R script Example charts for visualizing data below detection limits library(tidyverse) library(stringr) library(lubridate) library(car) library(DT) library(shiny) library(ggplot2) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) Table: Sample monitoring data Shiny tool for displaying non-detects Shiny code pollutants &lt;- unique(data$pollutant) years &lt;- unique(data$year) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = pollutants, selected=&quot;Benzene&quot;), selectInput(&quot;year&quot;, label=&quot;Choose a year&quot;, choices = years, selected=2009))), column(9, plotOutput(&#39;detlim&#39;, height = &quot;400px&quot;)))), server = function(input, output) { data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) output$detlim &lt;- renderPlot({ print(input$year) print(input$pollutant) data_sub = filter(data, year==input$year, pollutant==input$pollutant) mdl &lt;- mean(data_sub$md_limit) data_sub$Censored &lt;- ifelse(data_sub$conc &gt; data_sub$md_limit, FALSE, TRUE) ggplot(data=data_sub, aes(x= factor(aqs_id), y=conc)) + scale_colour_manual(values= c(&quot;#197519&quot;[FALSE %in% unique(data_sub$Censored)], &quot;#0000FF&quot;[TRUE %in% unique(data_sub$Censored)]), breaks=c(FALSE, TRUE)) + geom_boxplot(outlier.colour=NA) + geom_jitter(aes(color=Censored), size =2.4, alpha=0.55) + geom_hline(yintercept=mdl) + xlab(NULL) + ylab(&quot;Result (ug/m3)&quot;) + expand_limits(y=c(0, max(data_sub$conc))) + theme(text = element_text(size=15), axis.text.x = element_text(angle = -90, vjust = 0.3, size=14)) + ggtitle(paste0(&quot;Site Comparison Boxplot with Censored and Non-Censored Values for &quot;, input$pollutant, &quot;, from &quot;, input$year), subtitle = &quot;---- Horizontal Line ---- = Detection Limit&quot;) }) }) Contributors References 4.6 Multiple detection limits In progress. "],
["completeness-checks-dorian-cassie.html", " Completeness checks (Dorian &amp; Cassie) 5.1 Completeness checks", " Completeness checks (Dorian &amp; Cassie) We should check for data completeness before generating summaries for sampling data. Completeness checks inlcude tests for: Seasonal completeness (75%) This is an EPA guideline for criteria pollutants and has been applied to air toxics (citation). 20% values above detection This is based on Cox 2006 and MPCA simulations (cite work and results) Label for criteria not met “Criteria to calculate annual mean not met.” More than 3 unique values (cite this) 3 per year? 3 per quarter? Minimum samples. What can we do for incomplete seasons or years: Compare to acute? Summarize based on number of values above detection? (Cassie did this) Annual reporting rules 5.1 Completeness checks Methods for performing completeness checks… Packages library(tidyverse) Our example data is organized by monitoring site and date. Here’s a sample. AQS_ID Date Conc 270535501 2009-07-30 0.00148 270535501 2009-09-30 0.00064 270535501 2009-11-30 0.34256 270535501 2009-12-30 0.00064 270535502 2009-03-30 0.26219 270535502 2009-07-30 0.01113 270535502 2009-09-30 0.00044 270535502 2009-11-30 0.00127 270535502 2009-12-30 0.00113 "],
["summary-statistics.html", " Summary statistics 6.1 Bootstrapping (Dorian) 6.2 Confidence intervals (Derek)", " Summary statistics The summary methods described in this chapter inlcude: 6.1 Bootstrapping 6.2 Confidence intervals Notes Means When to use arithmetic / geometric Use an arithmetic mean when your data are normal and there are less than 5% below detection limit values per year/analyte/site [I made up the 5%, but there is an EPA document looking at the impacts of BDL data we can cite and incorporate. I’ll look for it] generic test for normality AND 5% BDL. For non-automated scripts: Use a geometric mean if your data are log-normally distributed and there are less than 5% below detection limit values per year/analyte/site. 6.1 Bootstrapping (Dorian) Bootstrapping provides methods for calculating summary statistics without making assumptions about whether the data is sampled from a normal dirstribution. Packages library(dplyr) library(readr) Our data is organized by monitoring site and date. Here’s a sample. AQS_ID Date Conc 270535501 2009-07-30 0.00148 270535501 2009-09-30 0.00064 270535501 2009-11-30 0.34256 270535501 2009-12-30 0.00064 270535502 2009-03-30 0.26219 270535502 2009-07-30 0.01113 270535502 2009-09-30 0.00044 270535502 2009-11-30 0.00127 270535502 2009-12-30 0.00113 Bootstrap function We currently use the EnvStats package to generate our summary values. It has built in functions to accunt for non-detect data and allows for different distributions. However, if you’re not dealing with non-detects you can use a simple loop to boot things yourself. Before you start you’ll want to set the random number generator to ensure you’ll be able to reproduce your results. I’ll use #27 below. set.seed(27) The general idea is to take a random sample from the data set, generate the statistic that you’re interested in, record it, then rinse and repeat. Below is the code for how to resample a single site. # Filter data to a single site df_site1 &lt;- filter(df, AQS_ID == AQS_ID[1]) # Pull random sample # `replace=T` allows for the same value to be pulled multiple times # `size=nrow(df)` ensures the number of observations in the new table to match the original random_df &lt;- sample(df_site1$Conc, replace=T) # Generate summary statistic quantile(random_df, 0.1) ## 10% ## 0.00064 To repeat this 3,000 times we can wrap these steps into a resample function, and then use sapply to collect the results. # Create resample function resample_Conc &lt;- function(data= df_site1$Conc, Conc_pct= 0.10){ random_df &lt;- sample(data, replace=T) quantile(random_df, Conc_pct, na.rm=T)[[1]] } # Repeat using `sapply` repeats &lt;- 3000 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(df_site1$Conc)) # The 50th percentile or median Conc median(booted_10pct, na.rm=T) ## [1] 0.00064 # Return the 95th percentile of the booted concentrations quantile(booted_10pct, 0.95, na.rm=T)[[1]] ## [1] 0.00148 # Force the 95th percentile to be a recorded value sort(booted_10pct)[repeats*.95 +1] ## [1] 0.00148 # Upper and lower confidence interval around the median quantile(booted_10pct, c(0.025, 0.5, 0.975), na.rm=T) ## 2.5% 50% 97.5% ## 0.000640 0.000640 0.103216 Automate To finish up, throw these steps into your personal boot function, and then run it on each site using group_by. # Create boot function boot_low_Conc &lt;- function(data=df$Conc, Conc_pct=0.10, conf_int=0.95, repeats=3000){ alpha &lt;- (1 - conf_int)/2 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(data, Conc_pct)) # Upper and lower confidence interval around the median list(quantile(booted_10pct, c(alpha, 0.5, 1-alpha), na.rm=T)) } # Use `group_by` to send data for each site to your boot function conc_summary &lt;- group_by(df, AQS_ID) %&gt;% mutate(boot_results = boot_low_Conc(Conc, Conc_pct=0.10, conf_int=0.95)) %&gt;% summarize(Conc_10pct = quantile(Conc, 0.10, na.rm=T)[[1]], Low_CL95_conc = unlist(boot_results[1])[[1]], Boot_conc = unlist(boot_results[1])[[2]], Upper_CL95_conc = unlist(boot_results[1])[[3]]) Results The booted confidence limits AQS_ID Conc_10pct Low_CL95_conc Boot_conc Upper_CL95_conc 270535501 0.000640 0.00064 0.000640 0.103216 270535502 0.000716 0.00044 0.000716 0.005214 6.2 Confidence intervals (Derek) "],
["site-comparisons-derek-cassie.html", " Site comparisons (Derek &amp; Cassie) 7.1 Confidence intervals 7.2 Correlation matrices (Cassie)", " Site comparisons (Derek &amp; Cassie) Notes Steps: Chart data with box and whisker plots. Some site differences might be very apparent and not require statistical comparisons. [Need to think about what is apparent enough for a visual test, maybe no overlap of box, or only half or less overlap] look at cenboxplot with NADA packages, or use Kristies boxplot with censored values. Check for normality Chi-squared test (Is normality a valid assumption?). If the data are normal, and there is equal variance [Levenes test–generic script here] complete an ANOVA to compare sites. If there are more than 2 sites, follow with a post hoc test [I say Tukey, but Derek weigh in here]. If the data are not normal, and there is not equal variance [see generic script from above] complete a Kruskal wallis test (but there are other ones if you have &lt; or &gt; 50 samples…look into this). If there are more than 2 sites to compare, use Dunns test for post hoc site by site comparisons [Kristie has a script for this, will make it generic]. Bayesian ANOVA [Derek likes this!] Correlation matrices (Cassie likes these) 7.1 Confidence intervals Some significant applications are demonstrated in this chapter. 7.2 Correlation matrices (Cassie) "],
["charts.html", " Charts 8.1 Boxplots (Dorian) 8.2 Calendar plots (Cassie) 8.3 Colors (Dorian) 8.4 openair package (Derek &amp; Cassie)", " Charts 8.1 Boxplots (Dorian) Dividing by zero is an amazing super power. Unfortunately for you your computer does not have this power. Making a log boxplot with data containing zeros or negative values will result in your computer melting itself. It will never forgive you. 8.1.1 Log boxplot Coming soon… 8.1.2 Outliers Coming soon… 8.2 Calendar plots (Cassie) We have created a nice plot. 8.3 Colors (Dorian) Please use Ghibli colors. See Ghibli. Ghibli colors 8.4 openair package (Derek &amp; Cassie) Coming soon… 8.4.1 Pollution roses and love poems (Derek) Pollution roses are a visual display of pollutant concentrations and wind directions correspnding to those concentrations. The length of each “paddle” is correlated with the percentage of days of valid measurements taken when the wind was blowing from that direction. So if the longest paddle the one extending upward, then the wind blew from the North on average more times than any other direction for days with valid measurements of a pollutant. The colors on a paddle correspond to the concentration of the pollutant. Blue means lower concentrations up to red which are the highest concentrations. Note that for 24-hour samples, wind direction is the vector averaged wind direction of each hour in a day. Sample R script # Generates pollution roses for all air toxics for a site and saves them in a folder defined by output_path #Currently does not address POCs. Only use for single POC sites until fixed. #Currently only works for single site. Multi-site capability may be added in future. #------------------------------------------------------------------------------------------------ #Enter all values between the dashed lines # Monitoring data must be a csv in ATDE format with columns &quot;AQS_ID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Concentration&quot;, # &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot; # Date needs to be in yyyy-mm-dd format or yyyymmdd #met_data needs to be filtered to site of interest (pull from Tableau) monitoring_data_file = &quot;.csv&quot; #complete path of csv file with monitoring data USE FORWARD SLASHES / met_data_file = &quot;.csv&quot; #complete path of csv file with met data USE FORWARD SLASHES / output_path = &quot;&quot; #path to output folder USE FORWARD SLASHES / site_num = 123456789 # site number: enter complete 9 digit AQS ID without dashes site_name = &quot;&quot; #Enter site name in quotes as you want it to appear on the title of roses use \\n for new line png_height = 600 #Image height png_width = 600 #Image width num_breaks = 5 #number of colors desired on pollution rose. Breaks are minimum to MDL, then (num_breaks - 1) evenly spaced intervals from MDL to maximum value for pollutant. #------------------------------------------------------------------------------------------------- library(dplyr) library(lubridate) library(tidyr) library(openair) library(reshape) site_save = gsub(&quot;[.]&quot;, &quot;&quot;, site_name) site_save = gsub(&quot;\\n&quot;, &quot;&quot;, site_save) site_save = gsub(&quot;/&quot;, &quot;-&quot;, site_save) data = read.csv(monitoring_data_file, stringsAsFactors = F) names(data)[1:10] &lt;- c(&quot;AQS_ID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Concentration&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data$date = ymd(data$Date) data$Pollutant = gsub(&quot;/&quot;, &quot;-&quot;, data$Pollutant) #R confuses forward slashes for file paths when naming pngs met_data = read.csv(met_data_file) data_site = filter(data, AQS_ID %in% site_num &amp; !is.na(Concentration) ) data_site = data_site %&gt;% group_by(Pollutant) %&gt;% filter(max(Concentration) &gt; 0.001 &amp; max(Concentration) &gt; max(MDL)) pollutants_site = data_site %&gt;% group_by(Pollutant) %&gt;% summarise(MDL = max(MDL), minimum = min(Concentration), maximum = max(Concentration)) data_site = select(data_site, date, Pollutant, Concentration) breaks_site = NULL for (i in 1:nrow(pollutants_site)) { breaks_site = rbind(breaks_site, c(round_any(pollutants_site$minimum[i], 0.001, floor), round_any( c(pollutants_site$MDL[i], pollutants_site$MDL[i] + (pollutants_site$maximum[i] - pollutants_site$MDL[i]) * (1:(num_breaks-1) / (num_breaks-1) ) ), 0.001, ceiling ) ) ) } data_site = spread(data_site, key = Pollutant, value = Concentration) names(met_data)[c(1,7,8)] = c(&quot;Day&quot;,&quot;wd&quot;,&quot;ws&quot;) met_data = mutate(met_data, date = paste0(Year,&quot;/&quot;,Month,&quot;/&quot;,Day,&quot;&quot;,Hour,&quot;:00&quot;)) met_data$date = ymd_hm(met_data$date) met_data = met_data[,-c(1:3, 9:11)] met_data = timeAverage(met_data, avg.time = &quot;day&quot;) met_data$date = ymd(met_data$date) data_site = left_join(data_site, met_data, by = &quot;date&quot;) png(paste0(output_path,&quot;/Wind Rose for &quot;, site_save,&quot;.png&quot;), height = png_height, width=png_width) windRose(data_site, paddle = F, breaks = 5 * 0:5, key.footer = &quot;mph&quot;, main=paste(&quot;Daily Average Wind Rose \\n&quot;, site_name) ) dev.off() for(i in 1:nrow(pollutants_site)) { png(paste0(output_path,&quot;/Pollution Rose for &quot;, pollutants_site$Pollutant[i],&quot; &quot;, eight), width=png_width) pollutionRose(data_site, statistic = &quot;abs.count&quot;, pollutant = pollutants_site$Pollutant[i], breaks = breaks_site[i,], key.footer=&quot;ug/m3&quot;, main=paste(&quot;Daily Average Pollution Rose for&quot;, pollutants_site$Pollutant[i],&quot;\\n&quot;, site_name) ) dev.off() } ## ?? site_save,&quot;.png&quot;), height=png_h "],
["time-series.html", " Time series 9.1 Seasonality 9.2 Changepoints (before &amp; after tests)", " Time series 9.1 Seasonality The trend is up. 9.2 Changepoints (before &amp; after tests) Roll a dice. "],
["maps-kristie.html", " Maps (Kristie) 10.1 Kriging 10.2 Spatial averaging and aggregation 10.3 Creating shapefiles in R adn joining to EJ areas", " Maps (Kristie) 10.1 Kriging Coming soon… 10.2 Spatial averaging and aggregation To average across… 10.3 Creating shapefiles in R adn joining to EJ areas I don’t really remember the points of this. This bit of script creates a shapefile from air monitoring data with coordinates and then joins to the EJ status for census tracts. Let’s discuss how to make this useful. library(tidyverse) library(stringr) library(RcppRoll) library(lubridate) library(car) library(DT) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) ######################################################################## ## Spatial join to census tracts and then left_join to EJ areas status # ######################################################################## coords &lt;- monitoring_locations[, c(&quot;Longitude&quot;, &quot;Latitude&quot;)] point_source_wypoints &lt;- SpatialPointsDataFrame(coords, data=data, proj4string = CRS(&#39;+proj=longlat +ellps=WGS84&#39;)) point_source_wypoints &lt;- spTransform(point_source_wypoints, CRS(&#39;+proj=utm +zone=15 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0&#39;)) census_tracts &lt;- readOGR(dsn=&quot;R:/demographics&quot;, layer=&quot;census_2010_tracts_usboc&quot;, stringsAsFactors = FALSE) point_sources_files_geo &lt;- point.in.poly(point_source_wypoints, census_tracts) writeOGR(obj=point_sources_files_geo, dsn=&quot;X:/Programs/Air_Quality_Programs/Air Monitoring Data and Risks/0 Methods and documentation/3. Analysis methods/Web book/air-methods&quot;, layer=&quot;airmonitors_tracts&quot;, driver=&quot;ESRI Shapefile&quot;) point_sources_tracts_dbf &lt;- read.dbf(&quot;X:/Programs/Air_Quality_Programs/Air Monitoring Data and Risks/0 Methods and documentation/3. Analysis methods/Web book/air-methods/airmonitors_tracts.dbf&quot;, as.is=TRUE) ej_layers &lt;- read.dbf(&quot;X:/Agency_Files/EJ/GIS/Shapefiles/ACS_2014_5Yr_Tract_MNPovPPC.dbf&quot;, as.is=TRUE) ej_layers &lt;- ej_layers[, c(&quot;GEOID&quot;, &quot;ov50per_nw&quot;, &quot;prp_under1&quot;)] point_sources_tracts_ej &lt;- left_join(point_sources_tracts_dbf, ej_layers, by=c(&quot;GEOID10&quot;=&quot;GEOID&quot;)) names(point_sources_tracts_ej) &lt;- c(&quot;Facility_ID&quot;, &quot;Facility_Name&quot;, &quot;Resident_Cancer_Risk&quot;, &quot;Resident_Hazard_Quotient&quot;, &quot;CAS&quot;, &quot;Pollutant&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;Annual_PM25_Concentration&quot;, &quot;STATEFP&quot;, &quot;COUNTYF&quot;, &quot;TRACTCE&quot;, &quot;GEOID10&quot;, &quot;NAME10&quot;, &quot;NAMELSA&quot;, &quot;MTFCC10&quot;, &quot;FUNCSTA&quot;, &quot;ALAND10&quot;, &quot;AWATER1&quot;, &quot;INTPTLA&quot;, &quot;INTPTLO&quot;, &quot;ov50per_nw&quot;, &quot;prp_under1&quot;) #Output file name (including path) point_sources_tracts_ej &lt;- unique(point_sources_tracts_ej) "],
["additional-resources-kristie.html", " Additional resources (Kristie)", " Additional resources (Kristie) Suggested readings: https://webgate.ec.europa.eu/fpfis/mwikis/essvalidserv/images/a/ad/PRACTICAL_GUIDE_TO_DATA_VALIDATION.pdf https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf Data validation workbook – Kristie will do thing https://academic.oup.com/annweh/article/54/3/257/223531/Much-Ado-About-Next-to-Nothing-Incorporating - Review article by Helsel about non-detects and substitution methods. https://pdfs.semanticscholar.org/182e/9278fc36dd73d48a414b8fbc30a44ea314d3.pdf A Chemosphere review article describing why BDL substitution should not be done. EPA, TADD Nothing over 80% detection paper: http://www.tandfonline.com/doi/pdf/10.1080/10473289.2006.10464576 "],
["edit-this-book.html", "Edit this book", " Edit this book This guide lives online at GitHub. To make changes go to github.com/MPCA-air/air-methods and click [ Fork ]. To make suggestions go to github.com/MPCA-air/air-methods/issues and click [ New issue ]. We are using the R bookdown package (Xie 2017) in this book, which was built on top of R-Markdown and knitr (Xie 2015). Code folding reference: https://stackoverflow.com/questions/34784121/interactively-show-hide-code-r-markdown-knitr-report "]
]
