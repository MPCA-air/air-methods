[
["index.html", "Introduction", " Introduction Updated Dec 18, 2017 This guide describes the methods used at the MPCA to analyze air monitoring and modeling data. The charts and code found in this guide were produced using the freely available R statistical software. To follow along with the examples download a copy of R to your computer from the r-project or use R-Fiddle in your browser. "],
["get-data.html", " Get data 1.1 Retrieving data from AQS DataMart using RQAMD 1.2 Retrieving data from MPCA WAIR database 1.3 Current AQI obs 1.4 Current AQI observations 1.5 Retrieving data from Tableau", " Get data This section describes the location of data sources relevant to air monitoring and how to access them using R. These data sources include: Air monitoring EPA’s AQS DataMart 1.1 RQAMD Package MPCA sources 1.2 WAIR Database ?? LIMS data via Tableau ?? Airvision - Hourly data EPA’s AirNow 1.3 Current AQI observations 1.4 Active AQI monitors Tableau 1.5 Workbooks Health and standards Inhalation health benchmarks NAAQs Air modeling NATA Downscaler for Ozone and PM2.5 MNRisks CMAQ Air emissions EPA’s NEI Facility locations Meteorology and Climate Observations HYSPLIT for wind trajectories Forecasts 1.5 Tableau workbooks Geography and American Community Survey (ACS) Census ACS Traffic Land use 1.1 Retrieving data from AQS DataMart using RQAMD The AQS Data Mart provides a convenient API to access air quality data stored in the EPA’s AQS database, [AQS Data Mart] (https://aqs.epa.gov/aqsweb/documents/data_mart_welcome.html) Note: The AQS Data Mart requires a user name and password.The username and password is not the same as your AQS User Account.To request a Data Mart account, follow the instructions on the Data Mart page. The RQAMD package allows users to query the AQS Data Mart in R, [RQAMD] (https://github.com/ebailey78/raqdm) Sample R script Click the button below to view a step by step example. Show R code ##install raqdm package library(devtools) devtools::install_github(&quot;ebailey78/raqdm&quot;) require(raqdm) ##set Data Mart username and password. setAQDMuser(&quot;User Name&quot;,&quot;PW&quot;,save=TRUE) #Note save=TRUE creates a file that stores username and password locally, you will not need to run setuser info each time you load raqdm. setAQDMdefaults(pc=&quot;CRITERIA&quot;, state=&quot;27&quot;, save=TRUE) #Set defaults that are locally stored for queries. This eliminates need to define the data type and state code. ##Single Paramteer Query x &lt;- getAQDMdata(state=&quot;27&quot;,pc=&quot;CRITERIA&quot;,param=&quot;42602&quot;,format=&quot;AQCSV&quot;,bdate=&quot;20140101&quot;,edate=&quot;20141231&quot;,synchronous = FALSE) # Queries Data Mart DataBase aqcsv &lt;- getAQDMrequest(x) # Wait for email confirming file is ready. ##Multiple Parameter Loops params &lt;- c(&quot;45201&quot;, &quot;42602&quot;, &quot;44201&quot;) #Create a vector with the parameters you are interested in # Use lapply to loop through the params vector, requesting each one from AQDM. A list of requests will be returned to the x variable x &lt;- lapply(params, function(p) { return(getAQDMdata(param=p)) }) # now loop through the requests to retrieve the data y &lt;- lapply(x, function(r) { return(getAQDMrequest(r)) }) # You could then use do.call and rbind to combine them into one data.frame d &lt;- do.call(rbind, y) 1.2 Retrieving data from MPCA WAIR database The WAIR database provides a queryable local copy of select air quality data extracted multiple data sources. This database is managed by Margaret McCourtney. Contact Margaret to request login credentials. See [WAIR Data Dictionary] (http://rainier.pca.state.mn.us/documentation/DataDictionary/wair/index.html) for available data tables. Use the following code to query WAIR using DPLYR Show R code ################################################################################################ ## This script loads the library and driver and connects to WAIR. A dplyr query extracts ## data from the database into a format specified by Cassie McMahon for calculating ## OZONE DESIGN VALUES ## ## Please disconnect from database before proceeding with analysis of the data in your ## dataframe. ## ## Cassie&#39;s headings ## &quot;State.Code&quot;, &quot;County.Code&quot;, &quot;Site.ID&quot;, &quot;Parameter&quot;, &quot;POC&quot;, &quot;Sample.Duration&quot;, &quot;Unit&quot;, ## &quot;Method&quot;, &quot;Date&quot;, &quot;Start.Time&quot;, &quot;Sample.Value&quot;, &quot;NullDataCode&quot;, &quot;SamplingFrequency&quot;, ## &quot;MonitorProtocolID&quot;, &quot;Qual1&quot; ## Note: WAIR does not contain values for SamplingFrequency and MonitorProtocolID ## ## Note: dplyr does not have a command to disconnect to the database. Connection will ## terminate upon quitting R. So please do not keep (many) connections open for long periods of ## time ################################################################################################ ## Load the library library(dplyr) ## Open a connection to the database WAIR, schema AQS ## my_wair &lt;- src_postgres(dbname=&#39;wair&#39;,host=&quot;eiger&quot;,user=&quot;username&quot;,password=&quot;password&quot;, options=&quot;-c search_path=aqs&quot;) ## Reference a table, or two if combining, in the database (e.g. aqs.monitor &amp; aqs.obs_value) ## ## Select columns and filter by row ## #aqs.monitor table in WAIR ## my_monitor &lt;- filter(select(tbl(my_wair, &quot;monitor&quot;), id_mon:poc_code), stateid==27 &amp;&amp; parm_code==44201) #aqs.obs_value table in WAIR ## my_obs &lt;- filter(select(tbl(my_wair, &quot;obs_value&quot;), id_mon, dur_code, unitid, method_code, sampldate, startime, value, nulldata, qual_code), parm_code==44201 &amp;&amp; between(sampldate, &quot;2014-06-01&quot;, &quot;2014-06-07&quot;)) ## Combine monitor data with observations ## my_mn_o3 &lt;- inner_join(my_monitor, my_obs, type = &quot;inner&quot;, by = c(&quot;id_mon&quot;)) ## Arrange combined data in specified order ## Collect data into a dataframe or table (&#39;collect&#39; only works on dataframe, if don&#39;t &#39;collect&#39; ## will only get first 100,000 rows with tbl_df or tbl_dt) my_mn_o3_df &lt;- collect(my_mn_o3, arrange(my_mn_o3, stateid, cntyid, siteid, parm_code, poc_code, dur_code, unitid, method_code, sampldate, startime, value, nulldata, qual_code)) ## To store data in a data.table instead of data.frame # my_mn_o3_dt &lt;- tbl_dt(my_mn_o3_df) ## To avoid &quot;Variables not shown&quot; options(dplyr.width = Inf) ## head(my_mn_o3_df) Use the following code to query WAIR using RPostgrSQL Show R code ################################################################################################ ## This script loads the library and driver and connects to WAIR. A PostgrSQL query extracts ## data from the database into a format specified by Cassie McMahon for calculating ## OZONE DESIGN VALUES ## ## Please disconnect from database and unload the driver before proceeding with analysis of the data in your dataframe. ## ## Cassie&#39;s headings ## &quot;State.Code&quot;, &quot;County.Code&quot;, &quot;Site.ID&quot;, &quot;Parameter&quot;, &quot;POC&quot;, &quot;Sample.Duration&quot;, &quot;Unit&quot;, ## &quot;Method&quot;, &quot;Date&quot;, &quot;Start.Time&quot;, &quot;Sample.Value&quot;, &quot;NullDataCode&quot;, &quot;SamplingFrequency&quot;, ## &quot;MonitorProtocolID&quot;, &quot;Qual1&quot; ## Note: WAIR does not contain values for SamplingFrequency and MonitorProtocolID ## ################################################################################################ ## call the library library(RPostgreSQL) ## load the PostgreSQL driver drv &lt;- dbDriver(&quot;PostgreSQL&quot;) ## Open a connection con &lt;- dbConnect(drv, dbname=&quot;wair&quot;,host=&#39;eiger&#39;,user=&#39;username&#39;,password=&#39;password&#39;) #***************************** all in 1 step *************************************************** dframe &lt;- dbGetQuery(con, statement = paste( ################ insert SQL here ###################### &quot;SELECT m.stateid AS state_code,\\ m.cntyid AS county_code,\\ m.siteid AS site_id,\\ m.parm_code AS parameter,\\ m.poc_code AS poc,\\ o.dur_code AS sample_duration,\\ o.unitid AS unit,\\ o.method_code AS method,\\ o.sampldate AS date,\\ o.startime AS start_time,\\ o.value AS sample_value,\\ o.nulldata AS nulldatacode,\\ NULL AS sampling_frequency,\\ NULL AS monitor_protocol_id,\\ o.qual_code\\ FROM aqs.monitor m \\ JOIN aqs.obs_value o \\ ON m.id_mon = o.id_mon \\ WHERE m.stateid = &#39;27&#39; \\ AND m.parm_code = &#39;44201&#39; \\ AND o.sampldate BETWEEN &#39;2014-06-01&#39; AND &#39;2014-06-07&#39;\\ &quot; ######################################################## )); #*********************************************************************************************** ## Closes the connection dbDisconnect(con) ## Frees all the resources on the driver dbUnloadDriver(drv) 1.3 Current AQI obs Air data for the entire United States is at your finger tips. EPA’s AirNow maintains a publicly accessible folder of current air monitoring data at https://files.airnowtech.org/. Sample R script Use the following R code to grab the most recent AQI results for the entire country.e. Show R code library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.4.3 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(readr) # Connect to AirNow data site #https://files.airnowtech.org/ airnow_link &lt;- paste0(&quot;https://s3-us-west-1.amazonaws.com//files.airnowtech.org/airnow/today/&quot;, &quot;HourlyData_&quot;, format(Sys.time()-60*75, &quot;%Y%m%d%H&quot;, tz=&quot;GMT&quot;), &quot;.dat&quot;) aqi_now &lt;- read_delim(airnow_link, &quot;|&quot;, col_names = F) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## X2 = col_time(format = &quot;&quot;), ## X3 = col_character(), ## X4 = col_character(), ## X5 = col_integer(), ## X6 = col_character(), ## X7 = col_character(), ## X8 = col_double(), ## X9 = col_character() ## ) #col_types = c(&#39;cccciccdc&#39;)) # Add column names names(aqi_now) &lt;- c(&quot;date&quot;, &quot;time&quot;, &quot;aqsid&quot;, &quot;city&quot;, &quot;local_time&quot;, &quot;parameter&quot;, &quot;units&quot;, &quot;concentration&quot;, &quot;agency&quot;) # Filter to Ozone and PM2.5 results aqi_now &lt;- filter(aqi_now, parameter %in% c(&quot;OZONE&quot;, &quot;PM2.5&quot;)) Contributors Dorian Kvale 1.4 Current AQI observations Current air monitoring locations can be accessed from AirNow by opening the monitoring_site_locations.dat file. Sample R script Click the button below to view a step by step example. Show R code library(dplyr) library(readr) # Connect to AirNow data site #https://files.airnowtech.org/ airnow_link &lt;- paste0(&quot;https://s3-us-west-1.amazonaws.com//files.airnowtech.org/airnow/today/&quot;, &quot;monitoring_site_locations.dat&quot;) aqi_sites &lt;- read_delim(airnow_link, &quot;|&quot;, col_names = F) ## Parsed with column specification: ## cols( ## .default = col_character(), ## X9 = col_double(), ## X10 = col_double(), ## X11 = col_double(), ## X12 = col_double(), ## X16 = col_integer() ## ) ## See spec(...) for full column specifications. # Drop empty columns aqi_sites &lt;- aqi_sites[ , -c(14:16,22:23)] # Add column names names(aqi_sites) &lt;- c(&quot;aqsid&quot;, &quot;parameter&quot;, &quot;local_id&quot;, &quot;name&quot;, &quot;status&quot;, &quot;state_region&quot;, &quot;agency&quot;, &quot;epa_region&quot;, &quot;lat&quot;, &quot;long&quot;, &quot;elevation&quot;, &quot;local_time&quot;, &quot;country&quot;, &quot;city&quot;, &quot;state_fips&quot;, &quot;state&quot;, &quot;county_fips&quot;, &quot;county&quot;) # Filter to Minnesota sites aqi_sites &lt;- filter(aqi_sites, state_fips %in% c(27)) Contributors Dorian Kvale References 1.5 Retrieving data from Tableau I’ll let you in on a secret. You can grab data directly from published Tableau workbooks without ever having to open them. This is handy when you don’t have access to an underlying database or if you’re dealing with a database that requires complicated table joins to extract the data you need. Here’s the trick. To download data from a Tableau workbook add “.csv” to the end of a worksheet’s URL. That’s it! Let’s the grab the data behind the internal Volkswagen vehicle workbook. Sample R script Click the button below to view a step by step example. Show R code library(readr) # MPCA Tableau site pca_tableau &lt;- &quot;http://tableau.pca.state.mn.us/views/&quot; # Tableau worksheet ## Delete the link garbage ## Remove the &quot;/#/&quot; in the middle and the question mark at the end w/ anything that comes after workbook &lt;- &quot;Volkswagenownership/VWownership&quot; # Complete URL tableau_url &lt;- paste0(pca_tableau, workbook) # Final URL will look like this tableau_url &lt;- &quot;http://tableau.pca.state.mn.us/views/VWVolkswagendieselregistrationsinMN/VWownership&quot; # Paste &quot;.csv&quot; to end of Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) # Read data into a data frame vw_cars &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## `County number` = col_character(), ## County = col_character(), ## `Latitude (generated)` = col_double(), ## `Longitude (generated)` = col_double(), ## `% of VW registrations` = col_character() ## ) # Take a look head(vw_cars) ## # A tibble: 6 x 5 ## `County number` County `Latitude (generated)` `Longitude (generated)` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aitkin AITKIN 46.637 -93.432 ## 2 Anoka ANOKA 45.224 -93.265 ## 3 Becker BECKER 46.933 -95.678 ## 4 Beltrami BELTRAMI 47.888 -95.008 ## 5 Benton BENTON 45.691 -94.056 ## 6 Big Stone BIGSTONE 45.455 -96.469 ## # ... with 1 more variables: `% of VW registrations` &lt;chr&gt; Do you feel like a Jedi? Good. But there is 1 small caveat. This trick doesn’t work on the URL of a Tableau Story. You’ll need to get to the worksheet or dashboard behind the story. As an example, take a look at the Watershed Pollutant Load Monitoring workbook. Since the workbook loads to a Story the .csv trick won’t work on this page. We need to jump over to a worksheet or dashboard behind the story. In this case the tabs are shown at the top, but this won’t always be true. Let’s try the tab “Download Annual data”. Show R code Now that you know the name of the worksheet, the data is free for the taking. Let’s read it into R. library(readr) # MPCA Tableau site pca_tableau &lt;- &quot;http://tableau.pca.state.mn.us/views/&quot; # Tableau workbook workbook &lt;- &quot;wplmn_data_browser/DownloadAnnualData&quot; # Complete URL tableau_url &lt;- paste0(pca_tableau, workbook) # Paste &quot;.csv&quot; to end of Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) # Read data into a data frame water &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## .default = col_character(), ## `CatchArea (acres)` = col_integer(), ## `FWMC (mg/L)` = col_double(), ## Latitude = col_double(), ## Longitude = col_double(), ## `Mass (kg)` = col_integer(), ## `Number of samples` = col_integer(), ## `Vol (acre ft)` = col_integer(), ## Year = col_integer(), ## `Yield (lbs/ac)` = col_double() ## ) ## See spec(...) for full column specifications. # Take a look head(water) ## # A tibble: 6 x 21 ## Blank `Bsn Name` `CatchArea (acres)` `End date` `Equis ID` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; Cedar River 92160 10/31/2013 S004-432 ## 2 &lt;NA&gt; Cedar River 92160 10/31/2013 S004-432 ## 3 &lt;NA&gt; Cedar River 92160 10/31/2013 S004-432 ## 4 &lt;NA&gt; Cedar River 92160 10/31/2013 S004-432 ## 5 &lt;NA&gt; Cedar River 92160 10/31/2013 S004-432 ## 6 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## # ... with 16 more variables: `FWMC (mg/L)` &lt;dbl&gt;, Huc12 &lt;chr&gt;, `Hydstra ## # ID` &lt;chr&gt;, `Intentionally blank` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, `Mass (kg)` &lt;int&gt;, Name &lt;chr&gt;, `Number of ## # samples` &lt;int&gt;, `Org Name` &lt;chr&gt;, Parameter &lt;chr&gt;, `Site Type` &lt;chr&gt;, ## # `Start date` &lt;chr&gt;, `Vol (acre ft)` &lt;int&gt;, Year &lt;int&gt;, `Yield ## # (lbs/ac)` &lt;dbl&gt; # How many rows are there? nrow(water) ## [1] 4116 Wow! There’s over 4,000 rows of data. It looks like we got ALL the data. That’s because everything is selected in the filters on the right of the dashboard. If you’d like to download data only for a single site or a single year, you can change the filter by adding a little text to the end of the URL. Requesting a subset The filters on the worksheet are accessible by using the format Year=2014. To add a filter to your data request you first add a ? to the end of the workseet URL and then the filter. The final URL with the 2014 year filter looks like this http://tableau.pca.state.mn.us/views/wplmn_data_browser/DownloadAnnualData.csv?Year=2014. Let’s get all the data for year 2014. Show R code library(readr) # MPCA Tableau site pca_tableau &lt;- &quot;http://tableau.pca.state.mn.us/views/&quot; # Tableau worksheet workbook &lt;- &quot;wplmn_data_browser/DownloadAnnualData&quot; # Complete URL tableau_url &lt;- paste0(pca_tableau, workbook) # Paste &quot;.csv&quot; to end of Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) # Add the Year=2014 filter tableau_url &lt;- paste0(tableau_url, &quot;?Year=2014&quot;) # Read data into a data frame water_2014 &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## .default = col_character(), ## `CatchArea (acres)` = col_integer(), ## `FWMC (mg/L)` = col_double(), ## Latitude = col_double(), ## Longitude = col_double(), ## `Mass (kg)` = col_integer(), ## `Number of samples` = col_integer(), ## `Vol (acre ft)` = col_integer(), ## Year = col_integer(), ## `Yield (lbs/ac)` = col_double() ## ) ## See spec(...) for full column specifications. # Take a look head(water_2014) ## # A tibble: 6 x 21 ## Blank `Bsn Name` `CatchArea (acres)` `End date` `Equis ID` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 2 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 3 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 4 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 5 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 6 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## # ... with 16 more variables: `FWMC (mg/L)` &lt;dbl&gt;, Huc12 &lt;chr&gt;, `Hydstra ## # ID` &lt;chr&gt;, `Intentionally blank` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, `Mass (kg)` &lt;int&gt;, Name &lt;chr&gt;, `Number of ## # samples` &lt;int&gt;, `Org Name` &lt;chr&gt;, Parameter &lt;chr&gt;, `Site Type` &lt;chr&gt;, ## # `Start date` &lt;chr&gt;, `Vol (acre ft)` &lt;int&gt;, Year &lt;int&gt;, `Yield ## # (lbs/ac)` &lt;dbl&gt; Perfect! Were down to only 926 rows of data. Filtering with special characters Want to try one of the other filters? Things can get a little trickier when you filter on values that contain special characters, such as text with spaces — “Total phosphorous” — or slashes — “FWMC (mg/L)” —. To filter with special characters in a URL you’ll need to let the browser know that you really mean a text slash “/”, and that you don’t mean “move up a folder” as in Desktop/Documents/Reports. To do this, URL’s have codes for the characters that usually mean something else to computers. For example, the forward slash “/” has the special code %2. A space has the code %20. Here is a table of codes for the most common symbols. A more complete reference and a tool to convert your text to a URL friendly format is online here. Let’s try filtering the data to one the values listed for Parameter. If the filter doesn’t work, Tableau will be nice and send you all of the data. However, it isn’t helpful enough to give you clues about what’s wrong with your filter. Like if I happen to misspell phosphourus 3 times. The code below filters the data to the Parameter Total phosphorus. Show R code library(readr) # MPCA Tableau site pca_tableau &lt;- &quot;http://tableau.pca.state.mn.us/views/&quot; # Tableau worksheet workbook &lt;- &quot;wplmn_data_browser/DownloadAnnualData&quot; # Complete URL tableau_url &lt;- paste0(pca_tableau, workbook) # Paste &quot;.csv&quot; to end of Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) # Add the Measures=&quot;Mass+(kg)&quot; filter tableau_url &lt;- paste0(tableau_url, &quot;?Parameter=Total%20phosphorus&quot;) # Read data into a data frame water_phos &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## .default = col_character(), ## `CatchArea (acres)` = col_integer(), ## `FWMC (mg/L)` = col_double(), ## Latitude = col_double(), ## Longitude = col_double(), ## `Mass (kg)` = col_integer(), ## `Number of samples` = col_integer(), ## Year = col_integer(), ## `Yield (lbs/ac)` = col_double() ## ) ## See spec(...) for full column specifications. # Take a look head(water_phos, 5) ## # A tibble: 5 x 21 ## Blank `Bsn Name` `CatchArea (acres)` `End date` `Equis ID` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; Cedar River 92160 10/31/2014 S004-432 ## 2 &lt;NA&gt; Cedar River 122240 12/31/2009 S000-084 ## 3 &lt;NA&gt; Cedar River 122240 12/31/2010 S000-084 ## 4 &lt;NA&gt; Cedar River 122240 12/31/2011 S000-084 ## 5 &lt;NA&gt; Cedar River 122240 12/31/2014 S000-084 ## # ... with 16 more variables: `FWMC (mg/L)` &lt;dbl&gt;, Huc12 &lt;chr&gt;, `Hydstra ## # ID` &lt;chr&gt;, `Intentionally blank` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, `Mass (kg)` &lt;int&gt;, Name &lt;chr&gt;, `Number of ## # samples` &lt;int&gt;, `Org Name` &lt;chr&gt;, Parameter &lt;chr&gt;, `Site Type` &lt;chr&gt;, ## # `Start date` &lt;chr&gt;, `Vol (acre ft)` &lt;chr&gt;, Year &lt;int&gt;, `Yield ## # (lbs/ac)` &lt;dbl&gt; Try filtering the sites to only the Hydstra ID W38014001. Show R code library(readr) #Tableau worksheet URL tableau_url &lt;- &quot;http://tableau.pca.state.mn.us/views/wplmn_data_browser/DownloadAnnualData&quot; # Paste &quot;.csv&quot; to end of Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) # Add the Measures=&quot;Mass+(kg)&quot; filter tableau_url &lt;- paste0(tableau_url, &quot;?Hydstra%20ID=W38014001&quot;) # Read data into a data frame water_missip &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## .default = col_character(), ## `CatchArea (acres)` = col_integer(), ## `FWMC (mg/L)` = col_double(), ## Latitude = col_double(), ## Longitude = col_double(), ## `Mass (kg)` = col_integer(), ## `Number of samples` = col_integer(), ## `Vol (acre ft)` = col_integer(), ## Year = col_integer(), ## `Yield (lbs/ac)` = col_double() ## ) ## See spec(...) for full column specifications. # Take a look head(water_missip, 5) ## # A tibble: 5 x 21 ## Blank `Bsn Name` `CatchArea (acres)` `End date` `Equis ID` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; Lower Mississippi River 29824000 12/31/2007 S000-132 ## 2 &lt;NA&gt; Lower Mississippi River 29824000 12/31/2007 S000-132 ## 3 &lt;NA&gt; Lower Mississippi River 29824000 12/31/2007 S000-132 ## 4 &lt;NA&gt; Lower Mississippi River 29824000 12/31/2007 S000-132 ## 5 &lt;NA&gt; Lower Mississippi River 29824000 12/31/2007 S000-132 ## # ... with 16 more variables: `FWMC (mg/L)` &lt;dbl&gt;, Huc12 &lt;chr&gt;, `Hydstra ## # ID` &lt;chr&gt;, `Intentionally blank` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, `Mass (kg)` &lt;int&gt;, Name &lt;chr&gt;, `Number of ## # samples` &lt;int&gt;, `Org Name` &lt;chr&gt;, Parameter &lt;chr&gt;, `Site Type` &lt;chr&gt;, ## # `Start date` &lt;chr&gt;, `Vol (acre ft)` &lt;int&gt;, Year &lt;int&gt;, `Yield ## # (lbs/ac)` &lt;dbl&gt; Simple site IDs are your friend. Dashes, slashes, and surprising capital letters are your not. 1.5.1 A Happy note! To add multiple filters separate them with the &amp; character with no spaces. As in ?Year=2014&amp;Parameter=Flow. 1.5.2 Sad note /#1 If the author of the worksheet hasn’t added the filter to the page, you won’t be able to use it in a magic URL. 1.5.3 Sad note /#2 If the workbook author has all of its worksheets and dashboards hidden, you won’t be able to access its data using the “.csv” trick. Contact the workbook author and politley request that they unhide one of their data tabs so you can more easily steal the data. 1.5.3.1 Facility emissions The tabs or Tableau workbook are often hidden. Below are two methods for finding the names of the worksheets and dashboards in a Tableau project. Download and open the workbook to find the name of the worksheet with the data you want. When viewing the Tableau story online, click Share on the bottom of the page and copy the lower link. Paste this URL into your browser. When it loads scroll to the bottom of the page. You should see a section called Metadata: on the right. These are all the names of the hidden tabs! You can even click to open one of them. Let’s try downloading data from the tab “Download Annual data”. Show R code library(dplyr) #Tableau worksheet URL tableau_url &lt;- &quot;https://public.tableau.com/profile/mpca.data.services#!/vizhome/Pointsourceairemissionsdata/EmissionsbyFacility&quot; # Add &quot;.csv?:embed=y&quot; to Tableau URL tableau_url &lt;- paste0(tableau_url, &quot;.csv&quot;) fac_emits &lt;- read_csv(tableau_url) ## Parsed with column specification: ## cols( ## `&lt;!DOCTYPE html&gt;` = col_character() ## ) ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1) ## Warning: 503 parsing failures. ## row # A tibble: 5 x 5 col row col expected actual expected &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; actual 1 10 &lt;NA&gt; 1 columns 2 columns file 2 11 &lt;NA&gt; delimiter or quote f row 3 11 &lt;NA&gt; delimiter or quote = col 4 11 &lt;NA&gt; delimiter or quote d expected 5 11 &lt;NA&gt; delimiter or quote ) actual # ... with 1 more variables: file &lt;chr&gt; ## ... ................. ... .......................................... ........ .......................................... ...... .......................................... .... .......................................... ... .......................................... ... .......................................... ........ .......................................... ...... ....................................... ## See problems(...) for more details. head(fac_emits) ## # A tibble: 6 x 1 ## `&lt;!DOCTYPE html&gt;` ## &lt;chr&gt; ## 1 &quot;&lt;!--[if IEMobile 7]&gt;&lt;html class=\\&quot;no-js ie iem7\\&quot; lang=\\&quot;en\\&quot; dir=\\&quot;ltr\\&quot;&gt; ## 2 &quot;&lt;!--[if lte IE 6]&gt;&lt;html class=\\&quot;no-js ie lt-ie9 lt-ie8 lt-ie7\\&quot; lang=\\&quot;en\\ ## 3 &quot;&lt;!--[if (IE 7)&amp;(!IEMobile)]&gt;&lt;html class=\\&quot;no-js ie lt-ie9 lt-ie8\\&quot; lang=\\&quot; ## 4 &quot;&lt;!--[if IE 8]&gt;&lt;html class=\\&quot;no-js ie lt-ie9\\&quot; id=\\&quot;ng-app\\&quot; ng-app=\\&quot;vizhu ## 5 &quot;&lt;!--[if IE 9]&gt;&lt;html class=\\&quot;no-js ie ie9 lte-ie9\\&quot; id=\\&quot;ng-app\\&quot; ng-app=\\&quot; ## 6 &quot;&lt;!--[if (gte IE 9)|(gt IEMobile 7)]&gt;&lt;html class=\\&quot;no-js ie\\&quot; id=\\&quot;ng-app\\&quot; Contributors Dorian Kvale References Back to top "],
["quality-assurance-methods.html", "Quality assurance methods", " Quality assurance methods The following sections describe the data quality assurance methods used to find anomalies or errors in the air monitoring data meets. "],
["data-cleaning.html", " Data cleaning 2.1 Remove blank, NULL, and missing values 2.2 Remove Qualified data 2.3 Duplicate observations", " Data cleaning Before jumping into your analysis you’ll want to clean it up with some helpful quality checks and formatting procedures. These procedures include steps to: 2.1 Remove blank, invalid, NULL, and missing values 2.2 Evaluate qualified data 2.3 Remove duplicate observations 2.1 Remove blank, NULL, and missing values Description Large monitoring data sets often contain observations with missing concentrations, detection limits or other labeling errors that can lead to incorrect summary statistics. Recommended steps Identify any blank, NULL, -999, and missing values. If available, review Null Data Codes. These codes will tell you why the data is missing. Most data sets will include null data codes. For AQS null code descriptions, see https://aqs.epa.gov/aqsweb/documents/codetables/qualifiers.html. Determine if your analysis needs to account for missing data before deleting observations. You may want to perform a count on all sample dates to quantify the expected number of observations. If you no longer need these records, remove them from the dataset. Document your process. Why not keep them? In most cases, our analyses do not require or allow filling in missing values. For this reason, it makes sense to remove them. However, for some analyses you may want to fill the missing values. The method for filling values will be project specific and decisions should be documented. Data sets should identify any records that include a replaced missing value. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load example monitoring data library(tidyverse) data &lt;- read_csv(&#39;aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,-999.99,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,NA,NA,0.08,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,&quot; &quot;,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,NULL,a,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot;&#39;) Sample monitoring data Create a function to test for missing concentration values. # Test for missing concentrations, non-numeric values, and -999 missing_conc &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } Use the function to add a column to your data conc_missing that tests for missing concentration values. # Create a new TRUE/FALSE column labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc)) # Select all missing observations missing_values &lt;- filter(data, conc_missing == TRUE) Missing values Filter the data to only non-missing observations. data &lt;- filter(data, conc_missing == FALSE) The new and improved cleaner data You can create similar functions to test for missing dates, site IDs, detection limits, and parameter codes. # Test for missing dates missing_dates &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &gt; 11 || nchar(as.character(x)) &lt; 6 } # Test for missing site IDs missing_sites &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &lt; 5 } # Test for missing detection limits missing_dls &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; 0 } # Test for missing parameter codes missing_param &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; 0 || nchar(as.character(x)) &lt; 5 || nchar(as.character(x)) &gt; 9 } To apply these functions all at once use dplyr’s great function called mutate(). # Create new TRUE/FALSE columns labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc), date_missing = missing_dates(date), site_missing = missing_sites(aqs_id), dl_missing = missing_dls(md_limit), param_missing = missing_param(param_code)) # Filter to remove any rows with a missing parameter. # We use sum() to count the number of missing parameters. # In this case we will drop any row with at least one missing parameter. data &lt;- data %&gt;% filter(sum(c(conc_missing, date_missing, site_missing, dl_missing, param_missing), na.rm = T) &lt; 1) The super cleaner data Contributors Dorian Kvale References 2.2 Remove Qualified data Description Valid sample results may have data qualifiers. These qualifiers provide contextual information about factors that may have influenced the result. In the AQS data format, qualifiers are listed in 10 qualifier fields. A sample may have more than one qualifier. Descriptions of the AQS qualifier codes are available from the EPA at https://aqs.epa.gov/aqsweb/documents/codetables/qualifiers.html. Recommended steps Filter data to view samples with Qualifier codes Evaluate whether any of the qualified data will unduly influence your analysis. For example, you may want to remove samples influenced by unique events. When removing qualified data, maintain a copy of the original data set and document what values have been removed. In some cases, you may want to run your analysis with and without the qualified values to characterize the influence of the qualified data on the results. Sample R script Contributors Cassie McMahon References 2.3 Duplicate observations Description Large monitoring data sets often contain multiple observations from the same monitor for the same time period. When multiple observations do occur they tend to be identified by or by a qualifier describing why the second observation was recorded (e.g. a duplicate for quality control). The treatment of these duplicate values will depend on the analysis. Recommended steps Identify duplicate observations. A duplicate in the data is likely the result of an error. Consult the lab about duplicate values. Treatment of a duplicate observation will depend on the project. For criteria pollutant design values, treatment of duplicate samples is defined for each pollutant in the Appendices of 40 CFR 50. For air toxics, treatment of duplicate values will depend on the analysis. In some cases you may want to average the results. In others, you may want to take the maximum value. If you identify records that are completely duplicated (same date, same site, same POC, same result, same MDL), delete these records prior to completing analyses. For automated annual summaries, use the following hierarchy to remove duplicates: Calculate the mean concentration of all non-censored observations. If all observations are censored, select the observation with the lowest detection limit. If all observations are censored and the detection limits are equal, select a single observation. Why not keep all the data? Leaving multiple observations for some dates in the data set is likely to bias calculated summary statistics. For example, duplicate ozone observations from January would be likely to skew the annual average lower. By limiting the number of observations to 1 per day, we can remove the bias due to duplicate observations. Sample R script Click the button below to view a step by step example of the methods above. Show R code Load the sample monitoring data. library(dplyr) data &lt;- read.csv(text = &#39; aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,0.05,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,0.01,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.02,NA,0.05,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.04,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.18,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; &#39;, stringsAsFactor = F) Sample monitoring data Delete rows of data that are exact duplicates. # Check for exact duplicate rows dup_rows &lt;- data[duplicated(data), ] # Drop the exact duplicates data &lt;- data[!duplicated(data), ] Add a unique column ID to each site/poc/param-code/date combination. # Add unique key to each row data$key &lt;- 1:nrow(data) # Create a unique ID for each site/poc/param-code/date combination data$unique_sample_id &lt;- paste(data$aqs_id, data$poc, data$param_code, data$date, sep = &quot;_&quot;) Figure 2.1: Unique ID column added. Test for duplicate observations. # Label duplicate samples data &lt;- group_by(data, unique_sample_id) %&gt;% mutate(duplicate = n() &gt; 1) # Create duplicate table dupes &lt;- filter(data, duplicate == T) Duplicate observations If duplicates are found, use the following hierarchy to remove duplicates: Calculate the mean concentration of all non-censored observations. If all observations are censored, select the observation with the lowest detection limit. If all observations are censored and the detection limits are equal, select a single observation. # Replace censored observations with NA dupes &lt;- dupes %&gt;% mutate(conc = ifelse(conc &gt;= md_limit, conc, NA)) # Calculate the mean of non-censored observations dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% mutate(conc = mean(conc, na.rm = T)) # Arrange by concentration and detection limit, then select the first observation dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% arrange(desc(conc), md_limit) %&gt;% filter(key == key[1]) # Remove Unique IDs with duplicates from data data &lt;- filter(data, !duplicate) # Attach the selected duplicates with highest result and lowest detection limit data &lt;- rbind(data, dupes) The new cleaner data Figure 2.2: Final data set with 1 duplicate removed. Contributors Dorian Kvale, Cassie McMahon Back to top "],
["data-validation.html", " Data validation 3.1 Identifying Instrument drift or leaks in a system. 3.2 Identify exceptional and extreme values aka outliers. 3.2 Identify sequential repeats aka “sticky” numbers", " Data validation Environmental monitoring data can be subject to error. Three important errors to look for are: 3.1 Leaks and sample contamination ?? Instrument errors such as drift or repeated results (sticking values) 3.2 Outliers or extreme values Validation steps Identify exceptional values. Identify sequential identical values (sticking). Identify potential instrument drift. Report findings to laboratory for further investigation. Why not average the exceptional data over the year and assume the problem will go away? Further investigation by laboratory staff is required if the laboratory equipment or system is in error. If the laboratory is not in error, these concentrations require additional investigation by data analysts and potentially by air pollution source specialists. For these reasons, these data validation steps are critical. A decreasing signal or repeated sequential measurement means that the data are not interpretable. An exceptionally high value could mean laboratory contamination or potentially could contribute to human health impacts if the high value is accurate. Notes Possible Contamination Issues-by site and pollutant 3*IQR: I used as a simple outlier identifier in EU guidelines. This includes a lot of data for UFPs. Cassie has tried: 3*75th percentile(UFP) - quantiles from previous year of data 2.5 *75th percentile of the data (put in Derek’s explanation) this is for integrated air toxics data 3*SD (Poor performance) Site mean more than twice the second highest site mean(NO) Sites with maxima over 2X the next highest value(NO) Only one detection at a site (special case) And Detection was greater than 3X the MDL And Detection was within 10% of state maximum that year Possible Sample loss issues (leaks) by site and pollutant Check for several (2,3) below MDL values when values have been above MDLs previously (need discussion). MDL would be from the previous year. I tried taking the mean of 5 sequential values, and then the next 5 sequential values. If the second set of means was 1.5 times less than the first mean I flagged the values. Recommendation: Use the [75th percentile * 3] for all pollutants. Sample R script Click the button below to view a step by step example of the methods above. Show R code Goals: Find extreme values that may be in error or from contamination. Find 3 repeating values that are exactly the same. Find low values where this is not common. Do this in a routine way (identify frequency of validation checks). The following sections on data validation will use the example monitoring data loaded below. library(tidyverse) library(stringr) library(RcppRoll) library(lubridate) ##need to confirm that this data set has sequential repeats, some high values and at least 10 dates with numbers data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) Table: Sample monitoring data _First its’ always a good idea to look at the data _Open this web tool to chart all measured pollutants for each site for the cleaned data set. (https://mpca-pahs.shinyapps.io/ChartCleanData/) 3.1 Identifying Instrument drift or leaks in a system. The test looks for a difference in variance of carbon tetrachloride between a calendar quarter for two consecutive years. Carbon tetrachloride was chosen because it is a banned substance and no longer in use, it has very few below detection limit values, and no direct sources. It has seasonally variable concentrations, so calendar quarters were compared to eliminate the detection of seasonal differences. The statistical test used was a Levenes Test for homogeneity of variance. Sample R script Click the button below to view a step by step example of the method above. Show R code library(car) data$conc &lt;- as.numeric(data$conc) data$aqs_id &lt;- as.character(data$aqs_id) sites &lt;- unique(data$aqs_id) years &lt;- unique(data$year) data$quarter = quarter(data$date) quarters &lt;- unique(data$quarter) pocs &lt;- unique(data$poc) clean_values = function(data) { data$conc = as.numeric(as.character(data$conc)) data$conc[abs(data$conc) &gt;= 999] = NA return(data) } data = clean_values(data) levene_function = function(conc, quarter_year, row, col) { if(length(unique(quarter_year))&lt;2){ return(NA)} data = data.frame(conc = conc, quarter_year = quarter_year) return(leveneTest(conc~as.factor(quarter_year), data = data)[row,col]) } leak_table_unfiltered &lt;- data.frame() for(i in max(years):max(years)-1:length(years)){ for(j in i+1){ data_carbontet=data.frame() data_carbontet &lt;- filter(data, pollutant==&quot;Carbon Tetrachloride&quot;, !is.na(conc), year %in% c(i, j)) data_carbontet$quarter_year = paste(data_carbontet$quarter, &quot;_&quot;, data_carbontet$year) data_carbontet &lt;- data_carbontet %&gt;% group_by(aqs_id, poc, quarter) %&gt;% summarise(fvalue_levene = levene_function(conc, quarter_year, 1,2), pval_levene = levene_function(conc, quarter_year, 1,3), deg_free_levene = levene_function(conc, quarter_year, 2,1), Year_1 = min(year), Year_2 = max(year), Mean_Year_1 = mean(conc[year==min(year)], na.rm=T), Mean_Year_2 = mean(conc[year==max(year)], na.rm=T)) %&gt;% ungroup() leak_table_unfiltered &lt;- rbind(leak_table_unfiltered,data_carbontet) } } leak_table &lt;- filter(leak_table_unfiltered, pval_levene&lt;0.01, abs(Year_1-Year_2)==1) leak_table$Warning_Type &lt;- &quot;Decrease_in_Measurements&quot; datatable(head(leak_table, 10), options = dt_options) %&gt;% formatSignif(c(&quot;fvalue_levene&quot;, &quot;pval_levene&quot;, &quot;Mean_Year_1&quot;, &quot;Mean_Year_2&quot;), digits = 2) 3.2 Identify exceptional and extreme values aka outliers. These are detected by comparing each measured concentration to 3 X the 75th percentile of the data set by year, site, and pollutant. For now, pocs collocated measurements are not tested separately. Sample R script Click the button below to view a step by step example of the method above. Show R code # Test for exceptionally high values [above 75th percentile X 3] high_data &lt;- group_by(data, year, aqs_id, pollutant) %&gt;% mutate(AR_Mean = mean(conc, na.rm=T), Percentile_75 = quantile(conc, 0.75, na.rm=T), Percentile_75_X3 = Percentile_75*3) %&gt;% ungroup() high_data &lt;- filter(high_data, conc&gt;Percentile_75_X3, !is.na(conc), AR_Mean&gt;md_limit, Percentile_75&gt;0) high_data$Warning_Type &lt;- &quot;Exceptionally_High_Value_Test&quot; high_data &lt;- unique(high_data) datatable(head(high_data, 10), options = dt_options) high_data &lt;- high_data[, c(&quot;date&quot;, &quot;pollutant&quot;, &quot;aqs_id&quot;, &quot;Warning_Type&quot;)] ##Toxics - Multiplying by 3 returns ~4000/1176818 values as extreme, multiplying by 1.5 returns &gt;20,000. Multiplying by 3 returns between 2 and 45% of analyte/site/year groups. ##UFP - Multiplying by 3 returns 28894 of 1335083 total values. Multiplying by 3 returns between 2 and 15% of analyte/site/year groups. ##PAHs - Multiplying by 3 returns 168 of 39298 total values. Multiplying by 3 returns between 3 and 10% of analyte/site/year groups. 3.2 Identify sequential repeats aka “sticky” numbers Three sequential replicate values may be a result of a machine error. # Test for exceptionally high values [above 75th percentile X 3] repeat_data &lt;- group_by(data, poc, year, aqs_id, pollutant) %&gt;% arrange(year, aqs_id, poc, pollutant, date) %&gt;% mutate(Previous_Day = lag(conc, 1), Two_Days_Prior = lag(conc, 2)) %&gt;% ungroup() repeat_data &lt;- mutate(repeat_data, Repeat_Test = ifelse(round(conc, digits=4)==round(Previous_Day, digits=4) &amp; round(Previous_Day, digits=4)==round(Two_Days_Prior, digits=4), &quot;TRUE&quot;, &quot;FALSE&quot;)) repeat_data$Warning_Type &lt;- &quot;Repeat_Test&quot; repeat_data &lt;- filter(repeat_data, conc&gt;0 &amp; Repeat_Test==TRUE) datatable(head(repeat_data, 10), options = dt_options) repeat_data &lt;- repeat_data[, c(&quot;date&quot;, &quot;pollutant&quot;, &quot;aqs_id&quot;, &quot;Warning_Type&quot;)] ###There are many repeating values in the Toxics data. None in PAHs. Approximately 3200 for UFPs, but this is 15 minute data. We may need a more stringent test for UFPs. Table: Dates and pollutants to check including the potential issue Contributors Kristie, Cassie, Dorian, Derek References USEPA Technical Support Document for the Nation Air Toxics Trends Sites European Union Guide to Data Validation [US EPA Guidance on Data Verification and Data Validation] (https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf) [US EPA Data Validation Workbook Presentation and Training Materials] (https://www3.epa.gov/ttnamti1/files/ambient/airtox/workbook/T-Workbook_Secs1-8.pdf) [US EPA Data Validation Workbook] (https://nepis.epa.gov/Exe/ZyNET.exe/P1006PAB.TXT?ZyActionD=ZyDocument&amp;Client=EPA&amp;Index=2006+Thru+2010&amp;Docs=&amp;Query=&amp;Time=&amp;EndTime=&amp;SearchMethod=1&amp;TocRestrict=n&amp;Toc=&amp;TocEntry=&amp;QField=&amp;QFieldYear=&amp;QFieldMonth=&amp;QFieldDay=&amp;IntQFieldOp=0&amp;ExtQFieldOp=0&amp;XmlQuery=&amp;File=D%3A%5Czyfiles%5CIndex%20Data%5C06thru10%5CTxt%5C00000016%5CP1006PAB.txt&amp;User=ANONYMOUS&amp;Password=anonymous&amp;SortMethod=h%7C-&amp;MaximumDocuments=1&amp;FuzzyDegree=0&amp;ImageQuality=r75g8/r75g8/x150y150g16/i425&amp;Display=hpfr&amp;DefSeekPage=x&amp;SearchBack=ZyActionL&amp;Back=ZyActionS&amp;BackDesc=Results%20page&amp;MaximumPages=1&amp;ZyEntry=1&amp;SeekPage=x&amp;ZyPURL) [Review article by Helsel, D about non-detects and substitution methods.] (https://academic.oup.com/annweh/article/54/3/257/223531/Much-Ado-About-Next-to-Nothing-Incorporating) [A Chemosphere review article describing why BDL substitution should not be done.] (https://pdfs.semanticscholar.org/182e/9278fc36dd73d48a414b8fbc30a44ea314d3.pdf) [Temporal variability of selected air toxics in the United States] (http://www.sciencedirect.com/science/article/pii/S1352231007004840) [Spatial and temporal analysis of national air toxics data] (http://www.tandfonline.com/doi/pdf/10.1080/10473289.2006.10464576) "],
["collocated-monitors.html", " Collocated monitors 4.1 Evaluate collocated air monitors (POCs) 4.2 Prioritize multiple air monitors (POCs)", " Collocated monitors Analysis steps for collocated monitors include: 4.1 Evaluate sites with collocated monitors (POCs) 4.2 Prioritize observations from collocated monitors (POCs) 4.1 Evaluate collocated air monitors (POCs) Description Collocated samples are multiple samples taken for a pollutant from the same monitoring site at the same time. Collocated samples provide an extra layer of quality assurance by taking multiple measurements under very similar conditions which helps detect machine, lab, and human error. Depending on the results from collocated monitors, only one of the monitors, or potentially neither may be considered as valid measurements. Why not keep all the data? Leaving observations for multiple POCs in the data set is likely to bias calculated summary statistics. For example, consider a site that starts with two monitors operating during the winter months, but after one malfunctions, the site is left with one monitor operating for the remainder of the year. If we were to calculate the annual average for this site, the mean would be biased by having twice as many observations during the winter months. By limiting the number of observations to 1 per day, we can remove the bias due to the extra winter observations. Recommended steps Viewing a scatterplot of collocated measurements is suggested as it helps to view both obvious and subtle irregularities between collocated measurements. If there is evidence of bias in one of the monitors (values from one collocated monitor are usually higher than the other), or there is large variance between the collocated monitors (values differ from each other by significant margins), then both collocated monitors need to be investigated further. EPA recommends that collocated monitors have no higher than a 15% coefficient of variation between their measurements. Calculating the CV for every measurement pair does not make sense as small differences (i.e. 0.001 and 0.002) have the same CV as large differences (i.e. 10 and 20). Therefore, we chose to look at the median CV for each site, pollutant, and year then flag collocated monitors if the median CV is greater than 15 which suggests differences between the collocated monitors are systematic. We only calculate the CV for collocated monitors on days which both have valid measurements greater than or equal to the MDL since there is lower confidence in values below the MDL. We only compare the median CV to the threshold of 15 if there are at least 10 calculated CV values since it does not make sense to flag monitors where only a few values are above detection. If collocated monitors do not agree, then consult with the lab, field or quality assurance team to determine if there is a problem with one of the POCs. If they determine that one of POCs is reliable and the other is not, then only use data from the reliable POC. If they cannot determine which POC is the problem, then confidence in the measurements from both collocated monitors is reduced and * no measurements taken from either collocated monitor should be used.* More than likely, we will be asked to report results. We will have lower confidence in the accuracy of those results Notes Identify bias Pairwise t-test for the POC means? Look at quantiles of differences? Who should we notify if one POC is higher than the other? Develop a process for this potentially in Air Vision with email notifications to field and lab. Default option if one is biased higher than the other: Go with higher POC? Go with lower POC? Use mean? Exclude both POCs? This might be pollutant dependent. Carbon tetrachloride, for example, we can determine which monitor is correct. Formaldehyde is stable enough to somewhat know what is correct. Maybe this is true for 1,3 butadiene too. Otherwise, take the mean. Exclude extreme values for the POC or all values for the POC? Need input from laboratory or see the decision criteria above. Variance: Minimum correlation for both POCs to be valid? Possibly use a R2 of 0.99 (2 significant digits). May require unique COV for each pollutant or pollutant group. One value above detection, one value below? Derek suggests going with the value above detection. We don’t want to do substitutions unless we have to, so if there are not any problems with the POC above detection, then use the actual measured value. (EPA Data Analysis Workbook?) One value above acute IHB and one value below? First check validity of monitors to see if there are problems with one of the monitors. If both POCs seem valid, use the mean? (I suggest this because I think it’s the most defensible thing to do. The true concentration is likely between the POCs and the mean is the most unbiased estimate given the 2 POC measurements. If one POC is well above the IHB and one is just barely below, then the true concentration is likely above the IHB. If one POC is just barely above the IHB and the other is well below, then the true concentration is likely below the IHB. We could go with the higher POC if we want to be conservative.) Pull the highest POC for the daily maximum and the annual second high comparison to acute IHB. Take the mean of the POCS for the UCL-95, assuming both are valid values. Sample R script POC_compare generates scatterplots comparing all POCs in the data. Data must have columns with “AQSID”, “POC”, “Parameter”, “Date”,“Concentration”, “Null_Data_Code”, “MDL”, “Pollutant”, “Year”, “CAS” in order from left to right. Contributors Derek References 4.2 Prioritize multiple air monitors (POCs) Description If collocated monitors at a site are both considered valid, then a single value must be chosen from multiple collocated values to avoid double-counting values at a single site. Recommended steps For air toxics, measurements from collocated monitors are considered equally valid unless there is a specific reason to treat them differently. Therefore, these methods are used for dealing with collocated samples. If one monitor has a valid measurement, and the other does not, then the valid measurement is used. If one monitor has a valid measurement greater than or equal to the MDL and the other has a valid measurement less than the MDL, then the measurement above the MDL is used as a conservative approach as there is greater confidence in the value above the MDL. If both monitors have valid measurements above the MDL, then those values are averaged since both values are equally valid and the “true” value is assumed to likely be between the two values. Sample R script Averages POCs. All POCs with valid measurements &gt;= MDL are averaged. If no POC has a valid measurement &gt;= MDL, then all valid measurements are averaged (result will be &lt; MDL and estimated using MLE approximation). If there are no valid measurements, then result will be NA and filtered out later. Data must have columns with “Monitoring Site”, “AQSID”, “Parameter”, “Pollutant”,“PollutantGroup”, “Date”, “MDL”, “Result”, “Censored”. POC_average = function(data) { library(tidyverse) POC_averaging = function(Result, Censored) { Result = Result[!is.na(Result)] Censored = Censored[!is.na(Censored)] if(all(Censored, na.rm = T)) { return (mean(Result, na.rm = T)) } else { return (mean(Result[!Censored], na.rm = T ) ) } } data = data %&gt;% group_by(AQSID, Parameter, Pollutant, Date, MDL) %&gt;% summarise(Result = POC_averaging(Result, Censored), Censored = all(Censored, na.rm = T) ) %&gt;% mutate(Result = ifelse(is.na(Result), NA, Result), Censored = ifelse(is.na(Result), NA, Censored)) return (select(data, AQSID, Parameter, Pollutant, Date, Result, MDL, Censored) ) %&gt;% ungroup() } library(tidyverse) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) ## Parsed with column specification: ## cols( ## AQSID = col_integer(), ## POC = col_integer(), ## Param_Code = col_integer(), ## Date = col_date(format = &quot;&quot;), ## Concentration = col_double(), ## Null_Code = col_character(), ## Dlimit = col_double(), ## Pollutant = col_character(), ## Year = col_integer(), ## CAS = col_character() ## ) names(data) &lt;- c(&quot;AQSID&quot;, &quot;POC&quot;, &quot;Parameter&quot;, &quot;Date&quot;, &quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) POC_averaged_data = data %&gt;% mutate(Censored = Result &lt; MDL) %&gt;% POC_average() Contributors Derek References "],
["detection-limits.html", " Detection limits 5.1 Method Detection Limit - Definition 5.2 Procedure for calculating method detection limits 5.3 Finding detection limits (Cassie) 5.4 Qualifier Codes for Detection Limits 5.5 Below detection values 5.6 Multiple detection limits", " Detection limits Detection limit methods. Notes Types of detection limits (what detection limit the lab calculates, where it is, where it is not and when not to use what lims has) Does a value equal to the detection limit count as detected? “YES!” Things to consider when there are more than one detection limit per year. Discuss averaging two detection limit years and using that instead of the previous detection limit. Estimating below detection values (what not to do): Don’t delete data. Deleting data that are below a detection limit biases central tendency estimates and whole data summaries (quantiles of the data set) high. Don’t fill with zeros Substituting below detection limit data with 0’s biases central tendency estimates and whole data summaries (quantiles of the data set) low, and can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. Don’t fill with DLs or ½ DLs Substituting below detection limit data with 1/2 MDL creates the least bias in central tendency estimates and whole data summaries (quantiles of the data set), however, it can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. Substituting below detection limit data with MDLs biases central tendency estimates and whole data summaries (quantiles of the data set) high, and can change the distribution by the creation of multiple same value numbers, and can even create an inaccurate bimodal distribution of the data. 5.1 Method Detection Limit - Definition The method detection limit (MDL) is the minimum concentration of a substance that can be measured and reported with 99% confidence that the analyte concentration is greater than zero and is determined from analysis of a sample in a given matrix containing the analyte. 5.2 Procedure for calculating method detection limits Prior to 2017, the lab calculated detection limits according to the procedures defined in 40 CFR Appendix B to Part 136 - Definition and Procedure for the determination of Method Detection Limit Begining in 2017, the lab adopted a modified method for calcuating the method detection limit. Known as the “Method Update Rule(MUR)”, this method accounts for media background contamination when establishing the MDL. This method is defined in the NATTS TAD (October 2016). EPA has proposed to revise 40 CFR Appendix B to Part 136 to reflect this methodology. Both methods require preparing and analyzing 7 matrix spikes and 7 method blanks. Samples should be prepped and analyzed over the course of at least 3 seperate batches/days. Calculate the MDL of spiked samples (MDL_sp) Calculate standard deviation of the calcuated concentrations for the spiked samples. Calculate the MDL for the spiked samples by multiplying the standard deviation of spiked sample by the one-sided student’s T value at 99% confidence corresponding to the number of spikes analyzed (3.143 if 7 samples). New method requires calculation of the MDL of the method blanks(MDL_b) If no method blank provides a numerical result, no further action needed. If method blank pool includes a combination of ND and numeric values, set the method blank MDL to equal the highest of the method blank results. IF all concentration values for the method blank pool are numeric values, calculate the MDL_b as follows: Calculate the average concentration of method blanks Calculate the standard deviation of the method blank concentrations Multiply the standard deviation by the one-sided student’s T value at 99% confidence corresponding to the number of blanks analyzed Calculate the MDL_b as the sum of the average concentration method blanks, and the product of the standard deviation of blanks and the associated student’s T Value MDL_b = mean_blanks + stdv_blanks * Student T Compare MDL_sp and MDL_b. The higher of the two values is reported as the laboratory MDL. 5.3 Finding detection limits (Cassie) Annual method detection limits used for data analysis are stored on the x:drive, “X:\\Databases\\AQ\\AQ Lab\\Data Analysis\\Air Toxics\\Detection Limits\\Annual Detection Limits (micrograms).xlsx” Use ParamYear field to join data Detection limits are updated when received from the lab. Current year samples may not have a detection limit in the file if the lab has not performed the test. If you receive a detection limit from the lab it is likely in instrument units! Metals (ug/mL): conversion to ug/m3 requires accounting of flow rate, runtime, and 1/9 filter strip, and 40 mL liquid extraction Carbonyls (ug/mL): conversion requires account of flow rate, runtime, and extraction volume. VOCs (ppb):conversion can be done using MW and Standard Temp/Pressure. When Promium is adopted, method detection limits will be applied on a sample basis and will account for the varying air volumes of samples. As a result, there will be multiple detection limits for an analyte. 5.4 Qualifier Codes for Detection Limits Begining in 2017, the MPCA will be linking laboratory detection limits to sample data and applying appropriate data qualifiers depending on detection status. These qualifiers include: ND = HAP not qualitatively identified (reported as zero) MD = &lt; MDL SQ = &gt;= MDL but &lt; SQL No qualifier = measured concentration &gt;= to SQL The SQL is 3.18x the MDL 5.5 Below detection values Description Recommended steps Sample R script Example charts for visualizing data below detection limits library(tidyverse) library(stringr) library(lubridate) library(car) library(DT) library(shiny) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) Table: Sample monitoring data Shiny tool for displaying non-detects _Open this tool to chart pollutant values and detection limits by year (https://mpca-pahs.shinyapps.io/ChartDetectionLimits/) Shiny code library(shiny) library(rsconnect) library(readr) library(ggplot2) library(dplyr) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) pollutant &lt;- unique(data$pollutant) year &lt;- unique(data$year) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = pollutant, selected=&quot;Benzene&quot;), selectInput(&quot;year&quot;, label=&quot;Choose a year&quot;, choices = years, selected=2009))), column(9, plotOutput(&#39;detlim&#39;, height = &quot;400px&quot;)))), server = function(input, output) { output$detlim &lt;- renderPlot({ print(input$year) print(input$pollutant) data_sub = filter(data, year==input$year, pollutant==input$pollutant) mdl &lt;- mean(data_sub$md_limit) data_sub$Censored &lt;- ifelse(data_sub$conc &gt; data_sub$md_limit, FALSE, TRUE) ggplot(data=data_sub, aes(x= factor(aqs_id), y=conc)) + scale_colour_manual(values= c(&quot;#197519&quot;[FALSE %in% unique(data_sub$Censored)], &quot;#0000FF&quot;[TRUE %in% unique(data_sub$Censored)]), breaks=c(FALSE, TRUE)) + geom_boxplot(outlier.colour=NA) + geom_jitter(aes(color=Censored), size =2.4, alpha=0.55) + geom_hline(yintercept = mdl) + xlab(NULL) + ylab(&quot;Result (ug/m3)&quot;) + expand_limits(y=c(0, max(data_sub$conc))) + theme(text = element_text(size=15), axis.text.x = element_text(angle = -90, vjust = 0.3, size=14)) + ggtitle(paste0(&quot;Site Comparison Boxplot with Censored and Non-Censored Values for &quot;, input$pollutant, &quot;, from &quot;, input$year), subtitle = &quot;---- Horizontal Line ---- = Detection Limit&quot;) }) }) Contributors References 5.6 Multiple detection limits In progress. "],
["completeness-checks.html", " Completeness checks 6.1 Completeness checks", " Completeness checks Checking for data completeness before generating summaries ensures that results will be comparable from one year to the next. Without completeness checks, changes in the seasonal coverage from year to year may create the illusion of increasing or decreasing trends. Completeness checks inlcude tests for: Calendar quarter completeness Number of samples required for a season or quarter. Annual completeness Number of samples required for a year. Unique values Number of unique values required. 6.1 Completeness checks Description This section describes completeness guidelines and methods for performing completeness checks. Air toxic reporting guidelines based on MPCA and EPA guidelines Annual results are considered incomplete if any of the following conditions are not met: Annual completeness 75% or more of expected samples collected (rounded up) Calendar quarter completeness 75% or more of expected samples collected (rounded up) Unique values 2 or more unique values Criteria pollutant reporting guidelines Completeness rules for criteria pollutant design values are defined in the Appendices of 40 CFR 50. In general, these rules apply: Annual completeness 75% or more of samples collected Calendar quarter completeness 75% or more of samples collected Recommended steps Based on the monitoring schedule, record the total expected samples for each year and quarter. Air toxics monitors follow a fixed sampling schedule provided by EPA’s Air Toxics Calendar. Count the number of valid samples for each year and quarter. Divide the number of valid samples by the number of expected samples. Count the number of unique values for each year. Mark annual results as incomplete if they do not fulfill one of the completeness checks. Example R script Click the button below to view a step by step example of the methods above. Show R code Packages library(tidyverse) Our example data is organized by monitoring site and date. data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) Figure 6.1: Sample data table. Step 1: Find the expected number of samples. Monitors in EPA’s Air Quality System are required to follow the Air Toxics Monitoring Calendar. The sampling schedule for air toxics is generally 1 sample per every 6 days. Depending on the sampling start date and whether it is a leap year or not, the expected number of samples for the year will range from 60 to 61. If you are uncertain about the sampling schedule for your data consult the lab to confirm the expected number of samples. Entering the start and end date into the sample_calendar() function below will create a list of the expected sampling dates based on EPA’s air toxics monitoring schedule. # Create a sampling calendar based on EPA&#39;s air toxics monitoring schedule sample_calendar &lt;- function(start = &quot;2012-01-01&quot;, end = &quot;2016-12-31&quot;, day_interval = 6, type = &quot;air_toxics&quot;) { library(lubridate) # Convert &#39;start&#39; and &#39;end&#39; to class date start &lt;- ymd(start) end &lt;- ymd(end) # Set official start date to selected EPA calendar if(type == &quot;air_toxics&quot;) { epa_start &lt;- ymd(&quot;1990-01-09&quot;) } else { epa_start &lt;- start } # Create full table of sampling dates calendar &lt;- seq(from = epa_start, to = end, by = paste(day_interval, &quot;days&quot;)) # Subset to user&#39;s date range calendar &lt;- calendar[calendar &gt;= start &amp; calendar &lt;= end] # Print total sampling days and date range cat(length(calendar), &quot;sampling days from&quot;, as.character(min(calendar)), &quot;to&quot;, as.character(max(calendar))) return(calendar) } Find the date range of the data and create the expected sampling schedule with the function above. # Find the year range of your data date_range &lt;- range(data$Date) # Create expected sample calendar epa_schedule &lt;- data_frame(Date = sample_calendar(start = format(date_range[1], &quot;%Y-01-01&quot;), #Extend range to first day of the year end = format(date_range[2], &quot;%Y-12-31&quot;), #Extend range to last day of the year day_interval = 6)) ## 304 sampling days from 2009-01-05 to 2013-12-28 # Add year and calendar quarter columns epa_schedule &lt;- epa_schedule %&gt;% mutate(Year = year(Date), cal_quarter = quarter(Date)) # Count the expected number of samples per quarter and year. epa_schedule &lt;- epa_schedule %&gt;% group_by(Year, cal_quarter) %&gt;% summarize(expected_quarter_samples = length(unique(Date))) %&gt;% group_by(Year) %&gt;% mutate(expected_annual_samples = sum(expected_quarter_samples)) Expected number of samples. Figure 6.2: Expected number of samples. Step 2: Count number of valid samples. # Assign each date to a calendar quarter data &lt;- data %&gt;% mutate(cal_quarter = quarter(Date)) # Count the number of sampling dates for each quarter and year. data &lt;- data %&gt;% group_by(AQSID, POC, Param_Code, CAS, Year, cal_quarter) %&gt;% mutate(valid_quarter_samples = length(unique(Date[!is.na(Concentration)]))) %&gt;% group_by(AQSID, POC, Param_Code, CAS, Year) %&gt;% mutate(valid_annual_samples = length(unique(Date[!is.na(Concentration)]))) Step 3: Divide the number of valid samples by the number of expected samples. # Join expected sample table to data by quarter and year columns data &lt;- left_join(data, epa_schedule, by = c(&quot;Year&quot;, &quot;cal_quarter&quot;)) # Divide valid samples by expected samples data &lt;- data %&gt;% group_by(AQSID, POC, CAS, Year, cal_quarter) %&gt;% mutate(pct_quarter_samples = round(valid_quarter_samples / expected_quarter_samples, 2)) %&gt;% group_by(AQSID, POC, CAS, Year) %&gt;% mutate(pct_annual_samples = round(valid_annual_samples / expected_annual_samples, 2)) Step 4: Count the number of unique values for each year. data &lt;- data %&gt;% group_by(AQSID, POC, Param_Code, CAS, Year) %&gt;% mutate(only_detects = ifelse(Concentration &lt; Dlimit, NA, Concentration), unique_values = n_distinct(only_detects, na.rm = TRUE)) Step 5: Mark results as incomplete if they fail one of the completeness checks. # Set incomplete to zero if any completeness checks fail data &lt;- data %&gt;% rowwise() %&gt;% mutate(complete = sum(c(pct_quarter_samples &gt;= 0.75, pct_annual_samples &gt;= 0.75, unique_values &gt;= 2)) &gt;= 3) #names(data) Final table with added complete column. Figure 6.3: Final table with added complete column. Step 6 (Optional): By collapsing the complete status to 1 row per site-pollutant-quarter combination, you can save the table and attach a site’s completeness status to future data analysis as needed. # Collapse table to 1 row per unique site-pollutant-quarter data &lt;- data %&gt;% group_by(AQSID, POC, Pollutant, Year, cal_quarter) %&gt;% #Check completeness for every quarter summarize(complete = complete[1], valid_annual_samples = valid_annual_samples[1], valid_quarter_samples = valid_quarter_samples[1]) # Collapse table to 1 row per unique site-pollutant-year data &lt;- data %&gt;% group_by(AQSID, POC, Pollutant, Year) %&gt;% summarize(complete = sum(complete, na.rm = T) == 4, annual_samples = valid_annual_samples[1], min_quarter_samples = min(valid_quarter_samples, na.rm = T)) Figure 6.4: Collapsed table showing completeness status. Contributors Dorian Kvale, Cassie McMahon References Appendices of 40 CFR 50 "],
["summary-methods.html", "Summary methods", " Summary methods The following sections describe the methods used to calculate numeric summaries of the air monitoring data meets. "],
["summary-statistics.html", " Summary statistics 7.1 Bootstrapping 7.2 Below the detection limit 7.3 Upper confidence limits (UCLs) 7.4 Annual summaries for incomplete data 7.5 Comparison of data to inhalation health benchmarks", " Summary statistics The summary methods described in this chapter inlcude: 7.1 Bootstrapping 7.2 Below the detection limit 7.3 Confidence intervals 7.4 Annual summaries for incomplete data 7.5 Comparison of data to inhalation health benchmarks Notes Data should be tested for normality prior to determining the type of summary statistic to report. In general environmental data are non-normal and contain with many low values and a few high values. This causes the mean of the data to be higher than the median (right skewed). Many normality tests exist and they vary in sensitivity. A good article on normality tests can be found at this link: (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/) The normality test in the example script below is a recommended test for data of sample size between 5-5000 and will allow missing values. A Shapiro-Wilk test may also be performed for the typical sample size of a year of air toxics results. The script below includes a a line to switch to a Shapiro Wilk test. Click the button below to view the code for the normality test. Show R code library(readr) library(dplyr) library(stats) library(nortest) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) data_normresults &lt;- data.frame(site_name=character(), analyte=character(), year=character(), w_value=numeric(), p_value=numeric(), stringsAsFactors=FALSE) sites &lt;- unique(data$aqs_id) analytes &lt;- unique(data$pollutant) years &lt;- unique(data$year) for(i in sites[1:3]){ for(j in analytes){ for(k in years){ data_sub = filter(data , aqs_id == i, pollutant == j, year ==k) if (length(unique(data_sub$conc)) &gt; 5) { norm_test &lt;- sf.test(data_sub$conc) #norm_test &lt;- shapiro.test(data_sub$conc) print(i) print(norm_test) values &lt;- data.frame(site_name = i, analyte=j, year=k, w_value = round(norm_test$statistic, digits = 2), p_value = round(norm_test$p.value, digits = 2)) data_normresults &lt;- rbind(values, data_normresults) } } } } DT::datatable(head(data_normresults), options = dt_options) It is also suggested that one visualize the data to determine if normality may be a valid assumption. The script below are an example for visual normality tests and include explanation for their interpretation.In the Q-Q Plot, normal data will form a straight line with the theoretical distribution quantiles with little deviation from the line. This is a visual test and somewhat subjective. Normal data form histograms that produce a bell shaped curve. This web tool will allow visual inspection of air toxics data by year, site and pollutants. (https://mpca-pahs.shinyapps.io/NormViz/) Click the button below to view the code for the shiny tool above. Show R code When to use a arithmetic or geometric mean: Use an arithmetic mean when your data are normal and there are less than 5% below detection limit values per year/analyte/site [Kristie made up the 5%, but there is an EPA document looking at the impacts of BDL data. Will cite and incorporate.]. For non-automated scripts: Use a geometric mean if your data are log-normally distributed and there are less than 5% below detection limit values per year/analyte/site. 7.1 Bootstrapping Bootstrapping provides methods for calculating summary statistics without making assumptions about the distribution from which the data is sampled. Click the button below to view the bootstrapping code. Show R code Packages library(dplyr) library(readr) Our data is organized by monitoring site and date. Here’s a sample. AQS_ID Date Conc 270535501 2009-07-30 0.00148 270535501 2009-09-30 0.00064 270535501 2009-11-30 0.34256 270535501 2009-12-30 0.00064 270535502 2009-03-30 0.26219 270535502 2009-07-30 0.01113 270535502 2009-09-30 0.00044 270535502 2009-11-30 0.00127 270535502 2009-12-30 0.00113 Bootstrap function We currently use the EnvStats package to generate our summary values. It has built in functions to account for non-detect data and allows for different distributions. However, if you’re not dealing with non-detects you can use a simple loop to boot things yourself. Before you start you’ll want to set the random number generator to ensure you’ll be able to reproduce your results. I’ll use #27 below. set.seed(27) The general idea is to take a random sample with replacement from the data set, generate the statistic that you’re interested in, record it, then rinse and repeat. Below is the code for how to resample a single site. # Filter data to a single site df_site1 &lt;- filter(df, AQS_ID == AQS_ID[1]) # Pull random sample # `replace=T` allows for the same value to be pulled multiple times # `size=nrow(df)` ensures the number of observations in the new table to match the original random_df &lt;- sample(df_site1$Conc, replace = T) # Generate summary statistic quantile(random_df, 0.1) ## 10% ## 0.00064 To repeat this 3,000 times we can wrap these steps into a resample function, and then use sapply to collect the results. # Create resample function resample_Conc &lt;- function(data = df_site1$Conc, Conc_pct = 0.10){ random_df &lt;- sample(data, replace = T) quantile(random_df, Conc_pct, na.rm=T)[[1]] } # Repeat using `sapply` repeats &lt;- 3000 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(df_site1$Conc)) # The 50th percentile or median Conc median(booted_10pct, na.rm=T) ## [1] 0.00064 # Return the 95th percentile of the booted concentrations quantile(booted_10pct, 0.95, na.rm=T)[[1]] ## [1] 0.00148 # Force the 95th percentile to be a recorded value sort(booted_10pct)[repeats*.95 +1] ## [1] 0.00148 # Upper and lower confidence interval around the median quantile(booted_10pct, c(0.025, 0.5, 0.975), na.rm=T) ## 2.5% 50% 97.5% ## 0.000640 0.000640 0.103216 Automate To finish, put these steps into a boot function and run it on each site by using group_by. # Create boot function boot_low_Conc &lt;- function(data=df$Conc, Conc_pct=0.10, conf_int=0.95, repeats=3000){ alpha &lt;- (1 - conf_int)/2 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(data, Conc_pct)) # Upper and lower confidence interval around the median list(quantile(booted_10pct, c(alpha, 0.5, 1-alpha), na.rm=T)) } # Use `group_by` to send data for each site to your boot function conc_summary &lt;- group_by(df, AQS_ID) %&gt;% mutate(boot_results = boot_low_Conc(Conc, Conc_pct=0.10, conf_int=0.95)) %&gt;% summarize(Conc_10pct = quantile(Conc, 0.10, na.rm=T)[[1]], Low_CL95_conc = unlist(boot_results[1])[[1]], Boot_conc = unlist(boot_results[1])[[2]], Upper_CL95_conc = unlist(boot_results[1])[[3]]) Results The booted confidence limits: AQS_ID Conc_10pct Low_CL95_conc Boot_conc Upper_CL95_conc 270535501 0.000640 0.00064 0.000640 0.103216 270535502 0.000716 0.00044 0.000716 0.005214 7.2 Below the detection limit The annual mean or upper confidence limit is considered below the method detection limit when one of the following conditions is true: The number of censored values is greater than 80%. This is based on Cox 2006 and MPCA simulations (cite work and results)[www.google.com]. The annual 95% upper confidence limit is below the method detection limit. 7.3 Upper confidence limits (UCLs) If data is not normal, use bootstraping to generate means for each site. The 95% upper confidence limit is the 95th percentile of means generated using bootstrap sampling. The script below uses bootstrap sampling to generate 1000 means for each site, pollutant, and year and then calculates the 95% UCL by taking the 95th percentile of those means. Values below the MDL are estimated using lognormal MLE estimation for each bootstrap sample. The 95% UCL is not calculated for any site, pollutant, and year which fails to meet completeness requirements and/or minimum detection requirements. Click the button below to view the code for calculating UCLs. Show R code 7.4 Annual summaries for incomplete data 7.5 Comparison of data to inhalation health benchmarks Chronic Inhalation Health Benchmarks - The annual 95% upper confidence limit is compared to chronic inhalation health benchmarks which are chosen based on the MPCA/MDH hierarchy for inhalation health benchmarks information sources.These risk results are summarized differently for annual measured data reports (The Air Toxics Data Explorer) or if the comparison is used in a cumulative analysis (cumulative AERA) to inform air permitting or environmental review. Acute Inhalation Health Benchmarks - The 2nd highest annual value multiplied by ten is compared to acute inhalation health benchmarks. This adjustment of the 24 hour integrated value approximates an hourly maximum, since acute inhalation health benchmarks are developed to be protective for high short-term exposures (about an hour). Summarization of risk estimates for a cumulative Air Emission Risk Analaysis - Risk estimates are summed across pollutants thereby applying the concept of additivity. Since not all pollutants are measured at each monitoring site, means of sites are estimated based on population density since we asumme that pollution sources are greater near greater numbers of people. Means are calculated for mid-density sites, and rural sites. Facilities within urban areas are provided risk estimates at the closest or most representative monitor with all pollutants measured. This is described in the cumulative AERA section in the AERA guidance. (https://www.pca.state.mn.us/air/aera-guide) Cumulative AERAs utilitze the mean cumulative risk estimates for the most recent three years of monitored data. "],
["site-comparisons.html", " Site comparisons 8.1 Confidence intervals 8.2 Tools to visualize data 8.3 Beeswarm Plots 8.4 Correlation matrices", " Site comparisons Notes Steps: Chart data with box and whisker plots. Some site differences might be very apparent and not require statistical comparisons. [Need to think about what is apparent enough for a visual test, maybe no overlap of box, or only half or less overlap] look at cenboxplot with NADA packages, or use Kristies boxplot with censored values. Check for normality Chi-squared test (Is normality a valid assumption?). If the data are normal, and there is equal variance [Levenes test–generic script here] complete an ANOVA to compare sites. If there are more than 2 sites, follow with a post hoc test [I say Tukey, but Derek weigh in here]. If the data are not normal, and there is not equal variance [see generic script from above] complete a Kruskal wallis test (but there are other ones if you have &lt; or &gt; 50 samples…look into this). If there are more than 2 sites to compare, use Dunns test for post hoc site by site comparisons [Kristie has a script for this, will make it generic]. 8.1 Confidence intervals If data is not normal, use bootstraping to generate means for each site and take the differences of those means. If the lower bound of the 95% confidence interval for those differences is greater than zero, or the upper bound is less than zero, then the site means are significantly different. You cannot just genrate confidence intervals for the means individually and see if they overlap since the joint probability of both means being in the same region is lower than the marginal probabilities of each mean being in the region. It is not effective to do pairwise comparisons due to null data and non-detects. You cannot use an ANOVA test since the data more closely resembles a log-normal distribution than a normal distribution and for a log-normal distribution, the variance is not independent of the mean which means that transforming the data and doing an ANOVA test would bias the confidence interval for the difference. The script below uses bootstrap sampling to generate 1000 differences in site means which are ordered to create a 95% confidence interval for the difference in means. If the lower bound of the interval is greater than zero, then the second site has a higher mean concentration than the first site. If the upper bound of the confidence interval is less than zero, then the second site has a lower mean concentration than the first site. # Data must have names &quot;AQSID&quot;, &quot;POC&quot;, &quot;Parameter&quot;, &quot;Date&quot;, &quot;Result&quot;, &quot;MDL&quot;, &quot;Pollutant&quot; library(dplyr) library(EnvStats) library(lubridate) site_compare = function(data, site_number, Boot_Repeats = 1000) { library(EnvStats) set.seed(2017) annual_AT_means = function(air_toxics) { air_toxics = mutate(air_toxics, Year = year(ymd(Date)), Quarter = quarter(ymd(Date)) ) sample_complete = air_toxics %&gt;% group_by(AQSID, Pollutant, Year, Quarter, MDL) %&gt;% summarise(Complete = ( (sum(!is.na(Result) ) / length(Result) ) &gt;= 0.75 ) ) %&gt;% mutate(Complete = ifelse(is.na(Complete), F, Complete) ) %&gt;% group_by(AQSID, Pollutant, Year, MDL) %&gt;% summarise(Complete = all(Complete) ) enough_detects = air_toxics %&gt;% group_by(AQSID, Pollutant, Year, MDL) %&gt;% summarise(Detected = mean(Censored, na.rm = T) &lt;= 0.8 ) site_means = air_toxics %&gt;% group_by(AQSID, Pollutant, Year, MDL) %&gt;% summarise(Mean = ifelse(length(unique(Result[!is.na(Result) &amp; !Censored] ) ) &lt; 2, NA, ifelse (any(Censored, na.rm = T), elnormAltCensored(Result, Censored, method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]], mean(Result, na.rm = T) ) ) ) site_means = left_join(site_means, sample_complete, by = c(&quot;AQSID&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;MDL&quot;) ) %&gt;% left_join(enough_detects, by = c(&quot;AQSID&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;MDL&quot;) ) %&gt;% mutate(Mean = ifelse(Complete &amp; Detected, Mean, NA), ID = paste(AQSID, Pollutant, Year) ) return(site_means) } MLE_est &lt;- function(data){ results = data$Result censored = data$Censored n = sum(!is.na(results)) if (length(unique(results[!is.na(results) &amp; !censored] ) ) &lt; 2 ) { MLE_means = NA } else { random.rows = NULL random.rows = sample(which(!is.na(censored) &amp; (!censored) &amp; !duplicated(results) ), 2, replace = FALSE) random.rows = c(random.rows, sample(which(!is.na(censored)), n-2, replace = TRUE)) MLE_means = ifelse(sum(censored[random.rows], na.rm = T) == 0, mean(results[random.rows]), elnormAltCensored(results[random.rows], censored[random.rows], method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]] ) } return(MLE_means) } data = mutate(data, Result = ifelse(Censored, MDL, Result), ID = paste(AQSID, Pollutant, Year)) Bootstrap_means = replicate(Boot_Repeats, (by(data, data$ID, MLE_est) ) ) Bootstrap_means = rownames_to_column(as.data.frame(Bootstrap_means), &quot;ID&quot; ) Bootstrap_means = right_join(annual_AT_means(data), Bootstrap_means, by = &quot;ID&quot;) Bootstrap_means = Bootstrap_means %&gt;% group_by(Pollutant, Year) %&gt;% arrange(desc(AQSID == site_number), .by_group = T ) %&gt;% group_by(Pollutant, Year) %&gt;% mutate_at(vars(num_range(&quot;V&quot;, 1:Boot_Repeats)), funs(c(first(.), (. - first(.))[-1])) ) %&gt;% ungroup() LB = select(Bootstrap_means, num_range(&quot;V&quot;, 1:Boot_Repeats) ) %&gt;% apply(1, function(x) sort(-x)[floor(0.025 * Boot_Repeats)] ) UB = select(Bootstrap_means, num_range(&quot;V&quot;, 1:Boot_Repeats) ) %&gt;% apply(1, function(x) sort(-x)[ceiling(0.975 * Boot_Repeats)] ) CI = data.frame(Lower = LB, Upper = UB) CI = bind_cols(CI, Bootstrap_means) %&gt;% select(Lower:ID) %&gt;% mutate(Lower = ifelse(any(AQSID == site_number &amp; Complete &amp; Detected) &amp; AQSID != site_number &amp; Complete &amp; Detected, Lower, NA), Upper = ifelse(any(AQSID == site_number &amp; Complete &amp; Detected) &amp; AQSID != site_number &amp; Complete &amp; Detected, Upper, NA), Comparison = ifelse(Lower &gt; 0, &quot;Higher&quot;, ifelse(Upper &lt; 0, &quot;Lower&quot;, &quot;Same&quot;) ) ) return(CI) } data = read.csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;, stringsAsFactors = F) names(data)[1:10] &lt;- c(&quot;AQSID&quot;, &quot;POC&quot;, &quot;Parameter&quot;, &quot;Date&quot;,&quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data = data %&gt;% filter(AQSID %in% c(270370020, 270370470, 271230871) ) %&gt;% mutate(Censored = Result &lt; MDL) site_number = 271230871 seed = 2017 set.seed(seed) compare = site_compare(data, site_number, 50) #minimum 50 repeats 8.2 Tools to visualize data 8.3 Beeswarm Plots Beeswarm plots are useful for visualizing concentrations measured at different sites with more detail than boxplots. # Data must have columns &quot;AQSID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;, &quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot; beeswarm_plot = function(data, pollutant, year) { library(dplyr) library(ggbeeswarm) library(ggplot2) data = mutate(data, AQSID = as.factor(AQSID) ) ggplot(filter(data, Pollutant == pollutant, Year == year), aes(y = AQSID, x = Result, color = Censored) ) + geom_quasirandom(groupOnX=F) + labs(title = paste(pollutant, year), x = &quot;Result (ug/m^3)&quot; ) } library(tidyverse) data = read.csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;, stringsAsFactors = F) names(data)[1:10] &lt;- c(&quot;AQSID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data = mutate(data, Censored = Result &lt; MDL) beeswarm_plot(data, &quot;Lead&quot;, 2013) ## Warning: Removed 31 rows containing missing values (position_quasirandom). beeswarm_plot(data, &quot;Arsenic&quot;, 2013) ## Warning: Removed 31 rows containing missing values (position_quasirandom). beeswarm_plot(data, &quot;Benzene&quot;, 2013) ## Warning: Removed 33 rows containing missing values (position_quasirandom). References Visualizing uncertainty in housing data R packages: Beeswarm 8.4 Correlation matrices Correlation matrices help visualize the relationship between different pollutants at a site. library(tidyverse) correlation_plots = function(data, site) { library(tidyverse) library(corrplot) data_site &lt;- filter(data, AQSID %in% site) %&gt;% select(Date, Pollutant, Result) analytes &lt;- spread(data_site, Pollutant, Result, drop=T) analytes$Date &lt;- NULL coranalytes &lt;- cor(analytes, method=&quot;kendall&quot;, use=&quot;pairwise.complete.obs&quot;) %&gt;% as.data.frame() coranalytes &lt;- select_if(coranalytes, function(x) !all(is.na(x))) %&gt;% filter_all(any_vars(!is.na(.))) %&gt;% as.matrix() rownames(coranalytes) &lt;- colnames(coranalytes) corrplot(coranalytes, method = &quot;circle&quot;, type=&quot;lower&quot;, tl.cex=0.6) #plot matrix } data &lt;- read.csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;, stringsAsFactors = F) names(data)[1:10] &lt;- c(&quot;AQSID&quot;, &quot;POC&quot;, &quot;Param_Code&quot;, &quot;Date&quot;,&quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) data %&gt;% correlation_plots(270530962) ## corrplot 0.84 loaded ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## Warning in cor(analytes, method = &quot;kendall&quot;, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero "],
["start-finish.html", " Start to finish", " Start to finish To pull it all together we created some R packages to streamline the cleaning and analysis steps. airclean Cleaning and analysis functions for air monitoring data. airplot Plotting functions for air monitoring data. airrisk Inhalation health benchmarks for air toxics. The script below shows an example workflow that starts with a raw data set from the lab, walks through each of the analysis steps outlined in the previous sections, and finishes by producing a summary PDF report. Functions #Functions install.MPCAair.packages = function() { install.packages(c( &quot;easypackages&quot;, &quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;knitr&quot;, &quot;nortest&quot;, &quot;car&quot;, &quot;DT&quot;, &quot;RcppRoll&quot;, &quot;shiny&quot;, &quot;rsconnect&quot;, &quot;EnvStats&quot;, &quot;openair&quot;, &quot;ggbeeswarm&quot;, &quot;reshape&quot; )) } MPCA_air_libraries = function() { library(easypackages) libraries(c( &quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;knitr&quot;, &quot;stats&quot;, &quot;nortest&quot;, &quot;car&quot;, &quot;DT&quot;, &quot;RcppRoll&quot;, &quot;shiny&quot;, &quot;rsconnect&quot;, &quot;EnvStats&quot;, &quot;openair&quot;, &quot;ggbeeswarm&quot;, &quot;reshape&quot; )) } flag = function(data) { library(tidyverse) library(lubridate) data = mutate(data, AQSID_flag = is.na(AQSID), POC_flag = is.na(as.numeric(POC) ) | as.numeric(POC) &lt; 0 | as.numeric(POC) &gt; 4, Parameter_flag = is.na(as.numeric(Parameter)) | nchar(as.character(Parameter)) != 5, Date_flag = is.na(ymd(Date)), Result_flag = !is.na(Result) &amp; (abs(as.numeric(Result) ) &gt;= 900 | is.na(as.numeric(Result) ) ), Null_flag = (is.na(Result) &amp; is.na(Null_Data_Code) ) | (!is.na(Result) &amp; !is.na(Null_Data_Code) ), MDL_flag = is.character(MDL) | MDL &lt; 0, Pollutant_flag = is.na(Pollutant), any_flag = (AQSID_flag + POC_flag + Parameter_flag + Date_flag + Result_flag + Null_flag + MDL_flag + Pollutant_flag) &gt; 0 ) } remove_flagged = function(data) { library(tidyverse) return(filter(data, !any_flag) %&gt;% select(-contains(&quot;flag&quot;) ) %&gt;% mutate(Result = as.numeric(Result) ) ) } flag_duplicates = function(data) { library(tidyverse) return(data %&gt;% group_by(AQSID, POC, Parameter, Date) %&gt;% mutate(Count = n(), duplicate_flag = Count &gt; 1)) } average_duplicates = function(data) { library(tidyverse) dupe_averaging = function(Result, Censored) { Result = Result[!is.na(Result)] Censored = Censored[!is.na(Censored)] if(all(Censored, na.rm = T)) { return (mean(Result, na.rm = T)) } else { return (mean(Result[!Censored], na.rm = T ) ) } } data = data %&gt;% group_by(AQSID, POC, Parameter, Pollutant, Date, MDL) %&gt;% mutate(Result = dupe_averaging(Result, Censored), Censored = all(Censored, na.rm = T) ) %&gt;% slice(1) %&gt;% ungroup() %&gt;% mutate(Result = ifelse(is.na(Result), NA, Result), Censored = ifelse(is.na(Result), NA, Censored)) return (data) } time_series_plots = function(data) { library(shiny) library(readr) library(ggplot2) library(tidyverse) library(stringr) library(RcppRoll) library(lubridate) library(DT) library(rsconnect) colnames(data)[1:10] &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) data &lt;- mutate(data, sitePOC = paste0(aqs_id,&quot;-&quot;, poc) ) pollutant &lt;- unique(data$pollutant) site &lt;- unique(data$sitePOC) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = pollutant, selected=&quot;Benzene&quot;), selectInput(&quot;site&quot;, label=&quot;Choose a site&quot;, choices = site, selected=270535501))), column(9, plotOutput(&#39;detlim&#39;, height = &quot;400px&quot;)))), server = function(input, output) { output$detlim &lt;- renderPlot({ print(input$pollutant) print(input$site) data_sub = filter(data, pollutant==input$pollutant, sitePOC == input$site) data_sub$Censored &lt;- ifelse(data_sub$conc &gt; data_sub$md_limit, FALSE, TRUE) mdl &lt;- max(data_sub$md_limit) ggplot(data=data_sub, aes(x= date, y=conc)) + geom_point(aes(color=Censored), size =3, alpha=0.55) + geom_line() + scale_x_date(date_labels = &quot;%D&quot;) + xlab(NULL) + ylab(&quot;Result (ug/m3)&quot;) + expand_limits(y=c(0, max(data_sub$conc))) + scale_colour_manual(values= c(&quot;#197519&quot;[FALSE %in% unique(data_sub$Censored)], &quot;#0000FF&quot;[TRUE %in% unique(data_sub$Censored)]), breaks=c(FALSE, TRUE)) + theme(text = element_text(size=15), axis.text.x = element_text(angle = -90, vjust = 0.3, size=14)) + ggtitle(paste0(&quot;Time series for &quot;, input$pollutant, &quot; at site &quot;, input$site)) }) }) } POC_compare = function(data) { library(shiny) library(tidyverse) library(rsconnect) data = distinct(data, AQSID, POC, Parameter, Date, Pollutant, Year, .keep_all = T) #replace with better cleaning function data = spread(data, POC, Result) %&gt;% mutate(Status = ifelse(`1` &lt; MDL &amp; `2` &lt; MDL, &quot;POCs 1 and 2 below MDL&quot;, ifelse(`1` &lt; MDL, &quot;POC 1 below MDL&quot;, ifelse(`2` &lt; MDL, &quot;POC 2 below MDL&quot;, &quot;POCs 1 and 2 above MDL&quot;) ) ) ) %&gt;% drop_na(Status) Pollutant &lt;- unique(data$Pollutant) Site &lt;- unique(data$AQSID) Year &lt;- unique(data$Year) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;Pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = Pollutant), selectInput(&quot;Year&quot;, label=&quot;Choose a year&quot;, choices = Year), selectInput(&quot;Site&quot;, label=&quot;Choose a site&quot;, choices = Site))), column(9, plotOutput(&#39;normviz&#39;, height = &quot;500px&quot;)))), server = function(input, output) { output$normviz &lt;- renderPlot({ print(input$Pollutant) print(input$Site) print(input$Year) data_sub = filter(data, Pollutant==input$Pollutant, AQSID == input$Site, Year == input$Year) ggplot(data_sub, aes(x = `1`, y = `2`, color = Status)) + geom_point(size = 3) + geom_segment(x=-1000, xend=1000, y=-1000, yend=1000, color=&quot;red&quot;, size=1) + labs(title = &quot;POC comparison chart&quot;, x = &quot;POC 1&quot;, y = &quot;POC 2&quot;, subtitle = paste(&quot;Correlation =&quot;, round(cor(data_sub$`1`,data_sub$`2`, use = &quot;complete&quot;), 2) ) ) }) }) } POC_average = function(data) { library(tidyverse) POC_averaging = function(Result, Censored) { Result = Result[!is.na(Result)] Censored = Censored[!is.na(Censored)] if(all(Censored, na.rm = T)) { return (mean(Result, na.rm = T)) } else { return (mean(Result[!Censored], na.rm = T ) ) } } data = data %&gt;% group_by(AQSID, Parameter, Pollutant, Date, MDL) %&gt;% mutate(Result = POC_averaging(Result, Censored), Censored = all(Censored, na.rm = T) ) %&gt;% slice(1) %&gt;% select(-POC) %&gt;% ungroup() %&gt;% mutate(Result = ifelse(is.na(Result), NA, Result), Censored = ifelse(is.na(Result), NA, Censored)) return (data) } completeness_check = function(data) { # Create a sampling calendar based on EPA&#39;s air toxics monitoring schedule sample_calendar &lt;- function(start = &quot;2012-01-01&quot;, end = &quot;2016-12-31&quot;, day_interval = 6, type = &quot;air_toxics&quot;) { library(lubridate) # Convert &#39;start&#39; and &#39;end&#39; to class date start &lt;- ymd(start) end &lt;- ymd(end) # Set official start date to selected EPA calendar if(type == &quot;air_toxics&quot;) { epa_start &lt;- ymd(&quot;1990-01-09&quot;) } else { epa_start &lt;- start } # Create full table of sampling dates calendar &lt;- seq(from = epa_start, to = end, by = paste(day_interval, &quot;days&quot;)) # Subset to user&#39;s date range calendar &lt;- calendar[calendar &gt;= start &amp; calendar &lt;= end] return(calendar) } # Find the year range of your data date_range &lt;- range(data$Date) # Create expected sample calendar epa_schedule &lt;- data_frame(Date = sample_calendar(start = format(date_range[1], &quot;%Y-01-01&quot;), #Extend range to first day of the year end = format(date_range[2], &quot;%Y-12-31&quot;), #Extend range to last day of the year day_interval = 6)) # Add year and calendar quarter columns epa_schedule &lt;- epa_schedule %&gt;% mutate(Year = year(Date), cal_quarter = quarter(Date)) # Count the expected number of samples per quarter and year. epa_schedule &lt;- epa_schedule %&gt;% group_by(Year, cal_quarter) %&gt;% summarize(expected_quarter_samples = length(unique(Date))) %&gt;% group_by(Year) %&gt;% mutate(expected_annual_samples = sum(expected_quarter_samples)) # Assign each date to a calendar quarter data &lt;- data %&gt;% mutate(cal_quarter = quarter(Date)) # Count the number of sampling dates for each quarter and year. data &lt;- data %&gt;% group_by(AQSID, Parameter, Pollutant, Year, cal_quarter) %&gt;% mutate(valid_quarter_samples = length(unique(Date[!is.na(Result)]))) %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% mutate(valid_annual_samples = length(unique(Date[!is.na(Result)]))) # Join expected sample table to data by quarter and year columns data &lt;- left_join(data, epa_schedule, by = c(&quot;Year&quot;, &quot;cal_quarter&quot;)) # Divide valid samples by expected samples data &lt;- data %&gt;% group_by(AQSID, Parameter, Pollutant, Year, cal_quarter) %&gt;% summarise(pct_quarter_samples = round(valid_quarter_samples[1] / expected_quarter_samples[1], 2)) %&gt;% mutate(Complete = pct_quarter_samples &gt;= 0.75) %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% summarise(Complete = sum(Complete, na.rm = T) == 4) return(data %&gt;% select(AQSID, Parameter, Pollutant, Year, Complete) %&gt;% ungroup() ) } UCL_95 = function(data, Boot_Repeats = 1000) { library(EnvStats) set.seed(2017) annual_AT_means = function(air_toxics) { air_toxics = mutate(air_toxics, Year = year(ymd(Date)), Quarter = quarter(ymd(Date)) ) sample_complete = air_toxics %&gt;% completeness_check() enough_detects = air_toxics %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% summarise(Detected = mean(Censored, na.rm = T) &lt;= 0.8 ) site_means = air_toxics %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% summarise(Mean = ifelse(length(unique(Result[!is.na(Result) &amp; !Censored] ) ) &lt; 2, NA, ifelse (any(Censored, na.rm = T), elnormAltCensored(Result, Censored, method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]], mean(Result, na.rm = T) ) ) ) site_means = left_join(site_means, sample_complete, by = c(&quot;AQSID&quot;, &quot;Parameter&quot;, &quot;Pollutant&quot;, &quot;Year&quot;) ) %&gt;% left_join(enough_detects, by = c(&quot;AQSID&quot;, &quot;Parameter&quot;, &quot;Pollutant&quot;, &quot;Year&quot;) ) %&gt;% mutate(Mean = ifelse(Complete &amp; Detected, Mean, NA), ID = paste(AQSID, Parameter, Pollutant, Year) ) return(site_means) } MLE_est &lt;- function(data){ results = data$Result censored = data$Censored n = sum(!is.na(results)) if (length(unique(results[!is.na(results) &amp; !censored] ) ) &lt; 2 ) { MLE_means = NA } else { random.rows = NULL random.rows = sample(which(!is.na(censored) &amp; (!censored) &amp; !duplicated(results) ), 2, replace = FALSE) random.rows = c(random.rows, sample(which(!is.na(censored)), n-2, replace = TRUE)) MLE_means = ifelse(sum(censored[random.rows], na.rm = T) == 0, mean(results[random.rows]), elnormAltCensored(results[random.rows], censored[random.rows], method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]] ) } return(MLE_means) } data = mutate(data, ID = paste(AQSID, Parameter, Pollutant, Year), Result = ifelse(Censored, MDL, Result) ) Bootstrap_means = replicate(Boot_Repeats, (by(data, data$ID, MLE_est) ) ) CL = apply(Bootstrap_means, 1, function(x) sort(x)[ceiling(0.95 * Boot_Repeats)] ) CL = data.frame(ID = names(CL), UCL95 = unname(CL)) annual_summary = left_join(annual_AT_means(data), CL, by = &quot;ID&quot;) %&gt;% mutate(UCL95 = ifelse(Complete &amp; Detected, UCL95, NA) ) %&gt;% select(-ID) return(annual_summary) } site_compare = function(data, site_number, Boot_Repeats = 1000) { library(EnvStats) set.seed(2017) annual_AT_means = function(air_toxics) { air_toxics = mutate(air_toxics, Year = year(ymd(Date)), Quarter = quarter(ymd(Date)) ) sample_complete = air_toxics %&gt;% completeness_check() enough_detects = air_toxics %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% summarise(Detected = mean(Censored, na.rm = T) &lt;= 0.8 ) site_means = air_toxics %&gt;% group_by(AQSID, Parameter, Pollutant, Year) %&gt;% summarise(Mean = ifelse(length(unique(Result[!is.na(Result) &amp; !Censored] ) ) &lt; 2, NA, ifelse (any(Censored, na.rm = T), elnormAltCensored(Result, Censored, method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]], mean(Result, na.rm = T) ) ) ) site_means = left_join(site_means, sample_complete, by = c(&quot;AQSID&quot;, &quot;Parameter&quot;, &quot;Pollutant&quot;, &quot;Year&quot;) ) %&gt;% left_join(enough_detects, by = c(&quot;AQSID&quot;, &quot;Parameter&quot;, &quot;Pollutant&quot;, &quot;Year&quot;) ) %&gt;% mutate(Mean = ifelse(Complete &amp; Detected, Mean, NA), ID = paste(AQSID, Parameter, Pollutant, Year) ) return(site_means) } MLE_est &lt;- function(data){ results = data$Result censored = data$Censored n = sum(!is.na(results)) if (length(unique(results[!is.na(results) &amp; !censored] ) ) &lt; 2 ) { MLE_means = NA } else { random.rows = NULL random.rows = sample(which(!is.na(censored) &amp; (!censored) &amp; !duplicated(results) ), 2, replace = FALSE) random.rows = c(random.rows, sample(which(!is.na(censored)), n-2, replace = TRUE)) MLE_means = ifelse(sum(censored[random.rows], na.rm = T) == 0, mean(results[random.rows]), elnormAltCensored(results[random.rows], censored[random.rows], method = &quot;impute.w.mle&quot;, ci = F)$parameters[[1]] ) } return(MLE_means) } data = mutate(data, Result = ifelse(Censored, MDL, Result), ID = paste(AQSID, Parameter, Pollutant, Year)) Bootstrap_means = replicate(Boot_Repeats, (by(data, data$ID, MLE_est) ) ) Bootstrap_means = rownames_to_column(as.data.frame(Bootstrap_means), &quot;ID&quot; ) Bootstrap_means = right_join(annual_AT_means(data), Bootstrap_means, by = &quot;ID&quot;) Bootstrap_means = Bootstrap_means %&gt;% group_by(Parameter, Pollutant, Year) %&gt;% arrange(desc(AQSID == site_number), .by_group = T ) %&gt;% group_by(Parameter, Pollutant, Year) %&gt;% mutate_at(vars(num_range(&quot;V&quot;, 1:Boot_Repeats)), funs(c(first(.), (. - first(.))[-1])) ) %&gt;% ungroup() LB = select(Bootstrap_means, num_range(&quot;V&quot;, 1:Boot_Repeats) ) %&gt;% apply(1, function(x) sort(-x)[floor(0.025 * Boot_Repeats)] ) UB = select(Bootstrap_means, num_range(&quot;V&quot;, 1:Boot_Repeats) ) %&gt;% apply(1, function(x) sort(-x)[ceiling(0.975 * Boot_Repeats)] ) CI = data.frame(Lower = LB, Upper = UB) CI = bind_cols(CI, Bootstrap_means) %&gt;% select(Lower:ID) %&gt;% group_by(Pollutant, Year) %&gt;% mutate(Lower = ifelse(any(AQSID == site_number &amp; Complete &amp; Detected) &amp; AQSID != site_number &amp; Complete &amp; Detected, Lower, NA), Upper = ifelse(any(AQSID == site_number &amp; Complete &amp; Detected) &amp; AQSID != site_number &amp; Complete &amp; Detected, Upper, NA), Comparison = ifelse(Lower &gt; 0, &quot;Higher&quot;, ifelse(Upper &lt; 0, &quot;Lower&quot;, &quot;Same&quot;) ) ) return(CI %&gt;% ungroup() ) } correlation_plots = function(data, site) { library(tidyverse) library(corrplot) data_site &lt;- filter(data, AQSID %in% site) %&gt;% select(Date, Pollutant, Result) analytes &lt;- spread(data_site, Pollutant, Result, drop=T) analytes$Date &lt;- NULL coranalytes &lt;- cor(analytes, method=&quot;kendall&quot;, use=&quot;pairwise.complete.obs&quot;) %&gt;% as.data.frame() coranalytes &lt;- select_if(coranalytes, function(x) !all(is.na(x))) %&gt;% filter_all(any_vars(!is.na(.))) %&gt;% as.matrix() rownames(coranalytes) &lt;- colnames(coranalytes) return(corrplot(coranalytes, method = &quot;circle&quot;, type=&quot;lower&quot;, tl.cex=0.6) )#plot matrix } beeswarm_plot = function(data, pollutant, year) { library(dplyr) library(ggbeeswarm) library(ggplot2) data = mutate(data, AQSID = as.factor(AQSID) ) ggplot(filter(data, Pollutant == pollutant, Year == year), aes(y = AQSID, x = Result, color = Censored) ) + geom_quasirandom(groupOnX=F) + labs(title = paste(pollutant, year), x = &quot;Result (ug/m^3)&quot; ) } pollution_roses = function(data, met_data_filepath, num_breaks = 5) { #Met data must be in Tableau format library(tidyverse) library(lubridate) library(openair) library(reshape) library(shiny) library(rsconnect) data$Date = ymd(data$Date) met_data = read.csv(met_data_filepath) names(met_data)[c(1,8,9)] = c(&quot;Day&quot;,&quot;wd&quot;,&quot;ws&quot;) met_data = mutate(met_data, date = paste0(Year,&quot;/&quot;,Month,&quot;/&quot;,Day,&quot; &quot;,Hour,&quot;:00&quot;)) met_data$date = ymd_hm(met_data$date) met_data = met_data[,-c(1:3,10)] met_data = timeAverage(met_data, avg.time = &quot;day&quot;) met_data$date = ymd(met_data$date) data = left_join(data, met_data, by = c(&quot;Date&quot; = &quot;date&quot;)) Pollutant &lt;- unique(data$Pollutant) Site &lt;- unique(data$AQSID) Year &lt;- unique(data$Year) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;Pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = Pollutant), selectInput(&quot;Year&quot;, label=&quot;Choose a year&quot;, choices = Year), selectInput(&quot;Site&quot;, label=&quot;Choose a site&quot;, choices = Site))), column(9, plotOutput(&#39;normviz&#39;, height = &quot;500px&quot;)))), server = function(input, output) { output$normviz &lt;- renderPlot({ print(input$Pollutant) print(input$Site) print(input$Year) data_sub = filter(data, Pollutant==input$Pollutant, AQSID == input$Site, Year == input$Year, !is.na(Result)) data_sub = data_sub %&gt;% mutate(MDL = max(MDL), minimum = min(Result), maximum = max(Result), Result = ifelse(Censored, 1e-16, Result)) breaks_site = NULL if(!all(data_sub$Censored)){ breaks_site = c(breaks_site, 0, round_any( c(data_sub$MDL[1], data_sub$MDL[1] + (data_sub$maximum[1] - data_sub$MDL[1]) * (1:(num_breaks-1) / (num_breaks-1) ) ), 0.0001, ceiling ) ) pollutionRose(data_sub, statistic = &quot;abs.count&quot;, pollutant = &quot;Result&quot;, breaks = breaks_site, key.footer=&quot;ug/m3&quot;, main=paste(&quot;Daily Average Pollution Rose for&quot;, data_sub$Pollutant[1],&quot;\\n&quot;, data_sub$AQSID[1]) ) } else { breaks_site = c(breaks_site, c(0, round_any( c(data_sub$MDL[1], 2*data_sub$MDL[1] ), 0.0001, ceiling ) ) ) pollutionRose(data_sub, statistic = &quot;abs.count&quot;, pollutant = &quot;Result&quot;, breaks = breaks_site, key.footer=&quot;ug/m3&quot;, main=paste(&quot;Daily Average Pollution Rose for&quot;, data_sub$Pollutant[1],&quot;\\n&quot;, data_sub$AQSID[1]) ) } }) }) } Analysis Steps #Import data MPCA_air_libraries() library(tidyverse) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) names(data)[1:10] &lt;- c(&quot;AQSID&quot;, &quot;POC&quot;, &quot;Parameter&quot;, &quot;Date&quot;,&quot;Result&quot;, &quot;Null_Data_Code&quot;, &quot;MDL&quot;, &quot;Pollutant&quot;, &quot;Year&quot;, &quot;CAS&quot;) #1. Data cleaning flagged = data %&gt;% flag() #Check with QA about flagged values if necessary. duplicates_flagged = data %&gt;% flag_duplicates() data = data %&gt;% mutate(Censored = Result &lt; MDL) %&gt;% average_duplicates() #We decide to average the duplicates here #2. Data Validation time_series = data %&gt;% time_series_plots() #3. Collocated monitors poc_comparisons = data %&gt;% POC_compare() data = data %&gt;% POC_average() #4. Detection limits #5. Completeness checks complete = data %&gt;% completeness_check() #6. Summary statistics annual_summary = data %&gt;% filter(AQSID == 270370020) %&gt;% UCL_95(100) #7. Site Comparisons site_number = 270370020 #Our favorite site comparisons_to_FH = data %&gt;% filter(AQSID %in% c(270370020, 270370470, 271230871) ) %&gt;% site_compare(site_number, 50) pollutant_correlations = data %&gt;% correlation_plots(270370020) arsenic_beeswarm_plot = data %&gt;% beeswarm_plot(&quot;Arsenic&quot;, 2013) #8. Pollution Roses poll_rose_data = filter(data, Year == 2013) met_data_filepath = &quot;X:/Programs/Air_Quality_Programs/Air Monitoring Data and Risks/0 Methods and documentation/3. Analysis methods/Web book/air-methods/MSP met data 2013.csv&quot; #Need online filepath num_breaks = 5 pol_roses = poll_rose_data %&gt;% pollution_roses(met_data_filepath, num_breaks) Contributors Dorian, Derek Back to top "],
["charts.html", " Charts 10.1 Boxplots 10.2 Calendar plots 10.3 Color palettes 10.4 Pollution roses", " Charts 10.1 Boxplots Dividing by zero is an amazing super power. Unfortunately for you your computer does not have this power. Making a log boxplot with data containing zeros or negative values will result in your computer melting itself. It will never forgive you. 10.1.1 Log boxplots Coming soon… 10.1.2 Outliers Coming soon… 10.2 Calendar plots The openair package provides a convenient function, calendarPlot, for presenting data in a calendar format. ## Parsed with column specification: ## cols( ## AQSID = col_integer(), ## POC = col_integer(), ## Param_Code = col_integer(), ## Date = col_date(format = &quot;&quot;), ## Concentration = col_double(), ## Null_Code = col_character(), ## Dlimit = col_double(), ## Pollutant = col_character(), ## Year = col_integer(), ## CAS = col_character() ## ) 10.3 Color palettes 10.4 Pollution roses Pollution roses are a visual display of pollutant concentrations and wind directions correspnding to those concentrations. The length of each “paddle” is correlated with the percentage of days of valid measurements taken when the wind was blowing from that direction. So if the longest paddle is the one extending upward, then the wind blew from the North on average more times than any other direction for days with valid measurements of a pollutant. The colors on a paddle correspond to the concentration of the pollutant. Blue means lower concentrations up to red which are the highest concentrations. Note that for 24-hour samples, wind direction is the vector averaged wind direction of each hour in a day. Sample R script pollution_roses = function(data, met_data_filepath, num_breaks = 5) { #Met data must be in Tableau format library(tidyverse) library(lubridate) library(openair) library(reshape) library(shiny) library(rsconnect) data$Date = ymd(data$Date) met_data = read.csv(met_data_filepath) names(met_data)[c(1,8,9)] = c(&quot;Day&quot;,&quot;wd&quot;,&quot;ws&quot;) met_data = mutate(met_data, date = paste0(Year,&quot;/&quot;,Month,&quot;/&quot;,Day,&quot; &quot;,Hour,&quot;:00&quot;)) met_data$date = ymd_hm(met_data$date) met_data = met_data[,-c(1:3,10)] met_data = timeAverage(met_data, avg.time = &quot;day&quot;) met_data$date = ymd(met_data$date) data = left_join(data, met_data, by = c(&quot;Date&quot; = &quot;date&quot;)) Pollutant &lt;- unique(data$Pollutant) Site &lt;- unique(data$AQSID) Year &lt;- unique(data$Year) shinyApp( ui = fluidPage(responsive = FALSE, fluidRow( column(3, style = &quot;padding-bottom: 20px;&quot;, inputPanel( selectInput(&quot;Pollutant&quot;, label=&quot;Choose a pollutant&quot;, choices = Pollutant), selectInput(&quot;Year&quot;, label=&quot;Choose a year&quot;, choices = Year), selectInput(&quot;Site&quot;, label=&quot;Choose a site&quot;, choices = Site))), column(9, plotOutput(&#39;normviz&#39;, height = &quot;500px&quot;)))), server = function(input, output) { output$normviz &lt;- renderPlot({ print(input$Pollutant) print(input$Site) print(input$Year) data_sub = filter(data, Pollutant==input$Pollutant, AQSID == input$Site, Year == input$Year, !is.na(Result)) data_sub = data_sub %&gt;% mutate(MDL = max(MDL), minimum = min(Result), maximum = max(Result), Result = ifelse(Censored, 1e-16, Result)) breaks_site = NULL if(!all(data_sub$Censored)){ breaks_site = c(breaks_site, 0, #c(round_any(data_sub$minimum[1], 0.001, floor), round_any( c(data_sub$MDL[1], data_sub$MDL[1] + (data_sub$maximum[1] - data_sub$MDL[1]) * (1:(num_breaks-1) / (num_breaks-1) ) ), 0.0001, ceiling ) ) pollutionRose(data_sub, statistic = &quot;abs.count&quot;, pollutant = &quot;Result&quot;, breaks = breaks_site, key.footer=&quot;ug/m3&quot;, main=paste(&quot;Daily Average Pollution Rose for&quot;, data_sub$Pollutant[1],&quot;\\n&quot;, data_sub$AQSID[1]) ) } else { breaks_site = c(breaks_site, c(round_any(0, 0.0001, floor), round_any( c(data_sub$MDL[1], 2*data_sub$MDL[1] ), 0.0001, ceiling ) ) ) pollutionRose(data_sub, statistic = &quot;abs.count&quot;, pollutant = &quot;Result&quot;, breaks = breaks_site, key.footer=&quot;ug/m3&quot;, main=paste(&quot;Daily Average Pollution Rose for&quot;, data_sub$Pollutant[1],&quot;\\n&quot;, data_sub$AQSID[1]) ) } }) }) } "],
["time-series.html", " Time series 11.1 Seasonality", " Time series 11.1 Seasonality The trend is up. "],
["maps.html", " Maps 12.1 Kriging 12.2 Spatial averaging and aggregation 12.3 Creating shapefiles in R adn joining to EJ areas", " Maps 12.1 Kriging Coming soon… 12.2 Spatial averaging and aggregation To average across… 12.3 Creating shapefiles in R adn joining to EJ areas I don’t really remember the points of this. This bit of script creates a shapefile from air monitoring data with coordinates and then joins to the EJ status for census tracts. Let’s discuss how to make this useful. library(tidyverse) library(stringr) library(RcppRoll) library(lubridate) library(car) library(DT) data &lt;- read_csv(&#39;https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv&#39;) colnames(data) &lt;- c(&quot;aqs_id&quot;, &quot;poc&quot;, &quot;param_code&quot;, &quot;date&quot;, &quot;conc&quot;, &quot;null_code&quot;, &quot;md_limit&quot;, &quot;pollutant&quot;, &quot;year&quot;, &quot;cas&quot;) dt_options &lt;- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F) ######################################################################## ## Spatial join to census tracts and then left_join to EJ areas status # ######################################################################## coords &lt;- monitoring_locations[, c(&quot;Longitude&quot;, &quot;Latitude&quot;)] point_source_wypoints &lt;- SpatialPointsDataFrame(coords, data=data, proj4string = CRS(&#39;+proj=longlat +ellps=WGS84&#39;)) point_source_wypoints &lt;- spTransform(point_source_wypoints, CRS(&#39;+proj=utm +zone=15 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0&#39;)) census_tracts &lt;- readOGR(dsn=&quot;R:/demographics&quot;, layer=&quot;census_2010_tracts_usboc&quot;, stringsAsFactors = FALSE) point_sources_files_geo &lt;- point.in.poly(point_source_wypoints, census_tracts) writeOGR(obj=point_sources_files_geo, dsn=&quot;X:/Programs/Air_Quality_Programs/Air Monitoring Data and Risks/0 Methods and documentation/3. Analysis methods/Web book/air-methods&quot;, layer=&quot;airmonitors_tracts&quot;, driver=&quot;ESRI Shapefile&quot;) point_sources_tracts_dbf &lt;- read.dbf(&quot;X:/Programs/Air_Quality_Programs/Air Monitoring Data and Risks/0 Methods and documentation/3. Analysis methods/Web book/air-methods/airmonitors_tracts.dbf&quot;, as.is=TRUE) ej_layers &lt;- read.dbf(&quot;X:/Agency_Files/EJ/GIS/Shapefiles/ACS_2014_5Yr_Tract_MNPovPPC.dbf&quot;, as.is=TRUE) ej_layers &lt;- ej_layers[, c(&quot;GEOID&quot;, &quot;ov50per_nw&quot;, &quot;prp_under1&quot;)] point_sources_tracts_ej &lt;- left_join(point_sources_tracts_dbf, ej_layers, by=c(&quot;GEOID10&quot;=&quot;GEOID&quot;)) names(point_sources_tracts_ej) &lt;- c(&quot;Facility_ID&quot;, &quot;Facility_Name&quot;, &quot;Resident_Cancer_Risk&quot;, &quot;Resident_Hazard_Quotient&quot;, &quot;CAS&quot;, &quot;Pollutant&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;, &quot;Annual_PM25_Concentration&quot;, &quot;STATEFP&quot;, &quot;COUNTYF&quot;, &quot;TRACTCE&quot;, &quot;GEOID10&quot;, &quot;NAME10&quot;, &quot;NAMELSA&quot;, &quot;MTFCC10&quot;, &quot;FUNCSTA&quot;, &quot;ALAND10&quot;, &quot;AWATER1&quot;, &quot;INTPTLA&quot;, &quot;INTPTLO&quot;, &quot;ov50per_nw&quot;, &quot;prp_under1&quot;) #Output file name (including path) point_sources_tracts_ej &lt;- unique(point_sources_tracts_ej) "],
["additional-resources.html", " Additional resources", " Additional resources Suggested readings: https://webgate.ec.europa.eu/fpfis/mwikis/essvalidserv/images/a/ad/PRACTICAL_GUIDE_TO_DATA_VALIDATION.pdf https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf Data validation workbook – Kristie will do thing https://academic.oup.com/annweh/article/54/3/257/223531/Much-Ado-About-Next-to-Nothing-Incorporating - Review article by Helsel about non-detects and substitution methods. https://pdfs.semanticscholar.org/182e/9278fc36dd73d48a414b8fbc30a44ea314d3.pdf A Chemosphere review article describing why BDL substitution should not be done. EPA, TADD Nothing over 80% detection paper: http://www.tandfonline.com/doi/pdf/10.1080/10473289.2006.10464576 "],
["mpca-data-tools.html", "MPCA data tools", " MPCA data tools The following sections contain the methods used to produce MPCA’s public data products. "],
["aqi-explorer.html", " AQI explorer", " AQI explorer This section describes the methods used to summarize the data shown in MPCA’s AQI explorer. "],
["criteria-pollutant-explorer.html", " Criteria pollutant explorer", " Criteria pollutant explorer This section describes the methods used to summarize the data shown in MPCA’s Criteria pollutant explorer. "],
["air-toxics-explorer.html", " Air toxics explorer 16.1 MPCA Air Toxics Methods", " Air toxics explorer This section describes the methods used to summarize the data shown in MPCA’s Air toxics data explorer. 16.1 MPCA Air Toxics Methods The MPCA conducts ambient air monitoring at many sites across the state in order to help assess air quality and health risks caused by pollutants in the air. Pollutant concentrations measured by air monitors help to quantify air quality and health risks. However, the results obtained from air monitors, also known as ambient air monitoring data, are not perfect. Air monitoring data often contains missing results for various reasons, erroneous values that may not reflect actual ambient air concentrations, and small values that cannot be reliably measured using current methods (non-detected values). Since monitoring data often has these irregularities, the MPCA has specific methods in place for analyzing air toxics data to account for irregularities (EPA has specific mandated practices for assessing criteria pollutants that are not covered in this document). This document explains the methods MPCA uses to analyze air toxics data. 16.1.1 Data Cleaning Null Results Any text values in the results column, such as null codes, are changed to blank cells. Any numeric results that appear to be placeholders for null values, such as 985 or -999, are changed to blank cells to reflect that they are null values. All values below the detection limit are left as-is during data cleaning, but are replaced with estimates later during analysis. Duplicate Observations If there are multiple results for a pollutant at the same site, time, and POC, then a single result must be selected for that site, time, and POC to avoid affecting summary statistics with extra samples which should not exist. The first step should be to consult the lab to determine why there is a duplicate observation and attempt to determine which one best reflects the conditions at the site and time. If consulting with the lab does not lead to a definitive conclusion for which result to keep, or if consulting with the lab is not a possibility, there these steps are recommended: If there is only one result greater than or equal to the detection limit (see information about method detection limits below), use that result. If there are multiple results greater than or equal to the detection limit, use the average of those results. If there are no results greater than or equal to the detection limit, use the result with the lowest detection limit. If all results have the same detection limit, then select any one of the results since it does not matter which one is selected. 16.1.2 Data Validation Monitoring results may be flagged for further review if there is a possibility that those values do not reflect actual ambient air concentrations. A flagged value is not immediately changed to null. Flagged values should be kept for all analysis until involved parties agree that they should be changed to null. Flagged values should only be removed without consultation if they significantly affect analysis results and there is a strong likelihood that those values do not accurately reflect actual ambient air concentrations at the site. When in doubt, keep flagged values as-is in all analysis. It is often a good idea to view a time series on monitoring results using software such as Tableau or R to spot irregularities in monitoring results that may be flagged immediately or after further review. Any values that appear extreme either on the high or low end are flagged for further investigation. Depending on feedback from the lab, QA, and/or air monitoring unit, flagged values may either be kept as-is or changed to blank if it is determined that those values are not appropriate to be included for the purpose of the analysis. Values which are equal in consecutive samples may be flagged depending on precision of the value and the number of consecutive samples with that value. For example, a value of 0.1234 in consecutive samples may be flagged for further review, but a value of 5 in consecutive samples for a method which produces values rounded to the nearest whole number may not be flagged unless that value is repeated in enough consecutive samples to be considered suspect. Flagging consecutive repeated values is a judgement call as to whether those values reflect no change in ambient air concentrations at a site, or whether those values likely represent the machine “sticking”. Other results which appear to not reflect ambient air concentrations may also be flagged. For example, a decreasing trend in results which may indicate a sample leak or calibration issue may also be flagged. 16.1.3 Parameter Occurrence Codes (POCs) Sometimes, there are multiple monitors at a site collecting samples for a pollutant. Each one of these monitors is assigned a POC. If there are multiple POCs at a site collecting samples at the same time, then a single result must be selected from those samples to avoid affecting summary statistics with extra samples caused by having multiple monitors at a site. The EPA recommends these steps for handling results from multiple POCs: If no results are valid, then the result for that time should be null. If there is only one valid result, use that result. If there is only one valid result greater than or equal to the detection limit, use that result. If there are multiple valid results greater than or equal to the detection limit, use the average of those results. If there are valid results, but none greater than or equal to the detection limit, then select the one with the lowest detection limit. If all results have the same detection limit, then use any result. 16.1.4 Producing Annual Summaries It is useful to calculate and report summary statistics for a calendar year such as a mean concentration or confidence interval for the mean concentration. Compilations of these summary statistics for reporting are referred to as “annual summaries”. Since ambient air monitoring data has results below detection limits, null results, and results that are flagged and removed, the reliability of annual summary statistics as a measure of true ambient air conditions can vary. Therefore, the MPCA has rules for analysis and reporting of air toxics for annual summaries that attempt to ensure all reported annual summary statistics approximately reflect true ambient air conditions at a site. Data Completeness For any summary statistic calculated from a sample, the reliability of that statistic improves as the sample size increases. Summary statistics calculated from small samples are not considered very reliable while summary statistics calculated from large samples are considered reliable. The MPCA has data completeness requirements so unreliable summary statistics are not reported. The EPA recommends that at least 75 percent of scheduled samples in a calendar year for a pollutant at a site produce a valid measurement (non-null result) in order to report summary statistics for the pollutant at that site. If fewer than 75 percent of scheduled samples in a calendar year produce a valid measurement, then annual summary statistics should not be reported for the pollutant at that site for that year. It may be reported that the summary statistic could not be calculated because this requirement was not met. EPA recommends that every 3-month quarter of a calendar year also have at least 75 percent of scheduled samples in that quarter produce a valid measurement. If any quarter of the year does not meet this requirement, then annual summary statistics should not be reported for the pollutant at that site for that year. It may be reported that the summary statistic could not be calculated because this requirement was not met. Results below the detection limit The precision of results below the detection limit is lower than the precision of results above the detection limit. However, there is strong confidence that monitor results below the detection limit indicate that the true ambient air concentration of a pollutant at the site is between zero and the method detection limit. The goal for values below the detection limit is to avoid making erroneous assumptions about values below the detection limit while still taking advantage of the information that they provide about ambient air concentrations. These are not recommended procedures for handling results below the detection limit Replace with blank cell. This treats results below the detection limit the same as null values and eliminates the useful information they provide. It generally biases annual summaries high. Replace with zero. While it is possible that the true ambient air concentration is zero, it is usually greater than zero. Replacing with zero biases summary statistics low. Replace with the detection limit. While it is possible that the true ambient air concentration is very close to the detection limit, it is often lower than the detection limit. Replacing with zero biases summary statistics low. Replace with half of the detection limit. This is considered a compromise between the two options above. It doesn’t bias summary statistics as much as the previous two options, but there are more sophisticated estimates for results below the detection limit which utilize results above the detection limit to help estimate results below the detection limit. The MPCA has elected to use maximum likelihood estimation to approximate values below the detection limit. The MPCA currently uses maximum likelihood estimation assuming a normal distribution for pollutant concentrations. For information about maximum likelihood estimation, visit . For information about why the MPCA uses maximum likelihood estimation instead of other non-detect estimation methods such as Kaplan Meier, visit . Maximum likelihood estimation requires a sufficient number of results above the detection limit in order to reliably estimate summary statistics. If more than 80 percent of results are below the detection limit, the estimates are unreliable. In that case, reliable annual summary statistics cannot be calculated and should not be reported. It may be reported that the summary statistic could not be calculated because there were not enough results above the detection limit. Maximum likelihood estimation also requires at least two results above the detection limit that are not equal. If there is one or no unique results above the detection limit, then maximum likelihood estimation cannot be used and annual summary statistics should not be reported. Calculating annual means In order to calculate an annual mean of an air toxic concentration, these criteria must be met as described above in data completeness and results below the detection limit: At least 75 percent of scheduled samples in the calendar year must produce a valid measurement. In addition, at least 75 percent of scheduled samples in each quarter must produce a valid measurement. At least 20 percent of valid samples must have a result equal to or greater than the detection limit. There must be at least two unique results above the detection limit. If any of these criteria are not met for a pollutant, then it is not recommended to report an annual mean for that pollutant. The annual mean for the concentration of a pollutant is the arithmetic mean of all samples in a calendar year after all null values are excluded and results below the detection limit are replaced with estimates using maximum likelihood estimation. Calculating upper confidence limits In order to calculate an upper confidence limit for the annual mean concentration of an air toxic, the same criteria required to calculate an annual mean for the concentration are also required to calculate an upper confidence limit. Nonparametric bootstrap sampling is used for calculating upper confidence limits since most air toxics data is not normally distributed. Information about bootstrapping can be found here: https://web.stanford.edu/class/psych252/tutorials/doBootstrapPrimer.pdf. At least 1,000 bootstrap samples should be taken, and a larger number of samples is better depending on the computation times. MPCA uses 95 percent upper confidence limits for comparisons to inhalation health benchmarks. Comparing site annual means There may be interest in comparing the annual means of a site over multiple years to determine if there is significant change over time or to compare the annuals means of multiple sites in a single year to determine if some sites have significantly higher concentrations of pollutants than others. MPCA uses this methodology to compare site means: MPCA does not use parametric methods for normally distrusted data such as t-tests since most air toxics data is not normally distributed. Pairwise comparisons of samples taken at different sites at the same time are not used since null values and values below the method detection limit make it difficult to perform pairwise comparisons across sites while still utilizing most of the samples collected. Bootstrap sampling is used to generate a confidence interval for the difference in annual means between years / sites. A confidence interval for the difference in annual means is generated instead of confidence intervals for each individual annual mean since the joint probability of two means being approximately the same is lower than the marginal probabilities of each mean existing in the other&#39;s mean&#39;s confidence interval. MPCA uses 95 percent two-sided confidence intervals for the difference in annual means. If the lower bound of the interval is greater than zero or the upper bound is less than zero, then there is a significant difference between the annual means. If the confidence interval includes zero, then there is no significant difference. "],
["edit-this-book.html", "Edit this book", " Edit this book This guide lives online at GitHub. To make changes go to github.com/MPCA-air/air-methods and click [ Fork ]. To make suggestions go to github.com/MPCA-air/air-methods/issues and click [ New issue ]. We are using the R bookdown package (Xie 2017) in this book, which was built on top of R-Markdown and knitr (Xie 2015). Code folding reference: https://stackoverflow.com/questions/34784121/interactively-show-hide-code-r-markdown-knitr-report "]
]
