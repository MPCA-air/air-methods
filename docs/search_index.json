[
["index.html", "Introduction Edit this book", " Introduction Updated Jul 26, 2017 This guide describes the methods used at the MPCA to analyze air monitoring and modeling data. The charts and code found in this guide were produced using the freely available R statistical software. To follow along with the examples, you can download a copy of R to your computer from the r-project or by using R-Fiddle in your browser. Edit this book This guide lives online at GitHub. To make changes go to github.com/MPCA-air/air-methods and click [ Fork ]. To make suggestions go to github.com/MPCA-air/air-methods/issues and click [ New issue ]. We are using the R bookdown package (Xie 2017) in this book, which was built on top of R-Markdown and knitr (Xie 2015). Code folding reference: https://stackoverflow.com/questions/34784121/interactively-show-hide-code-r-markdown-knitr-report References "],
["data-cleaning.html", " Data cleaning 1.1 Remove blank, NULL, and missing values 1.2 Remove Qualified data 1.3 Duplicate obersvations 1.4 Multiple air monitors (POCs) 1.5 Outliers", " Data cleaning .showopt { background-color: #4183c4; color: #FFFFFF; width: 104px; height: 26px; text-align: center; vertical-align: middle !important; float: right; font-family: sans-serif; border-radius: 9px; } .showopt:hover { background-color: #dfe4f2; color: #004c93; } pre.plot { background-color: white !important; } Before jumping into your analysis youâ€™ll want to clean it up with some helpful quality checks and formatting procedures. These procedures include steps to: 1.1 Remove blank, NULL, and missing values 1.2 Evaluate qualified data 1.3 Remove duplicate observations 1.4 Prioritize observations from multiple monitors (POCs) 1.5 Evaluate outliers 1.1 Remove blank, NULL, and missing values Description Large monitoring data sets often contain observations with missing concentrations, detection limits or other labeling errors that can lead to incorrect summary statistics if not removed. Recommended steps Identify blank, NULL, -999, and missing values. Remove those observations. Why not keep them? Unless you can confirm what the missing values are meant to represent, such as observations below the detection limit, there is not an accurate means to determine the meaning of a missing value. Sample R script Load the sample monitoring data. library(dplyr) data &lt;- read.csv(text = &#39; aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,-999.99,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,NA,NA,0.08,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,&quot; &quot;,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,NULL,a,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; &#39;, stringsAsFactor = F) Table: Sample monitoring data Create function to test for missing concentration values. # Test for missing concentrations, non-numeric values, and -999 missing_conc &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } Use the missing_conc() function to search for observations with missing concentrations. # Create a new TRUE/FALSE column labeling each result as missing or not data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc)) # Select all missing observations missing_values &lt;- filter(data, conc_missing == TRUE) Table: Missing values Remove the blank, NULL, -999, and missing values from the data set. data &lt;- filter(data, conc_missing == FALSE) Table: The new and improved cleaner data Create similar functions to test for missing dates, site IDs, detection limits, and parameter codes. # Test for missing dates missing_dates &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &gt; 11 || nchar(as.character(x)) &lt; 6 } # Test for missing site IDs missing_sites &lt;- function(x) { is.na(as.character(x)) || nchar(as.character(x)) &lt; 5 } # Test for missing detection limits missing_dls &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 } # Test for missing parameter code missing_param &lt;- function(x) { is.na(as.numeric(x)) || as.numeric(x) &lt; -900 || nchar(as.character(x)) &lt; 5 || nchar(as.character(x)) &gt; 9 } To apply all these functions at once, use the following script. # Create new TRUE/FALSE columns labeling each result as missing or not # Filter to remove any rows with a missing value data &lt;- data %&gt;% rowwise() %&gt;% mutate(conc_missing = missing_conc(conc), date_missing = missing_dates(date), site_missing = missing_sites(aqs_id), dl_missing = missing_dls(md_limit), param_missing = missing_param(param_code)) %&gt;% filter(sum(c(conc_missing, date_missing, site_missing, dl_missing, param_missing), na.rm = T) &lt; 1) Table: The super cleaner data Contributors Dorian Kvale 1.2 Remove Qualified data Description Recommended steps Contributors Not me 1.3 Duplicate obersvations Description Large monitoring data sets often contain a few instances when multiple observations occur over the same time period at the same site. When multiples observations do occur they tend to be identified by a unique monitor POC (parameter occurrence code) or by a qualifier describing why the second observation was recorded (e.g. a duplicate for quality control). Recommended steps Identify duplicate observations. If both values are detected, calculate the mean of the duplicates. If values are identical, use the observation with a lower detection limit. Why not keep them all? If more data is more better, why not keep both values? Leaving both observations in the data set is likely to bias calculated summary statistics. For example, consider a scenario where two monitors were operating on a site during the winter months, but only one monitor was operating the rest of the year. If we were to calculate the annual average for this site using all the observations, the mean would be biased by having twice as many observations from the winter months. By limiting the number of observations to 1 per day, the weighting of the seasons will be balanced. Sample R script Load the sample monitoring data. library(dplyr) data &lt;- read.csv(text = &#39; aqs_id,poc,param_code,date,conc,null_code,md_limit,pollutant,cas 271231003,1,12101,&quot;2004-01-04&quot;,0.05,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-10&quot;,0.23,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-16&quot;,0.35,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-22&quot;,0.22,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-01-28&quot;,0.01,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-03&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-09&quot;,0.02,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-15&quot;,0.07,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-21&quot;,0.03,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271231003,1,12101,&quot;2004-02-27&quot;,0.21,NA,0.06,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; 271377001,1,12101,&quot;2007-09-21&quot;,0.14,NA,0.04,&quot;Aluminum&quot;,&quot;7429-90-5&quot; &#39;, stringsAsFactor = F) Table: Sample monitoring data Add a unique column identifier to each observation. # Add unique key to each row data$key &lt;- 1:nrow(data) # Create s unique ID for each site/poc/param code/day combination data$unique_sample_id &lt;- paste(data$aqs_id, data$poc, data$param_code, data$date, sep = &quot;_&quot;) Figure 1.1: Unique ID column added. Test for duplicate observations. # Label duplicate samples data &lt;- group_by(data, unique_sample_id) %&gt;% mutate(duplicate = n() &gt; 1) # Create duplicate table dupes &lt;- filter(data, duplicate == T) Table: Duplicate observations If duplicates are found, use the following hierarchy to remove duplicates: Calculate the mean concentration of all detected observations. If no observations are detected, select the observation with the lower detection limit. If no obsercations are detected and the detection limits are all equal, select a single observation. dupes &lt;- group_by(dupes, unique_sample_id) %&gt;% arrange(desc(conc), md_limit) %&gt;% filter(key == key[1]) # Remove Unique IDs with duplicates from data data &lt;- filter(data, !duplicate) # Attach the selected duplicates with highest result and lowest detection limit data &lt;- rbind(data, dupes) Table: The new cleaner data Figure 1.2: Final data set with 1 duplicate removed. Contributors Dorian Kvale 1.4 Multiple air monitors (POCs) Description Recommended steps Contributors Not me 1.5 Outliers Description Recommended steps Contributors Not me Jump to top Reference: https://stackoverflow.com/questions/37755037/how-to-add-code-folding-to-output-chunks-in-rmarkdown-html-documents/37839683#37839683 "],
["detection-limits.html", " Detection limits 2.1 Below detection 2.2 Multiple detection limits", " Detection limits Here is a review of existing methods. 2.1 Below detection Coming soonâ€¦ 2.2 Multiple detection limits In progress. "],
["summary-statistics.html", " Summary statistics 3.1 Bootstrapping", " Summary statistics We describe our methods in this chapter. 3.1 Bootstrapping Bootstrapping provides methods for calculating summary statistics without the neccessity to make assumptions about whether the data is derived from a normal dirstribution. Packages library(dplyr) library(readr) Our data is organized by monitoring site and date. Hereâ€™s a sample. AQS_ID Date Conc 270535501 2009-07-30 0.00148 270535501 2009-09-30 0.00064 270535501 2009-11-30 0.34256 270535501 2009-12-30 0.00064 270535502 2009-03-30 0.26219 270535502 2009-07-30 0.01113 270535502 2009-09-30 0.00044 270535502 2009-11-30 0.00127 270535502 2009-12-30 0.00113 Bootstrap function We currently use the EnvStats package to generate our summary values. It has built in functions to accunt for non-detect data and allows for different distributions. However, if youâ€™re not dealing with non-detects you can use a simple loop to boot things yourself. Before you start youâ€™ll want to set the random number generator to ensure youâ€™ll be able to reproduce your results. Iâ€™ll use #27 below. set.seed(27) The general idea is to take a random sample from the data set, generate the statistic that youâ€™re interested in, record it, then rinse and repeat. Below is the code for how to resample a single site. # Filter data to a single site df_site1 &lt;- filter(df, AQS_ID == AQS_ID[1]) # Pull random sample # `replace=T` allows for the same value to be pulled multiple times # `size=nrow(df)` ensures the number of observations in the new table to match the original random_df &lt;- sample(df_site1$Conc, replace=T) # Generate summary statistic quantile(random_df, 0.1) ## 10% ## 0.00064 To repeat this 3,000 times we can wrap these steps into a resample function, and then use sapply to collect the results. # Create resample function resample_Conc &lt;- function(data= df_site1$Conc, Conc_pct= 0.10){ random_df &lt;- sample(data, replace=T) quantile(random_df, Conc_pct, na.rm=T)[[1]] } # Repeat using `sapply` repeats &lt;- 3000 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(df_site1$Conc)) # The 50th percentile or median Conc median(booted_10pct, na.rm=T) ## [1] 0.00064 # Return the 95th percentile of the booted concentrations quantile(booted_10pct, 0.95, na.rm=T)[[1]] ## [1] 0.00148 # Force the 95th percentile to be a recorded value sort(booted_10pct)[repeats*.95 +1] ## [1] 0.00148 # Upper and lower confidence interval around the median quantile(booted_10pct, c(0.025, 0.5, 0.975), na.rm=T) ## 2.5% 50% 97.5% ## 0.000640 0.000640 0.103216 Automate To finish up, throw these steps into your personal boot function, and then run it on each site using group_by. # Create boot function boot_low_Conc &lt;- function(data=df$Conc, Conc_pct=0.10, conf_int=0.95, repeats=3000){ alpha &lt;- (1 - conf_int)/2 booted_10pct &lt;- sapply(1:repeats, FUN=function(x) resample_Conc(data, Conc_pct)) # Upper and lower confidence interval around the median list(quantile(booted_10pct, c(alpha, 0.5, 1-alpha), na.rm=T)) } # Use `group_by` to send data for each site to your boot function conc_summary &lt;- group_by(df, AQS_ID) %&gt;% mutate(boot_results = boot_low_Conc(Conc, Conc_pct=0.10, conf_int=0.95)) %&gt;% summarize(Conc_10pct = quantile(Conc, 0.10, na.rm=T)[[1]], Low_CL95_conc = unlist(boot_results[1])[[1]], Boot_conc = unlist(boot_results[1])[[2]], Upper_CL95_conc = unlist(boot_results[1])[[3]]) Results The booted confidence limits AQS_ID Conc_10pct Low_CL95_conc Boot_conc Upper_CL95_conc 270535501 0.000640 0.00064 0.000640 0.103216 270535502 0.000716 0.00044 0.000716 0.005214 "],
["site-comparisons.html", " Site comparisons 4.1 Confidence intervals", " Site comparisons 4.1 Confidence intervals Some significant applications are demonstrated in this chapter. "],
["charts.html", " Charts 5.1 Boxplots 5.2 Calendar plots 5.3 Tableau 5.4 R 5.5 Wind &amp; pollution roses 5.6 openair package", " Charts 5.1 Boxplots Dividing by zero is an amazing super power. Unfortunately for you your computer does not have this power. Making a log boxplot with data containing zeros or negative values will result in your computer melting itself. Obviously, it will never forgive you. 5.1.1 Log boxplot Coming soonâ€¦ 5.1.2 Outliers Coming soonâ€¦ 5.2 Calendar plots We have finished a nice book. 5.3 Tableau 5.4 R 5.5 Wind &amp; pollution roses Coming soonâ€¦ 5.6 openair package Coming soonâ€¦ "],
["time-series.html", " Time series 6.1 Seasonality 6.2 Change points (before &amp; after)", " Time series 6.1 Seasonality The trend is up. 6.1.1 Sub-sub section Nothing to see here. 6.2 Change points (before &amp; after) Roll a dice. "],
["maps.html", " Maps 7.1 Kriging 7.2 Spatial averaging and aggregation", " Maps 7.1 Kriging Coming soonâ€¦ 7.2 Spatial averaging and aggregation To average acrossâ€¦ "],
["edit-this-book-1.html", "Edit this book", " Edit this book This guide lives on GitHub. To make changes go to github.com/MPCA-air/air-methods and click [ Fork ]. To make suggestions go to github.com/MPCA-air/air-methods/issues and click [ New issue ]. "],
["references.html", "References", " References "]
]
