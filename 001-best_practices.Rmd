# Best practices


![](https://avatars3.githubusercontent.com/u/22032646?s=400&v=4){width="240" align="right" style="margin-left: 20px; margin-right: 18px; margin-top: -24px;"}


<br>

This section includes some _best practices_ to promote consistent, efficient, and accurate data analysis with `R`.  


<br> <br>

## Tidyverse

The  _Tidyverse_ consists of a group of R packages that work in harmony by sharing a common data format and syntax. Using packages within the _Tidyverse_ for your analysis will make your scripts easier to read for others.

![](images/tidy_packages.PNG){width="460"}



## Keeping R up-to-date

__Get the _installr_ package__
```{r, eval=F}
install.packages('installr') 
library('installr')
```

__Install the new R version__
```{r, eval=F}
updateR()
```

- When a popup suggests closing RStudio and running the update from _RGUI_, 
click __NO__. 
- When asked whether to copy and update all of your packages. Click __Yes__. This will save you heaps of time and prevent future headaches.

To learn more about _installr_ see https://github.com/talgalili/installr/#readme.


## Script layout

The following tips will help make your R scripts easier to read and easier for others to use your code in their own projects.

- Add a description to the top of your script.
- Load all your packages in one block at the top of the script.
- Assign important file paths and parameters near the top of the script.
- Avoid changing working directories with `setwd()`.
- Load usernames and passwords from a file such as `credentials.csv`.
- Save files to a local working directory such as `"~\results.csv"`, rather than to a fixed location, such as `X:\EAO\Air Data\Project1\results.csv`.


Here is an example script layout:

```{r, eval = F}
# name_of_script.R

# Purpose: This script does amazing things.
# Assumptions: The monitoring data for the sites is from different years, 
#              but we compare them anyways. 
# Warning! Don't use this script for Kryptonite. It will break everything.

# Load packages
library(dplyr)
library(readr)

# Set parameters
year <- 2017
data_file <- "monitors 1 and 2.csv"


# Load data
air_data <- read_csv(data_file)


# My functions
calc_stat <- function(data) {
  
  new_stat <- step1(data) %>% step2() %>% step3()
  
}

# My analysis
results <- air_data %>% summarize(stat = calc_stat(concentration))


# Save results to local folder
write_csv(results, "results/summary_results.csv")

```



## Divide and conquer

For complex analysis projects many small components are often better than one big super script. As much as possible, try to create small functions to perform each task within in your analysis. If it's a function that you will use across multiple projects, it is often helpful to save the function to it's own script. These functions will become reusable building blocks that both future you and others can apply to their own projects. 

Using an R Markdown document is an additional way you can split your project into manageable steps. R Markdown makes it easier to include a short description of each step of your analysis. In fact, you are reading an R Markdown document now.

An example folder structure for a project is shown below:

`New_Project\` 

> - data  
>     - monitors 1 and 2.csv 
> - R    
>     - load_data.R 
>     - calc_ucl95.R   
>     - lil_boot_function.R  
>     - summary_report.RMD  
> - results  
>     - summary_results.csv  
>     - summary_report.pdf  
>  


## Codebooks, metadata, and data dictionaries

A __codebook__ provides essential technical details and references for the variables in a dataset. The codebook ensures someone unfamiliar with your data wll have the necessary information to complete their analysis and help them to estimate the uncertainty of their results. An R package named `dataMaid` is available to assist with creating documentation for your datasets.

__An example codebook generated with _dataMaid_.__

![](images/tidy_packages.PNG){width="460"}

```{r, eval = F}

library(dataMaid)
library(readr)

data <- read_csv('https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv')

names(data) <- c("aqs_id", "poc", "param_code", "date", "conc", "null_code", "md_limit", "pollutant", "year", "cas")

attr(data$aqs_id, "labels") <- "Monitor ID"
attr(data$aqs_id, "shortDescription") <- "EPA assigned monitor ID for the national Air Quality System (AQS)."

makeCodebook(data, vol = 1, reportTitle = "Codebook for air toxics monitoring data", replace = TRUE)


```



## Pollutant names

Pollutants often go by different names depending on the context you're in. To prevent confusion it helps to include a unique `CAS #` or parameter code in your analysis results and summary data. If you are working with monitoring data that is missing a unique identifier for each pollutant, an R package named `aircas` is available to help join CAS numbers to pollutant names.   

## Formatting data

Performing similar analysis on multiple pollutants at multiple sites over multiple years becomes much easier when data is in a consitent format. MPCA prefers tidy data where each result has its own row in a table. More information on tidy data can be found here: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html. For monitoring data, each row should contain columns with the monitoring site ID (ASQ ID is best, but other unique identifiers for each site are okay) parameter code, 

https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html




