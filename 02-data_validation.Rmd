# Data validation (Kristie)


```{r out.width='65%', echo=F}
knitr::include_graphics("https://imgs.xkcd.com/comics/error_code.png")
```


Environmental monitoring data can be subject to error. Three important errors to look for are:


- \@ref(leaks)    __Leaks and sample contamination__
- \@ref(sticky)   __Instrument errors such as drift or repeated results (sticking values)__ 
- \@ref(outs)     __Outliers or extreme values__


<br> __Validation steps__

1. Identify exceptional values.
1. Identify sequential identical values (sticking).
1. Identify potential instrument drift.
1. Report findings to laboratory for further investigation.


<br> __Why not average the exceptional data over the year and assume the problem will go away?__

Further investigation by laboratory staff is required if the laboratory equipment or system is in error. If the laboratory is not in error, these concentrations require additional investigation by data analysts and potentially by air pollution source specialists. For these reasons, these data validation steps are critical. A decreasing signal or repeated sequential measurement means that the data are not interpretable. An exceptionally high value could mean laboratory contamination or potentially could contribute to human health impacts if the high value is accurate.    


<br> __Notes__  


- Possible Contamination Issues-by site and pollutant
  - 3*IQR: I used as a simple outlier identifier in EU guidelines. This includes a lot of data for UFPs.

Cassie has tried:  

  - 3*75th percentile(UFP) - quantiles from previous year of data
  - 2.5 *75th percentile of the data (put in Derek's explanation) this is for integrated air toxics data
  - 3*SD (Poor performance)
  - Site mean more than twice the second highest site mean(NO)
  - Sites with maxima over 2X the next highest value(NO)
  - Only one detection at a site (special case)
    - And Detection was greater than 3X the MDL
    - And Detection was within 10% of state maximum that year  


Possible Sample loss issues (leaks) by site and pollutant  

- Check for several (2,3) below MDL values when values have been above MDLs previously (need discussion). MDL would be from the previous year.  
- I tried taking the mean of 5 sequential values, and then the next 5 sequential values. If the second set of means was 1.5 times less than the first mean I flagged the values.  
- __Recommendation:__ Use the `[75th percentile * 3]` for all pollutants.  


<br> __Sample `R` script__ 

Click the button below to view a step by step example of the methods above.

<div class="toggle"><button class = "btn_code">Show __R__ code</button>

__Goals:__ 

1. Find extreme values that may be in error or from contamination.
1. Find 3 repeating values that are exactly the same.
1. Find low values where this is not common.
1. Do this in a routine way (identify frequency of validation checks).


The following sections on data validation will use the example monitoring data loaded below.

```{r validation, eval=T, message=F, warning=F}
library(tidyverse)
library(stringr)
library(RcppRoll)
library(lubridate)
library(car)

##need to confirm that this data set has sequential repeats, some high values and at least 10 dates with numbers
data <- read_csv('https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv')

colnames(data) <- c("aqs_id", "poc", "param_code", "date", "conc", "null_code", "md_limit", "pollutant", "year", "cas")

dt_options <- list(scrollX = T, autoWidth = T, searching = F, ordering=F, lengthChange = F, paginate=F, info=F)

```

<br> _Table: Sample monitoring data_   
```{r, eval=T, echo=F, message=F, warning=F}
library(knitr)
library(DT)
#kable(head(data), booktabs = T, caption = "Sample monitoring data.")
datatable(head(data, 10), options = dt_options)
```

</div>
_First its' always a good idea to look at the data

```{r, eval=T, echo=F, message=F, warning=F}
library(shiny)
pollutants <- unique(data$pollutant)
sites <- unique(data$aqs_id)

shinyApp(
 ui = fluidPage(responsive = FALSE,
                   fluidRow(
                     column(3,
                            style = "padding-bottom: 20px;",
                            inputPanel(
  selectInput("pollutant", label="Choose a pollutant", choices = pollutants, selected="Benzene"),
  selectInput("site", label="Choose a site", choices = sites, selected=270535501))),
                    column(9,
                   plotOutput('detlim', height = "400px")))),
   

  server = function(input, output) {

    data <- read_csv('https://raw.githubusercontent.com/MPCA-air/air-methods/master/airtoxics_data_2009_2013.csv')

   colnames(data) <- c("aqs_id", "poc", "param_code", "date", "conc", "null_code", "md_limit", "pollutant", "year", "cas")

output$detlim <- renderPlot({
  print(input$pollutant)
  print(input$site)
    data_sub = filter(data, pollutant==input$pollutant, aqs_id == input$site)
    data_sub$Censored <- ifelse(data_sub$conc > data_sub$md_limit, FALSE, TRUE)
    mdl <- mean(data_sub$md_limit)
    ggplot(data=data_sub, aes(x= date, y=conc)) +
    geom_point(aes(color=Censored), size =2.4, alpha=0.55) +
    geom_hline(yintercept=mdl) +
    xlab(NULL) +
    ylab("Result (ug/m3)") +
    expand_limits(y=c(0, max(data_sub$conc))) +
    scale_colour_manual(values= c("#197519"[FALSE %in% unique(data_sub$Censored)], "#0000FF"[TRUE %in% unique(data_sub$Censored)]), breaks=c(FALSE, TRUE)) +
    theme(text = element_text(size=15), axis.text.x = element_text(angle = -90, vjust = 0.3,  size=14)) +
    ggtitle(paste0("Time series for ", input$pollutant, " at site ", input$site),
    subtitle = "---- Horizontal Line ----  =  Detection Limit")
})

})
```


## Identifying Instrument drift or leaks in a system. {#leaks} 

The test looks for a difference in variance of carbon tetrachloride between a calendar quarter for two consecutive years. Carbon tetrachloride was chosen because it is a banned substance and no longer in use, it has very few below detection limit values, and no direct sources. It has seasonally variable concentrations, so calendar quarters were compared to eliminate the detection of seasonal differences. The statistical test used was a Levenes Test for homogeneity of variance. 


<br> __Sample `R` script__ 

Click the button below to view a step by step example of the method above.

<div class="toggle"><button class = "btn_code">Show __R__ code</button>

<br>

```{r echo=T, message=F, warning=F}  

data$conc <- as.numeric(data$conc)
data$aqs_id <- as.character(data$aqs_id)
sites <- unique(data$aqs_id)
years <- unique(data$year)
data$quarter = quarter(data$date)
quarters <- unique(data$quarter)
pocs <- unique(data$poc)

clean_values = function(data) {
  data$conc = as.numeric(as.character(data$conc))
  data$conc[abs(data$conc) >= 999] = NA
  return(data)
}

data = clean_values(data)

levene_function = function(conc, quarter_year, row, col) {
  if(length(unique(quarter_year))<2){
  return(NA)} 
  data = data.frame(conc = conc, quarter_year = quarter_year)
  return(leveneTest(conc~as.factor(quarter_year), data = data)[row,col])
}

leak_table_unfiltered <- data.frame()
for(i in max(years):max(years)-1:length(years)){
  for(j in i+1){
    data_carbontet=data.frame()
  data_carbontet <- filter(data, pollutant=="Carbon Tetrachloride", !is.na(conc), year %in% c(i, j))
data_carbontet$quarter_year = paste(data_carbontet$quarter, "_", data_carbontet$year)
data_carbontet <- data_carbontet %>% group_by(aqs_id, poc, quarter) %>% 
  summarise(fvalue_levene = levene_function(conc, quarter_year, 1,2), 
            pval_levene = levene_function(conc, quarter_year, 1,3), 
            deg_free_levene = levene_function(conc, quarter_year, 2,1),
            Year_1 = min(year), 
            Year_2 = max(year),
            Mean_Year_1 = mean(conc[year==min(year)], na.rm=T),
            Mean_Year_2 = mean(conc[year==max(year)], na.rm=T)) %>% ungroup()
leak_table_unfiltered  <- rbind(leak_table_unfiltered,data_carbontet)
  }
}
leak_table <- filter(leak_table_unfiltered, pval_levene<0.01, abs(Year_1-Year_2)==1)
leak_table$Warning_Type <- "Decrease_in_Measurements"

datatable(head(leak_table, 10), options = dt_options) %>% formatSignif(c("fvalue_levene", "pval_levene", "Mean_Year_1", "Mean_Year_2"), digits = 2)

```
</div>



## Identify exceptional and extreme values aka outliers. {#outs} 

These are detected by comparing each measured concentration to 3 X the 75th percentile of the data set by year, site, and pollutant. For now, pocs collocated measurements are not tested separately. 


<br> __Sample `R` script__ 


Click the button below to view a step by step example of the method above.

<div class="toggle"><button class = "btn_code">Show __R__ code</button>

<br>

```{r eval=T, message=F, warning=F}

# Test for exceptionally high values [above 75th percentile X 3]
high_data <- group_by(data, year, aqs_id, pollutant) %>% mutate(AR_Mean = mean(conc, na.rm=T), Percentile_75 = quantile(conc, 0.75, na.rm=T), Percentile_75_X3 = Percentile_75*3) %>% ungroup()
high_data <- filter(high_data, conc>Percentile_75_X3, !is.na(conc), AR_Mean>md_limit, Percentile_75>0)
high_data$Warning_Type <- "Exceptionally_High_Value_Test"
high_data <- unique(high_data)
datatable(head(high_data, 10), options = dt_options) 


high_data <- high_data[, c("date", "pollutant", "aqs_id", "Warning_Type")]
##Toxics - Multiplying by 3 returns ~4000/1176818 values as extreme, multiplying by 1.5 returns >20,000. Multiplying by 3 returns between 2 and 45% of analyte/site/year groups.
##UFP - Multiplying by 3 returns 28894 of 1335083 total values. Multiplying by 3 returns between 2 and 15% of analyte/site/year groups.
##PAHs - Multiplying by 3 returns 168 of 39298 total values. Multiplying by 3 returns between 3 and 10% of analyte/site/year groups.
```


## Identify sequential repeats aka "sticky" numbers {#sticky}
 
Three sequential replicate values may be a result of a machine error.

```{r eval=T, message=F, warning=F}

# Test for exceptionally high values [above 75th percentile X 3]
repeat_data <- group_by(data, poc, year, aqs_id, pollutant) %>% 
  arrange(year, aqs_id, poc, pollutant, date) %>% 
  mutate(Previous_Day = lag(conc, 1), 
         Two_Days_Prior = lag(conc, 2)) %>% ungroup()
repeat_data <- mutate(repeat_data, Repeat_Test = ifelse(round(conc, digits=4)==round(Previous_Day, digits=4) & round(Previous_Day, digits=4)==round(Two_Days_Prior, digits=4), "TRUE", "FALSE"))
repeat_data$Warning_Type <- "Repeat_Test"
repeat_data <- filter(repeat_data, conc>0 &  Repeat_Test==TRUE)

datatable(head(repeat_data, 10), options = dt_options)

repeat_data <- repeat_data[, c("date", "pollutant", "aqs_id", "Warning_Type")]
###There are many repeating values in the Toxics data. None in PAHs. Approximately 3200 for UFPs, but this is 15 minute data. We may need a more stringent test for UFPs.
```


<br> _Table: Dates and pollutants to check including the potential issue_   
```{r, eval=T, echo=F, message=F, warning=F}
warnings_table <- rbind(repeat_data, high_data)

datatable(head(warnings_table, 10), options = dt_options, rownames = FALSE)
```

</div>

<br> __Contributors__  

> Kristie Ellickson, Cassie McMahon, Dorian Kvale, Derek Nagel


<br> __References__    
[USEPA Technical Support Document for the Nation Air Toxics Trends Sites](https://www3.epa.gov/ttnamti1/files/ambient/airtox/NATTS%20TAD%20Revision%203_FINAL%20October%202016.pdf)

[European Union Guide toe Data Validation](https://webgate.ec.europa.eu/fpfis/mwikis/essvalidserv/images/a/ad/PRACTICAL_GUIDE_TO_DATA_VALIDATION.pdf)

[US EPA Guidance on Data Verification and Data Validation] (https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf)

[US EPA Data Validation Workbook Presentation and Training Materials] (https://www3.epa.gov/ttnamti1/files/ambient/airtox/workbook/T-Workbook_Secs1-8.pdf)

[US EPA Data Validation Workbook] (https://nepis.epa.gov/Exe/ZyNET.exe/P1006PAB.TXT?ZyActionD=ZyDocument&Client=EPA&Index=2006+Thru+2010&Docs=&Query=&Time=&EndTime=&SearchMethod=1&TocRestrict=n&Toc=&TocEntry=&QField=&QFieldYear=&QFieldMonth=&QFieldDay=&IntQFieldOp=0&ExtQFieldOp=0&XmlQuery=&File=D%3A%5Czyfiles%5CIndex%20Data%5C06thru10%5CTxt%5C00000016%5CP1006PAB.txt&User=ANONYMOUS&Password=anonymous&SortMethod=h%7C-&MaximumDocuments=1&FuzzyDegree=0&ImageQuality=r75g8/r75g8/x150y150g16/i425&Display=hpfr&DefSeekPage=x&SearchBack=ZyActionL&Back=ZyActionS&BackDesc=Results%20page&MaximumPages=1&ZyEntry=1&SeekPage=x&ZyPURL)

[Review article by Helsel, D about non-detects and substitution methods.] (https://academic.oup.com/annweh/article/54/3/257/223531/Much-Ado-About-Next-to-Nothing-Incorporating)

[A Chemosphere review article describing why BDL substitution should not be done.] (https://pdfs.semanticscholar.org/182e/9278fc36dd73d48a414b8fbc30a44ea314d3.pdf)

[Temporal variability of selected air toxics in the United States] (http://www.sciencedirect.com/science/article/pii/S1352231007004840)

[Spatial and temporal analysis of national air toxics data] (http://www.tandfonline.com/doi/pdf/10.1080/10473289.2006.10464576)

